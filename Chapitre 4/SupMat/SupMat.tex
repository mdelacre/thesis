% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
  man,mask,floatsintext]{apa6}
\title{Supplemental Material}
\author{Marie Delacre\textsuperscript{1}, Daniel Lakens\textsuperscript{2}, Christophe Ley\textsuperscript{3}, Limin Liu\textsuperscript{3}, \& Christophe Leys\textsuperscript{1}}
\date{}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Supplemental Material},
  pdfauthor={Marie Delacre1, Daniel Lakens2, Christophe Ley3, Limin Liu3, \& Christophe Leys1},
  pdflang={en-EN},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{}
\keywords{\newline\indent Word count: 2287 words}
\usepackage{csquotes}
\usepackage{bm}
\usepackage{lscape}
\newcommand{\blandscape}{\begin{landscape}}
\newcommand{\elandscape}{\end{landscape}}
\ifXeTeX
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[main=english]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi


\affiliation{\vspace{0.5cm}\textsuperscript{1} Universit√© Libre de Bruxelles, Service of Analysis of the Data (SAD), Bruxelles, Belgium\\\textsuperscript{2} Eindhoven University of Technology, Human Technology Interaction Group, Eindhoven, the Netherlands\\\textsuperscript{3} Universiteit Gent, Department of Applied Mathematics, Computer Science and Statistics, Gent, Belgium}

\begin{document}
\maketitle

\hypertarget{supplemental-material-1-theoretical-bias-and-variance-of-estimators}{%
\section{Supplemental Material 1 : theoretical bias and variance of estimators}\label{supplemental-material-1-theoretical-bias-and-variance-of-estimators}}

\hypertarget{theoretical-bias}{%
\subsection{Theoretical bias}\label{theoretical-bias}}

For all ``biased'' estimators, when the population effect size is null so is the bias. We will therefore focus on configurations where there is a non-null population effect size. The sampling distribution of Cohen's \(d\) (and therefore its bias) is only known under the assumptions of normality and homoscedasticity. On the other hand, the biases of Glass's \(d\), Cohen's \(d^*\) and Shieh's \(d\) are theoretically known for all configurations where the normality assumption is met. In order to simplify the analysis of their bias, it is convenient to subdivide all configurations into 3 conditions :\\
- when population variances are equal across groups;\\
- when population variances are unequal across groups, with equal sample sizes;\\
- when population variances are unequal across groups, with unequal sample sizes.

\hypertarget{preliminary-note}{%
\subsubsection{Preliminary note}\label{preliminary-note}}

For all previously mentioned estimators (Cohen's \(d\), Glass's \(d\), Cohen's \(d^*\) and Shieh's \(d\)), the theoretical expectation is computed by multiplying the population effect size (respectively \(\delta_{Cohen}\), \(\delta_{Glass}\), \(\delta^*_{Cohen}\) and \(\delta_{Shieh}\)) by the following multiplier coefficient:
\begin{equation} 
\gamma=\frac{\sqrt{\frac{df}{2}} \times \Gamma{\frac{df-1}{2}}}{\Gamma{\frac{df}{2}}}
\label{eq:mc}
\end{equation}
where \(df\) are the degrees of freedom (see the main article). \(\gamma\) is \emph{always} positive, meaning that when the population effect size is not zero, all estimators will overestimate the population effect size. Moreover, its limit tends to 1 when the degrees of freedom (\(df\)) tend to infinity, meaning that the larger the degrees of freedom, the lower the bias.

While we focus on the theoretical bias of biased estimators when the normality assumption is met, it is interesting to notice that our main conclusions seem to generalize to :\\
- biased estimators when samples are extracted from symmetric distributions;\\
- unbiased estimators when samples are extracted from heavy-tailed symmetric distributions.

\hypertarget{cohens-bmd-see-table-2}{%
\subsubsection{\texorpdfstring{Cohen's \(\bm{d}\) (see Table 2)}{Cohen's \textbackslash bm\{d\} (see Table 2)}}\label{cohens-bmd-see-table-2}}

Under the assumptions that independant residuals are normally distributed with equal variances, the \textbf{bias} of Cohen's \(d\) is a function of total sample size (\(N\)) and the population effect size (\(\delta_{Cohen}\)) :

\begin{itemize}
\item
  The larger the population effect size, the more Cohen's \(d\) will overestimate \(\delta_{Cohen}\);\\
\item
  The larger the total sample size, the lower the bias (see Figure SM1.1);
  \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.1.png}
  \emph{Figure SM1.1} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d\), when variances are equal across groups, as a function of the total sample size (\(N\)).
\item
  Of course, considering the degrees of freedom, the sample size ratio does not matter (i.e.~the bias will decrease when increasing \(n_1\), \(n_2\) or both sample sizes).
\end{itemize}

\newpage

\hypertarget{glasss-bmd-see-table-3}{%
\subsubsection{\texorpdfstring{Glass's \(\bm{d}\) (see Table 3)}{Glass's \textbackslash bm\{d\} (see Table 3)}}\label{glasss-bmd-see-table-3}}

Because degrees of freedom depend only on the control group size (neither on \(\sigma_1\) nor on \(\sigma_2\)), there is no need to distinguish between cases where there is homoscedasticity or heteroscedasticity!

The \textbf{bias} of Glass's \(d\) is a function of the control group size (\(n_c\)) and the population effect size (\(\delta_{Glass}\)) :

\begin{itemize}
\item
  The larger the population effect size, the more Glass's \(d\) will overestimate \(\delta_{Glass}\);
\item
  The larger the size of the control group, the lower the bias (see the two top plots in Figure SM1.2). On the other hand, increasing the experimental group size does not impact the bias (see the two bottom plots in Figure SM1.2).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.2.png}
\emph{Figure SM1.2} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Glass's \(d\), when variances are equal across groups, as a function of \(n_c\) (top) and \(n_e\) (bottom).
\newpage

\hypertarget{cohens-bmd-see-table-3}{%
\subsubsection{\texorpdfstring{Cohen's \(\bm{d^*}\) (see Table 3)}{Cohen's \textbackslash bm\{d\^{}*\} (see Table 3)}}\label{cohens-bmd-see-table-3}}

\hypertarget{when-variances-are-equal-across-populations}{%
\paragraph{When variances are equal across populations}\label{when-variances-are-equal-across-populations}}

When \(\sigma_1=\sigma_2=\sigma\) :
\[df_{Cohen's \; d^*} = \frac{(n_1-1)(n_2-1)(2\sigma^2)^2}{(n_2-1)\sigma^4+(n_1-1)\sigma^4} = \frac{(n_1-1)(n_2-1)\times 4\sigma^4}{\sigma^4(n_1+n_2-2)} = \frac{4(n_1-1)(n_2-1)}{n_1+n_2-2}\]
One can see that degrees of freedom depend only on the total sample size (\(N\)) and the sample size allocation ratio \(\left( \frac{n_2}{n_1}\right)\). As a consequence, the \textbf{bias} of Cohen's \(d^*\) is a function of the population effect size (\(\delta^*_{Cohen}\)), the sample size allocation ratio \(\left( \frac{n_2}{n_1}\right)\) and the total sample size (\(N\)).

\begin{itemize}
\tightlist
\item
  The further the sample size allocation ratio is from 1, the larger the bias (see Figure SM1.3);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.3.png}
\emph{Figure SM1.3} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\), when variances are equal across groups, as a function of the logarithm of the sample sizes ratio \(log\left(\frac{n_2}{n_1} \right)\).

\begin{itemize}
\item
  The larger the population effect size, the more Cohen's \(d^*\) will overestimate \(\delta^*_{Cohen}\);
\item
  The larger the total sample size, the lower the bias (see Figure SM1.4).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.4.png}
\emph{Figure SM1.4} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\), when variances are equal across groups, as a function of the total sample size (\(N\)).

\hypertarget{when-variances-are-unequal-across-populations-with-equal-sample-sizes}{%
\paragraph{When variances are unequal across populations, with equal sample sizes}\label{when-variances-are-unequal-across-populations-with-equal-sample-sizes}}

When \(n_1 = n_2 = n\):
\[df_{Cohen's \; d^*} = \frac{(n-1)^2(\sigma^2_1+\sigma^2_2)^2}{(n-1)(\sigma^4_1+\sigma^4_2)} =  \frac{(n-1)(\sigma^4_1+\sigma^4_2+2\sigma^2_1\sigma^2_2)}{\sigma^4_1+\sigma^4_2}\]
One can see that degrees of freedom depend only on the total sample size (\(N\)) and the \(SD\)-ratio \(\left( \frac{\sigma_2}{\sigma_1}\right)\). As a consequence, the \textbf{bias} of Cohen's \(d^*\) is a function of the population effect size (\(\delta^*_{Cohen}\)), the \(SD\)-ratio \(\left( \frac{\sigma_2}{\sigma_1}\right)\) and the total sample size (\(N\)) :
\newpage

\begin{itemize}
\tightlist
\item
  The larger the population effect size, the more Cohen's \(d^*\) will overestimate \(\delta^*_{Cohen}\);
\item
  The further the \(SD\)-ratio is from 1, the larger the bias (see Figure SM1.5);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.5.png}
\emph{Figure SM1.5} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\), when variances are unequal across groups and sample sizes are equal, as a function of the logarithm of the \(SD\)-ratio (\(log \left( \frac{\sigma_2}{\sigma_1} \right)\)).\\
\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the bias (see Figure SM1.6).\\
  \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.6.png}
  \emph{Figure SM1.6} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\), when variances are unequal across groups and sample sizes are equal, as a function of the total sample size (\(N\)).
\end{itemize}

\newpage

Note : for a constant \(SD\)-ratio, \(\sigma_1\) and \(\sigma_2\) do not matter (see Figure SM1.7).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.7.png}
\emph{Figure SM1.7} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\), when variances are unequal across groups and sample sizes are equal, as a function of \(\sigma_1\) (top plots) and \(\sigma_2\) (bottom plots), for a constant \(SD\)-ratio.

\hypertarget{when-variances-are-unequal-across-populations-with-unequal-sample-sizes}{%
\paragraph{When variances are unequal across populations, with unequal sample sizes}\label{when-variances-are-unequal-across-populations-with-unequal-sample-sizes}}

The \textbf{bias} of Cohen's \(d^*\) is a function of the population effect size (\(\delta^*_{Cohen}\)), the total sample size (\(N\)), and the interaction between the sample sizes ratio and the \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\) :

\begin{itemize}
\tightlist
\item
  The larger the population effect size, the more Cohen's \(d^*\) will overestimate \(\delta^*_{Cohen}\);
\end{itemize}

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the bias (see Figure SM1.8);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.8.png}
\emph{Figure SM1.8} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\), when variances and sample sizes are unequal across groups, as a function of the total sample size (\(N\)).
\newpage

\begin{itemize}
\tightlist
\item
  The smallest bias always occurs when there is a positive pairing between variances and sample sizes, because one gives more weight to the smallest variance, in the denominator of the \(df\) computation. Moreover, the further the \(SD\)-ratio is from 1, the further from 1 will also be the sample sizes ratio associated with the smallest bias (see Figure SM1.9). This can be explained by splitting the numerator and the denominator in the \(df\) computation.
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.9.png}
\emph{Figure SM1.9} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\) when variances and sample sizes are unequal across groups, as a function of the logarithm of the sample sizes ratio (\(log \left( \frac{n_2}{n_1} \right)\)), when \(SD\)-ratio equals .68 (first row), .29 (second row) or .14 (third row).

\newpage

As illustrated in Figure SM1.10, for any \(SD\)-ratio, the numerator of the degrees of freedom will be maximized when sample sizes are equal across groups (and is not impacted by the \(SD\)-ratio). On the other hand, the denominator will be minimized when there is a positive pairing between variances and sample sizes. For example, when \(\sigma_1 > \sigma_2\), the smallest denominator occurs when \(\frac{n_2}{n_1}\) reaches its minimum value and the further from 1 the \(SD\)-ratio, the larger the impact of the sample sizes ratio on the denominator.

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.10.png}
\emph{Figure SM1.10} : Numerator and denominator of the degrees of freedom (\(DF\)) computation, when computing the bias of Cohen's \(d^*\) when variances and sample sizes are unequal across groups, as a function of the logarithm of the sample sizes ratio (\(log \left( \frac{n_2}{n_1} \right)\)), when \(SD\)-ratio equals .68 (first row), .29 (second row) or .14 (third row).

\newpage

Note : for a constant \(SD\)-ratio, the variance does not matter. (See Figure SM1.11).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.11.png}
\emph{Figure SM1.11} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Cohen's \(d^*\), when variances and sample sizes are unequal across groups, as a function of \(\sigma= \frac{(\sigma_1^2+\sigma_2^2)}{2}\), for a constant \(SD\)-ratio.

\hypertarget{shiehs-bmd-see-table-3}{%
\subsubsection{\texorpdfstring{Shieh's \(\bm{d}\) (see Table 3)}{Shieh's \textbackslash bm\{d\} (see Table 3)}}\label{shiehs-bmd-see-table-3}}

\hypertarget{when-variances-are-equal-across-populations-1}{%
\paragraph{When variances are equal across populations}\label{when-variances-are-equal-across-populations-1}}

When \(\sigma_1=\sigma_2=\sigma\):
\[df_{Shieh's \; d} = \frac{\left( \frac{n_2\sigma^2+n_1\sigma^2}{n_1n_2}\right)^2}{\frac{(n_2-1)\left( \frac{\sigma^2}{n_1}\right)^2+(n_1-1)\left( \frac{\sigma^2}{n_2}\right)^2}{(n_1-1)(n_2-1)}}\]
\[\leftrightarrow df_{Shieh's \; d} = \frac{[\sigma^2(n_1+n_2)]^2}{n_1^2n_2^2} \times \frac{(n_1-1)(n_2-1)}{(n_2-1) \times  \frac{\sigma^4}{n_1^2}+(n_1-1) \times \frac{\sigma^4}{n_2^2}}\]
\[\leftrightarrow df_{Shieh's \; d} = \frac{\sigma^4N^2}{n_1^2n_2^2} \times \frac{(n_1-1)(n_2-1)}{\sigma^4 \left( \frac{n_2-1}{n^2_1}+\frac{n_1-1}{n^2_2}\right) }\]
\[\leftrightarrow df_{Shieh's \; d} = \frac{N^2(n_1-1)(n_2-1)}{n_1^2n_2^2 \left( \frac{n_2^2(n_2-1)+n_1^2(n_1-1)}{n_1^2n_2^2}\right)}\]
\[\leftrightarrow df_{Shieh's \; d} = \frac{N^2(n_1-1)(n_2-1)}{n_2^2(n_2-1)+n_1^2(n_1-1)}\]
One can see that degrees of freedom depend only on the total sample size (\(N\)) and the sample size allocation ratio \(\left( \frac{n_2}{n_1}\right)\). As a consequence, the \textbf{bias} of Shieh's \(d\) is a function of the population effect size (\(\delta_{Shieh}\)), the sample size allocation ratio \(\left( \frac{n_2}{n_1}\right)\) and the total sample size (\(N\)).

\begin{itemize}
\tightlist
\item
  The larger the population effect size, the more Shieh's \(d\) will overestimate \(\delta_{Shieh}\);
\item
  The further the sample size allocation ratio is from 1, the larger the bias (see Figure SM1.12);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.12.png}
\emph{Figure SM1.12} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances are equal across groups, as a function of the logarithm of the sample sizes ratio \((log \left(\frac{n_2}{n_1})\right)\).

\newpage

\begin{itemize}
\tightlist
\item
  For a constant sample sizes ratio, the larger the total sample size, the lower the bias (see Figure SM1.13).
  \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.13.png}
  \emph{Figure SM1.13} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances are equal across groups, as a function of the total sample size (\(N\)).
\end{itemize}

\newpage

Note : when computing Cohen's \(d^*\), degrees of freedom increased when adding subjects in either one or both groups, even when the sample size ratio increased. When computing Shieh's \(d\), this is not true anymore : there is a larger impact of the sample sizes ratio such that moving the sample sizes ratio away from 1 when adding subjects in only one group can decrease the degrees of freedom and therefore, increase the bias (See Figure SM1.14).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.14.png}
\emph{Figure SM1.14} : Degrees of freedom (\(DF\)), when computing the bias of Shieh's \(d\), when variances are equal across groups, when adding subjects only in the first group (left) or in the second group (right).

\newpage

\hypertarget{when-variances-are-unequal-across-populations-with-equal-sample-sizes-1}{%
\paragraph{When variances are unequal across populations, with equal sample sizes}\label{when-variances-are-unequal-across-populations-with-equal-sample-sizes-1}}

When \(n_1=n_2=n\):
\[df_{Shieh's \; d} = \frac{\left( \frac{\sigma_1^2+\sigma_2^2}{n} \right)^2}{\frac{(\sigma_1^2/n)^2+(\sigma_2^2/n)^2}{n-1}}\]
\[\leftrightarrow df_{Shieh's \; d} = \frac{(\sigma_1^2+\sigma_2^2)^2}{n^2} \times\frac{n-1}{\frac{\sigma_1^4+\sigma_2^4}{n^2}}\]
\[\leftrightarrow df_{Shieh's \; d} = \frac{(\sigma_1^2+\sigma_2^2)^2 \times (n-1)}{\sigma_1^4+\sigma_2^4}\]

One can see that degrees of freedom depend on the total sample size (\(N\)) and the \(SD\)-ratio \(\left( \frac{\sigma_2}{\sigma_1}\right)\). As a consequence, the bias depends on the population effect size (\(\delta_{Shieh}\)), the \(SD\)-ratio \(\left( \frac{\sigma_2}{\sigma_1}\right)\) and the total sample size (\(N\)).

\begin{itemize}
\tightlist
\item
  The larger the population effect size, the more Shieh's \(d\) will overestimate \(\delta_{Shieh}\);
\end{itemize}

\newpage

\begin{itemize}
\tightlist
\item
  The further the \(SD\)-ratio is from 1, the larger the bias (see Figure SM1.15);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.15.png}
\emph{Figure SM1.15} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances are unequal across groups and sample sizes are equal, as a function of the logarithm of the \(SD\)-ratio \((log \left(\frac{\sigma_2}{\sigma_1})\right)\).

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the bias (see Figure SM1.16);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.16.png}
\emph{Figure SM1.16} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances are unequal across groups and sample sizes are equal, as a function of the total sample size (\(N\)).

\newpage

Note : for a constant \(SD\)-ratio, \(\sigma_1\) and \(\sigma_2\) do not matter (see Figure SM1.17).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.17.png}
\emph{Figure SM1.17} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances are unequal across groups and sample sizes are equal, as a function of \(\sigma_1\), for a constant \(SD\)-ratio.

\hypertarget{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-1}{%
\paragraph{When variances are unequal across populations, with unequal sample sizes}\label{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-1}}

The \textbf{bias} of Shieh's \(d\) is a function of the population effect size (\(\delta_{Shieh}\)), the sample sizes (\(n_1\) and \(n_2\)), and the interaction between the sample sizes ratio and the \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\) :

\begin{itemize}
\tightlist
\item
  The larger the population effect size, the more Shieh's \(d\) will overestimate \(\delta_{Shieh}\);
\end{itemize}

\newpage

\begin{itemize}
\tightlist
\item
  For a constant sample sizes ratio, the larger the sample sizes, the lower the bias (See Figure SM1.18);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.18.png}
\emph{Figure SM1.18} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances and sample sizes are unequal across groups, as a function of the total sample size (\(N\)).

\newpage

Note : When variances were equal across populations, adding subjects only in the first group had the same impact on degrees of freedom (and therefore on bias) than adding subjects only in the second group (see Figure SM1.14). When variances are unequal across groups, this is not true anymore (see Figure SM1.19).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.19.png}
\emph{Figure SM1.19} : Degrees of freedom (\(DF\)), when computing the bias of Shieh's \(d\), when variances and sample sizes are unequal across groups, as a function of the total sample size, when adding subjects only in one group (either in the first group; see top plots; or in the second group; see bottom plots), and \(\sigma_1 > \sigma_2\) (left plots) or \(\sigma_1 < \sigma_2\) (right plots).

\newpage

\begin{itemize}
\tightlist
\item
  The smallest bias always occurs when there is a positive pairing between variances and sample sizes. Moreover, the further the \(SD\)-ratio is from 1, the further from 1 will also be the sample sizes ratio associated with the smallest bias (See Figure SM1.20);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.20.png}
\emph{Figure SM1.20} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances and sample sizes are unequal across groups, as a function of the logarithm of the sample sizes ratio (\(log \left( \frac{n_2}{n_1} \right)\)), when \(SD\)-ratio equals .68 (first row), .29 (second row) or .14 (third row).

\newpage

Moreover, for a constant \(SD\)-ratio, the variances do not matter (See Figure SM1.21).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.21.png}
\emph{Figure SM1.21} : Degrees of freedom (\(DF\)) and \(\gamma\), when computing the bias of Shieh's \(d\), when variances and sample sizes are unequal across groups, as a function of \(\sigma_1\) and \(\sigma_2\), for a constant \(SD\)-ratio.

\hypertarget{in-summary}{%
\subsubsection{In summary}\label{in-summary}}

The \textbf{bias} of Cohen's \(d\) is a function of the population effect size \(\delta_{Cohen}\) and the total sample size (\(N\)) :

\begin{itemize}
\tightlist
\item
  When \(\delta_{Cohen}\) is null, the bias is null. In all other configurations, the larger \(\delta_{Cohen}\), the more Cohen's \(d\) will overestimate \(\delta_{Cohen}\);\\
\item
  The bias decreases when the total sample size increases (it does not matter whether one adds subjects in only one group or in both).
\end{itemize}

The \textbf{bias} of Glass's \(d\) is a function of the population effect size (\(\delta_{Glass}\)) and the size of the control group (\(n_e\)) :

\begin{itemize}
\tightlist
\item
  When \(\delta_{Glass}\) is null, the bias is null. In all other configurations, the larger \(\delta_{Glass}\), the more Glass's \(d\) will overestimate \(\delta_{Glass}\);\\
\item
  The bias decreases when the size of the control group increases. On the other hand, increasing the size of the experimental group does not impact the bias.
\end{itemize}

The \textbf{bias} of Cohen's \(d^*\) is a function of the population effect size (\(\delta^*_{Cohen}\)), the total sample size, and the interaction between the sample sizes ratio and the \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\) :

\begin{itemize}
\tightlist
\item
  When \(\delta^*_{Cohen}\) is null, the bias is null. In all other configurations, the larger \(\delta^*_{Cohen}\), the more Cohen's \(d^*\) will overestimate \(\delta^*_{Cohen}\);\\
\item
  The bias decreases when the total sample size increases (it does not matter whether one adds subjects in only one group or in both);
\item
  The smallest bias always occurs when there is a positive pairing between \(\frac{\sigma_2}{\sigma_1}\) and \(\frac{n_2}{n_1}\). Moreover, the larger the \(SD\)-ratio, the further from 1 is the sample sizes ratio associated with the smallest bias.
\end{itemize}

The \textbf{bias} of Shieh's \(d\) is a function of the population effect size (\(\delta_{Shieh}\)), the total sample size, and the interaction between the sample sizes ratio and the \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\):

\begin{itemize}
\tightlist
\item
  When \(\delta_{Shieh}\) is null, the bias is null. In all other configurations, the larger \(\delta_{Shieh}\), the more Shieh's \(d\) will overestimate \(\delta_{Shieh}\);\\
\item
  For a constant sample sizes ratio, the bias decreases when the total sample size increases;\\
\item
  The smallest bias always occurs when there is a positive pairing between \(\frac{\sigma_2}{\sigma_1}\) and \(\frac{n_2}{n_1}\). Moreover, the larger the \(SD\)-ratio, the further from 1 is the sample sizes ratio associated with the smallest bias (for more details, see ``Theoretical Bias, as a function of population parameters'').
\end{itemize}

\newpage

\hypertarget{theoretical-variance}{%
\subsection{Theoretical variance}\label{theoretical-variance}}

Note : while we focus on the theoretical variance of biased estimators (Cohen's \(d\), Glass's \(d\), Shieh's \(d\) and Cohen's \(d^*\)) when the normality assumption is met, it is interesting to notice that our main conclusions seem to generalize to biased estimators when samples are extracted from symmetric distributions. Moreover, unbiased estimators depend on the same factors as biased estimators, so our conclusions remain similar for unbiased estimators when samples are extracted from heavy-tailed symmetric distributions.

\hypertarget{cohens-bmd}{%
\subsubsection{\texorpdfstring{Cohen's \(\bm{d}\)}{Cohen's \textbackslash bm\{d\}}}\label{cohens-bmd}}

\hypertarget{when-variances-are-equal-across-populations-2}{%
\paragraph{When variances are equal across populations}\label{when-variances-are-equal-across-populations-2}}

\hypertarget{when-delta_cohen0}{%
\subparagraph{\texorpdfstring{When \(\delta_{Cohen}=0\)}{When \textbackslash delta\_\{Cohen\}=0}}\label{when-delta_cohen0}}

When the population effect size is zero, the variance of Cohen's \(d\) can be simplified as follows:
\[Var_{Cohen's \; d} = \frac{N(N-2)}{n_1n_2(N-4)}\]
The \textbf{variance} of Cohen's \(d\) is a function of total sample size (\(N\)) and the sample sizes allocation ratio (\(\frac{n_2}{n_1}\)) :

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the variance. The variance tends to zero when the total sample size tends to infinity (see Figure SM1.22);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.22.png}
\emph{Figure SM1.22} : Variance of Cohen's \(d\), when variances are equal across groups, as a function of the total sample size (\(N\)).

\newpage

\begin{itemize}
\tightlist
\item
  The further the sample sizes allocation ratio is from 1, the larger the variance (see Figure SM1.23).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.23.png}
\emph{Figure SM1.23} : Variance of Cohen's \(d\), when variances are equal across groups, as a function of the logarithm of the sample sizes ratio (\(log\left(\frac{n_2}{n_1} \right)\)).

\hypertarget{when-delta_cohenneq-0}{%
\subparagraph{\texorpdfstring{When \(\delta_{Cohen}\neq 0\)}{When \textbackslash delta\_\{Cohen\}\textbackslash neq 0}}\label{when-delta_cohenneq-0}}

While the variance of Cohen's \(d\) still depends on the total sample size and the sample sizes allocation ratio, it also depends on the population effect size (\(\delta_{Cohen}\)). The larger the population effect size, the larger the variance. Note that the effect of the population effect size decreases when sample sizes increase since
\[\lim_{n_1\rightarrow \infty}\left[\frac{df}{df-2} - \left( \frac{\sqrt{\frac{df}{2}} \times \Gamma \left(\frac{df-1}{2} \right)}{\Gamma \left( \frac{df}{2}\right)}\right)^2 \right]=0\]\\
\[\lim_{n_2\rightarrow \infty}\left[\frac{df}{df-2} - \left( \frac{\sqrt{\frac{df}{2}} \times \Gamma \left(\frac{df-1}{2} \right)}{\Gamma \left( \frac{df}{2}\right)}\right)^2 \right]=0\]\\
\[\lim_{N\rightarrow \infty}\left[\frac{df}{df-2} - \left( \frac{\sqrt{\frac{df}{2}} \times \Gamma \left(\frac{df-1}{2} \right)}{\Gamma \left( \frac{df}{2}\right)}\right)^2 \right]=0\]

This is illustrated in Figure SM1.24.

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.24.png}
\emph{Figure SM1.24} : Effect size moderator, when computing the variance of Cohen's \(d\), as a function of \(n_1\) (left), \(n_2\) (center) and \(N=n_1+n_2\) (right).

\hypertarget{in-summary-1}{%
\paragraph{In summary}\label{in-summary-1}}

The variance of Cohen's \(d\) is a function of the population effect size (\(\delta_{Cohen}\)), the total sample size (\(N\)) and the sample sizes ratio (\(\frac{n_2}{n_1}\)) :

\begin{itemize}
\tightlist
\item
  The variance decreases when the total sample size increases;\\
\item
  The variance also decreases when the sample sizes ratio gets closer to 1;\\
\item
  Finally, the variance increases when \(\delta_{Cohen}\) increases. Note that the effect of \(\delta_{Cohen}\) is moderated by the total sample size (the larger \(N\), the smaller the effect of \(\delta_{Cohen}\) on the variance).
\end{itemize}

\hypertarget{glasss-bmd}{%
\subsubsection{\texorpdfstring{Glass's \(\bm{d}\)}{Glass's \textbackslash bm\{d\}}}\label{glasss-bmd}}

\hypertarget{when-variances-are-equal-across-populations-3}{%
\paragraph{When variances are equal across populations}\label{when-variances-are-equal-across-populations-3}}

\hypertarget{when-delta_glass0}{%
\subparagraph{\texorpdfstring{When \(\delta_{Glass}=0\)}{When \textbackslash delta\_\{Glass\}=0}}\label{when-delta_glass0}}

When the population effect size is zero, the variance of Glass's \(d\) can be simplified as follows:
\[Var_{Glass's \; d} = \frac{n_c-1}{n_c-3} \left( \frac{1}{n_c}+\frac{1}{n_e}\right)\] In this configuration, the \textbf{variance} of Glass's \(d\) is a function of the sample sizes of both control (\(n_c\)) and experimental (\(n_e\)) groups as well as of the sample sizes allocation ratio \(\left( \frac{n_c}{n_e}\right)\) :

\begin{itemize}
\tightlist
\item
  The larger the sample sizes, the lower the variance (Figure SM1.25);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.25.png}
\emph{Figure SM1.25} : Variance of Glass's \(d\), when variances are equal across groups, as a function of the total sample size (\(N\)).

The sample sizes ratio associated with the lowest variance is not exactly 1 (because of the term \(\frac{df}{df-2}\), \(df\) depending only on \(n_c\)), but is very close to 1 (and the larger the total sample size, the closer to 1 is the sample sizes ratio associated with the lowest variance). The further from this sample size ratio, the larger the variance (see Figure SM1.26).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.26.png}
\emph{Figure SM1.26} : Variance of Glass's \(d\), when variances are equal across groups, as a function of the logarithm of the sample sizes ratio (\(log\left(\frac{n_c}{n_e} \right)\)).

\hypertarget{when-delta_glass-neq-0}{%
\subparagraph{\texorpdfstring{When \(\delta_{Glass} \neq 0\)}{When \textbackslash delta\_\{Glass\} \textbackslash neq 0}}\label{when-delta_glass-neq-0}}

While the variance of Glass's \(d\) still depends on the total sample size and the sample sizes allocation ratio, it also depends on the population effect size (\(\delta_{Glass}\)). The larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the control group increases. On the other hand, the effect of the population effect size does \emph{not} depend on the size of the experimental group since\\
\[\lim_{n_c\rightarrow \infty}\left[\frac{df}{df-2} - \left( \frac{\sqrt{\frac{df}{2}} \times \Gamma \left(\frac{df-1}{2} \right)}{\Gamma \left( \frac{df}{2}\right)}\right)^2 \right]=0\]
\[\lim_{n_e\rightarrow \infty}\left[\frac{df}{df-2} - \left( \frac{\sqrt{\frac{df}{2}} \times \Gamma \left(\frac{df-1}{2} \right)}{\Gamma \left( \frac{df}{2}\right)}\right)^2 \right] \neq 0\]

These limits are illustrated in Figure SM1.27.

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.27.png}
\emph{Figure SM1.27} : Effect size moderator, when computing the variance of Glass's \(d\), as a function of the size of the control group (left) and experimental group (right).

Note : while the sample sizes ratio associated with the lowest variance was very close to 1 with a null population effect size, this is not true anymore when the population effect size is not zero. Indeed, because of the second term in the addition, when computing the variance, one gives much more weight to the effect size of the control group (see Figure SM1.28), especially when the effect size gets larger. For example, when \(\delta_{Glass}\)= 4, the lowest variance will occur when \(n_c\) is approximately 3 times larger than \(n_e\). When \(\delta_{Glass}\)= 7, the lowest variance will occur when \(n_c\) is approximately 5 times larger than \(n_e\), etc.

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.28.png}
\emph{Figure SM1.28} : Variance of Glass's \(d\), when variances are equal across groups, as a function of the logarithm of the sample sizes ratio (\(log\left(\frac{n_c}{n_e} \right)\)) when \(\delta_{Glass}\) equals 4 (left) or 7 (right).

\hypertarget{when-variances-are-unequal-across-populations-with-equal-sample-sizes-2}{%
\paragraph{When variances are unequal across populations, with equal sample sizes}\label{when-variances-are-unequal-across-populations-with-equal-sample-sizes-2}}

\hypertarget{when-delta_glass-0}{%
\subparagraph{\texorpdfstring{When \(\delta_{Glass} = 0\)}{When \textbackslash delta\_\{Glass\} = 0}}\label{when-delta_glass-0}}

When the population effect size is zero, the variance of Glass's \(d\) can be simplified as follows:
\[Var_{Glass's \; d} = \frac{n-1}{n(n-3)} \left( 1+\frac{\sigma^2_e}{\sigma^2_c}\right)\]
where \(n=N/2=\) sample size of each group. The variance is therefore a function of the total sample size and the \(SD\)-ratio (\(\frac{\sigma_c}{\sigma_e}\)) :

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the variance (See Figure SM1.29);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.29.png}
\emph{Figure SM1.29} : Variance of Glass's \(d\), when variances are unequal across groups and sample sizes are equal, as a function of the total sample sizes (\(N\)).

\newpage

\begin{itemize}
\tightlist
\item
  The larger the \(SD\)-ratio (i.e.~the larger is \(\sigma_c\) in comparison with \(\sigma_e\)), the lower the variance (see Figure SM1.30). However, the effect of the \(SD\)-ratio decreases when sample sizes increase, because \(\lim_{n(=n_c=n_e)\rightarrow \infty}\left[\frac{df}{n(df-2)} \right]=0\).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.30.png}
\emph{Figure SM1.30} : Variance of Glass's \(d\), when variances are unequal across groups and sample sizes are equal, as a function of the logarithm of the \(SD\)-ratio (\(log \left( \frac{\sigma_c}{\sigma_e} \right)\)).

\hypertarget{when-delta_glass-neq-0-1}{%
\subparagraph{\texorpdfstring{When \(\delta_{Glass} \neq 0\)}{When \textbackslash delta\_\{Glass\} \textbackslash neq 0}}\label{when-delta_glass-neq-0-1}}

While the variance of Glass's \(d\) still depends on the total sample size and the \(SD\)-ratio, it also depends on the population effect size (\(\delta_{Glass}\)). The larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the control group increases, as previously explained and illustrated in Figure SM1.27.

\newpage

\hypertarget{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-2}{%
\paragraph{When variances are unequal across populations, with unequal sample sizes}\label{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-2}}

\hypertarget{when-delta_glass-0-1}{%
\subparagraph{\texorpdfstring{When \(\delta_{Glass} = 0\)}{When \textbackslash delta\_\{Glass\} = 0}}\label{when-delta_glass-0-1}}

When the population effect size is zero, the variance of Glass's \(d\) can be simplified as follows:
\[Var_{Glass's \; d} = \frac{n_c-1}{n_c-3} \left( \frac{1}{n_c}+\frac{\sigma^2_e}{n_e\sigma^2_c}\right)\]
The variance of Glass's \(d\) is therefore a function of the total sample size (\(N\)), the \(SD\)-ratio and the interaction between the sample sizes ratio and the \(SD\)-ratio \(\left(\frac{n_c}{n_e}\times\frac{\sigma_c}{\sigma_e} \right)\):

\begin{itemize}
\tightlist
\item
  For any \(SD\) and sample sizes pairing, increasing \(n_c\) and/or \(n_e\) will decrease the variance (see Figure SM1.31);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.31.png}
\emph{Figure SM1.31} : Variance of Glass's \(d\), when variances and sample sizes are unequal across groups, as a function of the total sample sizes, when increasing only the control (right) or the experimental (left) group, when \(\sigma_c > \sigma_e\) (top plots) or \(\sigma_c < \sigma_e\) (bottom plots).

\begin{itemize}
\tightlist
\item
  The effect of the sample sizes ratio depends on the \(SD\)-ratio :

  \begin{itemize}
  \tightlist
  \item
    We previously mentioned that when \(\sigma_c=\sigma_e\), the variance is minimized when sample sizes of both groups are almost identical (see Figure SM1.26), meaning that it is more efficient, in order to reduce variance, to add subjects uniformly in both groups;\\
  \item
    When \(\sigma_e > \sigma_c\), more weight is given to \(n_e\), meaning that it is more efficient, in order to reduce variance, to add subjects in the experimental group (\(n_e\); see bottom plots in Figure M1.31);\\
  \item
    When \(\sigma_c > \sigma_e\), less weight is given to \(n_e\), meaning that it is more efficient, in order to reduce variance, to add sujects in the control group (\(n_c\); see top plots in Figure SM1.31).
  \end{itemize}
\end{itemize}

\newpage

\begin{itemize}
\tightlist
\item
  Finally, there is also a main effect of the \(SD\)-ratio : the larger is \(\sigma_c\) in comparison with \(\sigma_e\), the lower the variance, as we can observe in Figure SM1.32. We can also notice that in Figure SM1.31, the maximum variance is much larger in the two bottom plots (where \(\sigma_c<\sigma_e\)) than in the two top plots (where \(\sigma_c>\sigma_e\)).\\
  \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.32.png}
  \emph{Figure SM1.32} : Variance of Glass's \(d\), when sample sizes and variances are unequal across groups, as a function of the logarithm of the \(SD\)-ratio (\(log \left( \frac{\sigma_c}{\sigma_e} \right)\)).
\end{itemize}

Note that the effect of the \(SD\)-ratio, and the interaction effect between \(SD\)-ratio and sample sizes ratio decreases when the sample size of the control group increases (because \(\frac{n_c-1}{n_c-3}\) gets closer to 1).

\hypertarget{when-delta_glass-neq-0-2}{%
\subparagraph{\texorpdfstring{When \(\delta_{Glass} \neq 0\)}{When \textbackslash delta\_\{Glass\} \textbackslash neq 0}}\label{when-delta_glass-neq-0-2}}

While the variance of Glass's \(d\) still depends on the total sample size, the \(SD\)-ratio and the interaction between the \(SD\)-ratio and the sample sizes ratio, it also depends on the population effect size (\(\delta_{Glass}\)) : the larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the sample size of the control group increases, as previously explained and illustrated in Figure SM1.27.

Note : when the population effect size was null, when \(\sigma_c<\sigma_e\), it was much more efficient to add subjects in the experimental group in order to reduce the variance (because much more weight was given to \(n_e\)). When \(\delta_{Glass} \neq 0\), it is important to add subjects in both groups in order to reduce the variance (because \(\frac{df}{df-2} - \left( \frac{\sqrt{\frac{df}{2}} \times \Gamma \left(\frac{df-1}{2} \right)}{\Gamma \left( \frac{df}{2}\right)}\right)^2\) is only a function of the sample size of the control group). With huge population effect size, it is even always more important to add subjects in the control group (e.g.~when \(\delta_{Glass}=30\)).

\hypertarget{in-summary-2}{%
\paragraph{In summary}\label{in-summary-2}}

The variance of Glass's \(d\) is a function of the population effect size (\(\delta_{Glass}\)), the \(SD\)-ratio, the total sample size and the interaction between sample sizes ratio and \(SD\)-ratio \(\left(\frac{n_c}{n_e}\times\frac{\sigma_c}{\sigma_e} \right)\) :

\begin{itemize}
\tightlist
\item
  The variance decreases when the \(SD\)-ratio increases (i.e.~when \(\sigma_e >> \sigma_c\));\\
\item
  The variance always decreases when the control and/or the experimental group increases. The benefit of adding subjects rather in the control, in the experimental, or in both groups, in order to reduce the variance, varies as a function of the \(SD\)-ratio and the population effect size. The only situation where it is optimal to maximize the experimental group is when \(\sigma_e > \sigma_c\) and \(\delta_{Glass} \approx 0\). Most of the time, it is more efficient to maximize the control groups (e.g.~anytime \(\sigma_e < \sigma_c\), and when \(\delta_{Glass}\) is very large) or to uniformly add subjects in both groups (e.g.~when \(\sigma_e > \sigma_c\) and \(\delta_{Glass}\) is neither null nor huge);
\item
  The variance increases when \(\delta_{Glass}\) increases. Note that the effect of \(\delta_{Glass}\) is moderated by the control group size (the larger \(n_e\), the smaller the effect of \(\delta_{Glass}\) on the variance).
\end{itemize}

\newpage

\hypertarget{cohens-bmd-1}{%
\subsubsection{\texorpdfstring{Cohen's \(\bm{d^*}\)}{Cohen's \textbackslash bm\{d\^{}*\}}}\label{cohens-bmd-1}}

\hypertarget{when-variances-are-equal-across-populations-4}{%
\paragraph{When variances are equal across populations}\label{when-variances-are-equal-across-populations-4}}

\hypertarget{when-delta_cohen-0}{%
\subparagraph{\texorpdfstring{When \(\delta^*_{Cohen} = 0\)}{When \textbackslash delta\^{}*\_\{Cohen\} = 0}}\label{when-delta_cohen-0}}

When the population effect size is zero, the variance of Cohen's \(d^*\) is computed as follows :
\[Var_{Cohen's \; d^*} = \frac{df}{df-2} \times \frac{N}{n_1n_2}\]
with \[df = \frac{4(n_1-1)(n_2-1)}{n_1+n_2-2}\]
In this configuration, the degrees of freedom as well as the variance of Cohen's \(d^*\) depend on the total sample size (\(N\)) and the sample sizes allocation ratio (\(\frac{n_2}{n_1}\)):

\newpage

\begin{itemize}
\tightlist
\item
  The further the sample sizes allocation ratio is from 1, the larger the variance (see Figure SM1.33);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.33.png}
\emph{Figure SM1.33} : Variance of Cohen's \(d^*\) when variances are equal across groups, as a function of the logarithm of the sample sizes ratio (\(log\left(\frac{n_2}{n_1} \right)\)).

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the bias (see Figure SM1.34).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.34.png}
\emph{Figure SM1.34} : Variance of Cohen's \(d^*\) when variances are equal across groups, as a function of the total sample size (\(N\)).

\newpage

\hypertarget{when-delta_cohen-neq-0}{%
\subparagraph{\texorpdfstring{When \(\delta^*_{Cohen} \neq 0\)}{When \textbackslash delta\^{}*\_\{Cohen\} \textbackslash neq 0}}\label{when-delta_cohen-neq-0}}

While the variance of Cohen's \(d^*\) still depends on the total sample size and the sample sizes ratio, it also depends on the population effect size (\(\delta^*_{Cohen}\)) : the larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the degrees of freedom increase (i.e.~when sample sizes increase and/or the sample sizes ratio get closer to 1), as illustrated in Figure SM1.35 since
\[\lim_{df\rightarrow \infty}\left[\frac{df}{df-2} - \left( \frac{\sqrt{\frac{df}{2}} \times \Gamma \left(\frac{df-1}{2} \right)}{\Gamma \left( \frac{df}{2}\right)}\right)^2 \right]=0\]

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.35.png}
\emph{Figure SM1.35} : Effect size moderator (for all estimators), as a function of the degrees of freedom.

\newpage

\hypertarget{when-variances-are-unequal-across-populations-with-equal-sample-sizes-3}{%
\paragraph{When variances are unequal across populations, with equal sample sizes}\label{when-variances-are-unequal-across-populations-with-equal-sample-sizes-3}}

\hypertarget{when-delta_cohen-0-1}{%
\subparagraph{\texorpdfstring{When \(\delta^*_{Cohen} = 0\)}{When \textbackslash delta\^{}*\_\{Cohen\} = 0}}\label{when-delta_cohen-0-1}}

When the population effect size is zero, the variance of Cohen's \(d^*\) can be simplified as follows:
\[Var_{Cohen's \; d^*} = \frac{df}{df-2} \times \frac{2}{n}\]
where \(n=N/2\)=sample size of each group, and \(df=\frac{(n-1)(\sigma^4_1+\sigma^4_2+2\sigma^2_1\sigma^2_2)}{\sigma^4_1+\sigma^4_2}\). In this configuration, the degrees of freedom as well as the variance of Cohen's \(d^*\) depend on the total sample size (\(N\)) and the \(SD\)-ratio \(\left( \frac{\sigma_2}{\sigma_1}\right)\):

\begin{itemize}
\tightlist
\item
  The further the \(SD\)-ratio is from 1, the larger the variance (see Figure SM1.36);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.36.png}
\emph{Figure SM1.36} : Variance of Cohen's \(d^*\) when variances are unequal across groups and sample sizes are equal, as a function of the logarithm of the \(SD\)-ratio (\(log \left( \frac{\sigma_2}{\sigma_1} \right)\)).

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the variance (see Figure SM1.37).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.37.png}
\emph{Figure SM1.37} : Variance of Cohen's \(d^*\) when variances are unequal across groups and sample sizes are equal, as a function of the total sample size (\(N\)).

\newpage

Note : for a constant \(SD\)-ratio, the size of the variance does not matter (see Figure SM1.38).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.38.png}
\emph{Figure SM1.38} : Variance of Cohen's \(d^*\), when variances are unequal across groups and sample sizes are equal, as a function of \(\sigma_1\) and \(\sigma_2\), for a constant \(SD\)-ratio.

\hypertarget{when-delta_cohen-neq-0-1}{%
\subparagraph{\texorpdfstring{When \(\delta^*_{Cohen} \neq 0\)}{When \textbackslash delta\^{}*\_\{Cohen\} \textbackslash neq 0}}\label{when-delta_cohen-neq-0-1}}

While the variance of Cohen's \(d^*\) still depends on the total sample size and the \(SD\)-ratio, it also depends on the population effect size (\(\delta^*_{Cohen}\)) : the larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the degrees of freedom increase (i.e.~when the total sample size increases and/or the \(SD\)-ratio get closer to 1), as previously illustrated in Figure SM1.35.

\newpage

\hypertarget{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-3}{%
\paragraph{When variances are unequal across populations, with unequal sample sizes}\label{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-3}}

\hypertarget{when-delta_cohen-0-2}{%
\subparagraph{\texorpdfstring{When \(\delta^*_{Cohen} = 0\)}{When \textbackslash delta\^{}*\_\{Cohen\} = 0}}\label{when-delta_cohen-0-2}}

When the population effect size is zero, the variance of Cohen's \(d^*\) can be simplified as follows:
\[Var_{Cohen's \; d^*} = \frac{df}{df-2} \times \frac{2\left( \frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2} \right)}{\sigma^2_1+\sigma^2_2}\]
with \(df =\frac{(n_1-1)(n_2-1)(\sigma^2_1+\sigma^2_2)^2}{(n_2-1)\sigma_1^4+(n_1-1)\sigma_2^4}\). In this configuration, the degrees of freedom are a function of the total sample size (\(N\)) and the interaction between sample sizes ratio and the \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\):

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the variance (illustration in Figure SM1.39);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.39.png}
\emph{Figure SM1.39} : Variance of Cohen's \(d^*\) when variances and sample sizes are unequal across groups, as a function of the total sample size (\(N\)).

\begin{itemize}
\tightlist
\item
  The smallest variance always occurs when there is a positive pairing between variances and sample sizes, because one gives more weight to the smallest variance in the denominator of the \(df\) computation and in the numerator of the variance computation. Moreover, the further the \(SD\)-ratio is from 1, the further from 1 will also be the sample sizes ratio associated with the smallest variance (see Figure SM1.40). This can be explained by splitting the numerator and the denominator of the \(df\) computation (see the file ``Theoretical Bias, as a function of population parameters'').
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.40.png}
\emph{Figure SM1.40} : The variance of Cohen's \(d^*\), when variances and sample sizes are unequal across groups, as a function of the logarithm of the sample sizes ratio (\(log \left( \frac{n_2}{n_1} \right)\)), when \(SD\)-ratio equals .68 (first row), .29 (second row) or .14 (third row).

\newpage

Note : for a constant \(SD\)-ratio, the variance does not matter. (See Figure SM1.41).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.41.png}
\emph{Figure SM1.41} : Variance of Cohen's \(d^*\), when variances and sample sizes are unequal across groups, as a function of \(\sigma_1\) and \(\sigma_2\), for a constant \(SD\)-ratio.

\hypertarget{when-delta_cohen-neq-0-2}{%
\subparagraph{\texorpdfstring{When \(\delta^*_{Cohen} \neq 0\)}{When \textbackslash delta\^{}*\_\{Cohen\} \textbackslash neq 0}}\label{when-delta_cohen-neq-0-2}}

While the variance of Cohen's \(d^*\) still depends on the total sample size, the \(SD\)-ratio and the interaction between the sample sizes ratio and the \(SD\)-ratio, it also depends on the population effect size (\(\delta^*_{Cohen}\)) : the larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the degrees of freedom increase (i.e.~when the total sample size increases and/or when there is a positive pairing between the sample sizes ratio and the \(SD\)-ratio), as previously illustrated in Figure SM1.35.

\hypertarget{in-summary-3}{%
\paragraph{In summary}\label{in-summary-3}}

The variance of Cohen's \(d^*\) is a function of the population effect size (\(\delta^*_{Cohen}\)), the total sample size (\(N\)) and the interaction between sample sizes ratio and \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\) :

\begin{itemize}
\tightlist
\item
  The variance always decreases when the control and/or the experimental group increases. The benefit of adding subjects rather in the control or in the experimental group depends on the \(SD\)-ratio. Indeed, the smallest variance always occurs when there is a positive pairing between variances and sample sizes. Moreover, the further the \(SD\)-ratio is from 1, the further from 1 will also be the sample sizes ratio associated with the smallest variance;\\
\item
  The variance increases when \(\delta^*_{Cohen}\) increases. Note that the effect of \(\delta^*_{Cohen}\) is moderated by the total sample size and the interaction between sample sizes ratio and \(SD\)-ratio.
\end{itemize}

\hypertarget{shiehs-bmd}{%
\subsubsection{\texorpdfstring{Shieh's \(\bm{d}\)}{Shieh's \textbackslash bm\{d\}}}\label{shiehs-bmd}}

\hypertarget{when-variances-are-equal-across-populations-5}{%
\paragraph{When variances are equal across populations}\label{when-variances-are-equal-across-populations-5}}

\hypertarget{when-delta_shieh-0}{%
\subparagraph{\texorpdfstring{When \(\delta_{Shieh} = 0\)}{When \textbackslash delta\_\{Shieh\} = 0}}\label{when-delta_shieh-0}}

When the population effect size is zero, the variance of Shieh's \(d\) can be simplified as follows:
\[Var_{Shieh's \; d} = \frac{df}{(df-2)N}\]
with \(df = \frac{N^2(n_1-1)(n_2-1)}{n_2^2(n_2-1)+n_1^2(n_1-1)}\). In this configuration, the degrees of freedom as well as the variance of Shieh's \(d\) depend on the total sample size (\(N\)) and the sample sizes allocation ratio (\(\frac{n_2}{n_1}\)):

\newpage

\begin{itemize}
\tightlist
\item
  The further the sample sizes allocation ratio is from 1, the larger the variance (see Figure SM1.42);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.42.png}
\emph{Figure SM1.42} : Variance of Shieh's \(d\) when variances are equal across groups, as a function of the logarithm of the sample sizes ratio (\(log\left(\frac{n_2}{n_1} \right)\)).

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the variance. It does not matter whether the sample sizes ratio is constant (see Figure SM1.43) or not (see Figure SM1.44).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.43.png}
\emph{Figure SM1.43} : Variance of Shieh's \(d\) when variances are equal across groups, as a function of the total sample size (\(N\)), for a constant sample sizes ratio (\(log\left(\frac{n_2}{n_1} \right)\)).

\newpage

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.44.png}
\emph{Figure SM1.44} : Variance of Shieh's \(d\) when variances are equal across groups, as a function of the total sample size (\(N\)), when adding subjects only in one group (either in the first group; see top plots; or in the second group; see bottom plots).

Note : in ``Theoretical Bias,'' we noticed that moving the sample sizes ratio away from 1 when adding subjects in only one group could decrease the degrees of freedom. However, due to the total sample size term (\(N\)) in the denominator of the variance computation, even when degrees of freedom decrease due to the fact that one adds subjects only in one group, the variance still decreases (because the denominator of the variance computation increases; see Figure SM1.44).

\hypertarget{when-delta_shieh-neq-0}{%
\subparagraph{\texorpdfstring{When \(\delta_{Shieh} \neq 0\)}{When \textbackslash delta\_\{Shieh\} \textbackslash neq 0}}\label{when-delta_shieh-neq-0}}

While the variance of Shieh's \(d\) still depends on the total sample size and the sample sizes ratio, it also depends on the population effect size (\(\delta_{Shieh}\)) : the larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the degrees of freedom increase (i.e.~when sample sizes increase, without increasing the sample sizes ratio, and/or the sample sizes ratio gets closer to 1), as previously illustrated in Figure SM1.35.

Note : we previously noticed that when the effet size is zero, the variance of Shieh's \(d\) decreases, even when the sample sizes ratio increases. It is no longer true when there is a non-null effect size because the larger the sample sizes ratio, the more the variance will increase with increasing effect size.

\newpage

\hypertarget{when-variances-are-unequal-across-populations-with-equal-sample-sizes-4}{%
\paragraph{When variances are unequal across populations, with equal sample sizes}\label{when-variances-are-unequal-across-populations-with-equal-sample-sizes-4}}

\hypertarget{when-delta_shieh-0-1}{%
\subparagraph{\texorpdfstring{When \(\delta_{Shieh} = 0\)}{When \textbackslash delta\_\{Shieh\} = 0}}\label{when-delta_shieh-0-1}}

When the population effect size is zero, the variance of Shieh's \(d\) can be simplified as follows:
\[Var_{Shieh's \; d} = \frac{df}{(df-2)N}\]
with \(df = \frac{(\sigma_1^2+\sigma_2^2)^2 \times (n-1)}{\sigma_1^4+\sigma_2^4}\). In this configuration, the degrees of freedom as well as the variance of Shieh's \(d\) depend on the total sample size (\(N\)) and the \(SD\)-ratio (\(\frac{\sigma_2}{\sigma_1}\)).

\begin{itemize}
\tightlist
\item
  The further the \(SD\)-ratio is from 1, the larger the variance (see Figure SM1.45);
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.45.png}
\emph{Figure SM1.45} : Variance of Shieh's \(d\) when variances are unequal across groups and sample sizes are equal, as a function of the logarithm of the \(SD\)-ratio (\(log \left( \frac{\sigma_2}{\sigma_1} \right)\)).

\newpage

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the variance (see Figure SM1.46).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.46.png}
\emph{Figure SM1.46} : Variance of Shieh's \(d\) when variances are unequal across groups and sample sizes are equal, as a function of the total sample size (\(N\)).

\newpage

Note : for a constant \(SD\)-ratio, the size of the variance does not matter (see Figure SM1.47).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.47.png}
\emph{Figure SM1.47} : Variance of Shieh's \(d\), when variances are unequal across groups and sample sizes are equal, as a function of \(\sigma_1\) and \(\sigma_2\), for a constant \(SD\)-ratio.

\hypertarget{when-delta_shieh-neq-0-1}{%
\subparagraph{\texorpdfstring{When \(\delta_{Shieh} \neq 0\)}{When \textbackslash delta\_\{Shieh\} \textbackslash neq 0}}\label{when-delta_shieh-neq-0-1}}

While the variance of Shieh's \(d\) still depends on the total sample size and the \(SD\)-ratio, it also depends on the population effect size (\(\delta_{Shieh}\)) : the larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the degrees of freedom increase (i.e.~when sample sizes increase and/or the \(SD\)-ratio gets closer to 1), as previously illustrated in Figure SM1.35.

\newpage

\hypertarget{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-4}{%
\paragraph{When variances are unequal across populations, with unequal sample sizes}\label{when-variances-are-unequal-across-populations-with-unequal-sample-sizes-4}}

\hypertarget{when-delta_shieh-0-2}{%
\subparagraph{\texorpdfstring{When \(\delta_{Shieh} = 0\)}{When \textbackslash delta\_\{Shieh\} = 0}}\label{when-delta_shieh-0-2}}

When the population effect size is zero, the variance of Shieh's \(d\) can be simplified as follows:
\[Var_{Shieh's \; d} = \frac{df}{(df-2)N}\]
with \(df = \frac{\left(\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2} \right)^2}{\frac{(\sigma^2_1/n_1)^2}{n_1-1}+\frac{(\sigma^2_2/n_2)^2}{n_2-1}}\). In this configuration, the degrees of freedom as well as the variance of Shieh's \(d\) depend on the total sample size (\(N\)) and the interaction between the sample sizes ratio and the \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\):

\begin{itemize}
\tightlist
\item
  The larger the total sample size, the lower the variance. It remains true when the sample sizes ratio is constant (see Figure SM1.48) and when it is not (see Figure SM1.49).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.48.png}
\emph{Figure SM1.48} : Variance of Shieh's \(d\) when variances and sample sizes are unequal across groups, as a function of the total sample size (\(N\)), for a constant sample sizes ratio (\(log\left(\frac{n_2}{n_1} \right)\)).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.49.png}
\emph{Figure SM1.49} : Variance of Shieh's \(d\) when variances and sample sizes are unequal across groups, as a function of the total sample size (\(N\)), when adding subjects only in one group (either in the first group; see left plots; or in the second group; see right plots), and \(\sigma_1 > \sigma_2\) (top plots) or \(\sigma_1 < \sigma_2\) (bottom plots).

Note : When variances were equal across populations, adding subjects only in the first group had the same impact on the variance as adding subjects only in the second group (see Figure SM1.34). When variances are unequal across groups, this is not true anymore (see Figure SM1.49).

\newpage

\begin{itemize}
\tightlist
\item
  The smallest variance always occurs when there is a positive pairing between variances and sample size. Moreover, the further the \(SD\)-ratio is from 1, the further from 1 will also be the sample sizes ratio associated with the smallest variance (see Figure SM1.50).
\end{itemize}

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.50.png}
\emph{Figure SM1.50} : The variance of Shieh's \(d\), when variances and sample sizes are unequal across groups, as a function of the logarithm of the sample sizes ratio (\(log \left( \frac{n_2}{n_1} \right)\)), when \(SD\)-ratio equals .68 (first row), .29 (second row) or .14 (third row).

\newpage

Moreover, for a constant \(SD\)-ratio, the variances do not matter (See Figure SM1.51).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM1chp4/Figure SM1.51.png}
\emph{Figure SM1.51} : Variance of Shieh's \(d\), when variances and sample sizes are unequal across groups, as a function of \(\sigma_1\) (left) or \(\sigma_2\) (right), for a constant \(SD\)-ratio.

\hypertarget{when-delta_shieh-neq-0-2}{%
\subparagraph{\texorpdfstring{When \(\delta_{Shieh} \neq 0\)}{When \textbackslash delta\_\{Shieh\} \textbackslash neq 0}}\label{when-delta_shieh-neq-0-2}}

While the variance of Shieh's \(d\) still depends on the total sample size and the interaction between the sample sizes ratio and the \(SD\)-ratio, it also depends on the population effect size (\(\delta_{Shieh}\)) : the larger the population effect size, the larger the variance. However, the effect of the population effect size decreases when the degrees of freedom increase, as previously illustrated in Figure SM1.35.

\newpage

\hypertarget{in-summary-4}{%
\paragraph{In summary}\label{in-summary-4}}

The variance of Shieh's \(d\) is a function of the population effect size (\(\delta_{Shieh}\)), the total sample size (\(N\)) and the interaction between sample sizes ratio and \(SD\)-ratio \(\left(\frac{n_2}{n_1}\times\frac{\sigma_2}{\sigma_1} \right)\) :

\begin{itemize}
\item
  The variance always decreases when the control and/or the experimental group increases. The benefit of adding subjects rather in the control or in the experimental group depends on the \(SD\)-ratio. Indeed, the smallest variance always occurs when there is a positive pairing between variances and sample sizes. Moreover, the further the \(SD\)-ratio is from 1, the further from 1 will also be the sample sizes ratio associated with the smallest variance;
\item
  The variance increases when \(\delta_{Shieh}\) increases. Note that the effect of \(\delta_{Shieh}\) is moderated by the total sample size and the interaction between the sample sizes ratio and the \(SD\)-ratio.
\end{itemize}

\newpage

\hypertarget{supplemental-material-2-simulation-checks}{%
\section{Supplemental Material 2 : Simulation checks}\label{supplemental-material-2-simulation-checks}}

In order to insure the reliability of our calculation method, for all scenarios where \(G_1=G_2=0\), we compared empirical means and variances of all estimators (i.e.~means and variances of all estimates) with theoretical means and variances (i.e.~expected means and variances, computed based on equations in Tables 2, 3 and 4 in the main article). Because we can draw exactly the same conclusions for \textbf{biased} (Cohen's \(d\), Glass's \(d\) using either \(S_1\) or \(S_2\) as standardizer, Shieh's \(d\) and Cohen's \(d^*\)) and \textbf{unbiased} (Hedges' \(g\), Glass's \(g\) using either \(S_1\) or \(S_2\) as standardizer, Shieh's \(g\) and Hedges' \(g^*\)) estimators, we will simultaneously present results for both categories of estimators. Results will be subdivided into 4 conditions :\\
- When population variances and sample sizes are equal across groups (condition a; see Tables SM2.1 and SM2.5 for respectively biased and unbiased estimators);\\
- When population variances are equal across groups and sample sizes are unequal (condition b; see Tables SM2.2 and SM2.6 for respectively biased and unbiased estimators);\\
- When population variances are unequal across groups and sample sizes are equal (condition c; see Tables SM2.3 and SM2.7 for respectively biased and unbiased estimators);\\
- When population variances and sample sizes are unequal across groups (condition d; see Tables SM2.4 and SM2.8 for respectively biased and unbiased estimators).

Because the equations of theoretical means and variances of Cohen's \(d\) and Hedges' \(g\) rely on the assumption of normality and equality of population variances, we expect empirical and theoretical parameters to be very close only in conditions a and b. For all other estimators, the equations of theoretical means and variances rely solely on the assumption of normality and therefore, we expect empirical and theoretical parameters to be very close in all conditions.

On average, empirical means (and variances) of all estimators are very close to theoretical expectations when population variances are equal across groups, with equal sample sizes (condition a; see Tables SM2.1 and SM2.5) or unequal sample sizes (condition b; see Tables SM2.2 and SM2.6).

When population variances are unequal across groups (conditions c and d; see Tables SM2.3, SM2.4, SM2.7 and SM2.8), empirical means (and variances) of Cohen's \(d^*\) (Hedges' \(g^*\)) and Shieh's \(d\) (Shieh's \(g\)) are still very close to theoretical expectations. Regarding Glass's \(d\) (Glass's \(g\)), on average, while empirical variances remain very close to theoretical expectations, one observes larger departures between empirical and theoretical means when using \(S_2\) as standardizer. However, when looking at details in results for each scenario (see ``biased estimator\_condition C.xlsx,'' ``biased estimator\_condition D.xlsx,'' ``unbiased estimator\_condition C.xlsx'' and ``unbiased estimator\_condition C.xlsx'' in Supplemental Material 2), one notices that the larger the population effect size, the larger the departure between empirical and theoretical means, and that relative to the population effect size, departures between empirical and theoretical means are always very small. On the other hand, both empirical bias and variance of Cohen's \(d\) (Hedges' \(g\)) highly depart from theoretical expectations, even when looking at relative departures to the population effect size, especially when sample sizes are unequal across groups (condition d; see Tables SM2.4 and SM2.8), which is not surprising, as Cohen's \(d\) (Hedges' \(g\)) relies on the equality of population variances assumption.

\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.1} 

}

\end{figure}

\end{landscape}
\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.2} 

}

\end{figure}

\end{landscape}
\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.3} 

}

\end{figure}

\end{landscape}
\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.4} 

}

\end{figure}

\end{landscape}
\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.5} 

}

\end{figure}

\end{landscape}
\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.6} 

}

\end{figure}

\end{landscape}
\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.7} 

}

\end{figure}

\end{landscape}
\begin{landscape}
\newpage

\begin{figure}

{\centering \includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM2chp4/Table SM2.8} 

}

\end{figure}

\end{landscape}

\newpage

\hypertarget{supplemental-material-3-correlation-between-sample-means-and-bmsd}{%
\section{\texorpdfstring{Supplemental Material 3 : correlation between sample means and \(\bm{SD}\)}{Supplemental Material 3 : correlation between sample means and \textbackslash bm\{SD\}}}\label{supplemental-material-3-correlation-between-sample-means-and-bmsd}}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

The \emph{d}-family effect sizes are commonly used with between-subject designs where individuals are randomly assigned into one of two independent groups and group means are compared. The population effect size is defined as
\begin{equation*} 
\delta = \frac{\mu_{1}-\mu_{2}}{\sigma} 
\label{eq:Cohendelta}
\end{equation*}
where both populations follow a normal distribution with mean \(\mu_j\) in the \(j^{th}\) population (\(j\)=1,2) and common standard deviation \(\sigma\). There exist different estimators of this population effect size, varying as a function of the chosen standardizer. When the equality of variances assumption is met, \(\sigma\) is estimated by pooling both sample standard deviations (\(S_1\) and \(S_2\)) :
\begin{equation*} 
S_{Cohen's \; d} = \sqrt{\frac{(n_1-1) \times S_1^2+(n_2-1) \times S_2^2}{n_1+n_2-2}}
\label{eq:Cohends}
\end{equation*}\\
When the equality of variances assumption is not met, we are considering three alternative estimates :\\
- Using the standard deviation of the control group (\(S_c\)) as standardizer :
\begin{equation*} 
S_{Glass's \; d} = S_{c}
\label{eq:Glassds}
\end{equation*}\\
- Using a standardizer that takes the sample sizes allocation ratio \(\left( \frac{n_2}{n_1}\right)\) into account :\\
\begin{equation*}  
S_{Shieh's \; d} = \sqrt{S_1^2/q_1+S_2^2/q_2}; \;\;\; q_j=\frac{n_j}{N} (j=1,2)
\label{eq:Shiehds}
\end{equation*}\\
- Or using the square root of the non pooled average of both variance estimates (\(S^2_1\) and \(S^2_2\)) as standardizer :
\begin{equation*} 
S_{Cohen's \; d^*} = \sqrt{\frac{\left(S^2_{1}+S^2_{2} \right)}{2}}
\label{eq:cohenprimeds}
\end{equation*}\\
As we previously mentioned, the use of these formulas requires to meet the assumption of normality. Using them when distributions are not normal will have consequences on both bias and variance of all estimators. More specifically, when samples are extracted from skewed distributions, correlations might occur between the sample mean difference (\(\bar{X_1}-\bar{X_2}\)) and standardizers (\(S\)). Throughout this Supplemental Material, we will study when these correlations occur. To this end, we will distinguish 3 situations :\\
- when \(\sigma_1=\sigma_2\) and \(n_1=n_2\) (condition a);\\
- when \(\sigma_1=\sigma_2\) and \(n_1\neq n_2\) (condition b);\\
- when \(\sigma_1 \neq \sigma_2\) and \(n_1 = n_2\) (condition c).

Before studying conditions a, b and c, we will briefly introduce the impact of these correlations on the bias. Note that we will compute correlations using the coefficient of Spearman's \(\rho\). We decided to use Spearman's \(\rho\) instead of Pearson's \(\rho\) because some plots revealed non-perfectly linear relations.

\hypertarget{how-correlations-between-the-mean-difference-bmbarx_1-barx_2-and-standardizers-affect-the-bias-of-estimators.}{%
\subsection{\texorpdfstring{How correlations between the mean difference (\(\bm{\bar{X_1}-\bar{X_2}}\)) and standardizers affect the bias of estimators.}{How correlations between the mean difference (\textbackslash bm\{\textbackslash bar\{X\_1\}-\textbackslash bar\{X\_2\}\}) and standardizers affect the bias of estimators.}}\label{how-correlations-between-the-mean-difference-bmbarx_1-barx_2-and-standardizers-affect-the-bias-of-estimators.}}

When population distributions are right-skewed, there is a positive (negative) correlation between \(S_1\) (\(S_2\)) and (\(\bar{X_1}-\bar{X_2}\)). When distributions are left-skewed, there is a negative (positive) correlation between \(S_1\) (\(S_2\)) and (\(\bar{X_1}-\bar{X_2}\)). When the population mean difference (\(\mu_1-\mu_2\)) is positive (like in our simulations), all other parameters being equal, an estimator is always less biased and variable when choosing a standardizer that is positively correlated with \(\bar{X_1}-\bar{X_2}\) than when choosing an estimator that is negatively correlated with \(\bar{X_1}-\bar{X_2}\). When the population mean difference is negative, the reverse is true.

``All other parameters being equal'' is mentioned because it is always possible that other factors in action have an opposite effect on bias and variance in order that increasing the magnitude of the correlation between \(S_j\) and \(\bar{X_1}-\bar{X_2}\) does not necessarily reduce the bias and the variance. For example, when population variances are equal across groups and sample sizes are unequal, we will see below that the lower \(n_j\), the larger the magnitude of the correlation between \(S_j\) and \(\bar{X_1}-\bar{X_2}\). When the correlation between \(S_j\) and \(\bar{X_1}-\bar{X_2}\) is positive, the smaller the sample size, the larger the positive correlation. At the same time, we know that increasing the sample size decreases the bias. This is a nice example of situations where two factors might have an opposite action on bias.

\hypertarget{correlations-between-the-mean-difference-bmbarx_1-barx_2-and-all-standardizers}{%
\subsection{\texorpdfstring{Correlations between the mean difference (\(\bm{\bar{X_1}-\bar{X_2}}\)) and all standardizers}{Correlations between the mean difference (\textbackslash bm\{\textbackslash bar\{X\_1\}-\textbackslash bar\{X\_2\}\}) and all standardizers}}\label{correlations-between-the-mean-difference-bmbarx_1-barx_2-and-all-standardizers}}

\hypertarget{when-equal-population-variances-are-estimated-based-on-equal-sample-sizes-condition-a}{%
\subsubsection{When equal population variances are estimated based on equal sample sizes (condition a)}\label{when-equal-population-variances-are-estimated-based-on-equal-sample-sizes-condition-a}}

While \(\bar{X_j}\) and \(S_j\) (\(j\)=1,2) are uncorrelated when samples are extracted from symmetric distributions (see Figure SM3.1), there is a non-null correlation between \(\bar{X_j}\) and \(S_j\) when distributions are skewed (Zhang, 2007).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.1.png}

\setlength\parindent{0pt}\emph{Figure SM3.1} : \(S_j\) as a function of \(\bar{X_j}\) (\(j\)=1,2), when samples are extracted from symmetric distributions (\(\gamma_1 = 0\)).

\setlength\parindent{24pt}More specifically, when distributions are right-skewed, there is a \textbf{positive} correlation between \(\bar{X_j}\) and \(S_j\) (see the two top plots in Figure \ref{fig:pltSDHombalRskew}), resulting in a \emph{positive} correlation between \(S_1\) and \(\bar{X_1}-\bar{X_2}\) and in a \emph{negative} correlation between \(S_2\) and \(\bar{X_1}-\bar{X_2}\) (see the two bottom plots in Figure SM3.2). This can be explained by the fact that \(\bar{X_1}\) and \(\bar{X_1}-\bar{X_2}\) are positively correlated while \(\bar{X_2}\) and \(\bar{X_1}-\bar{X_2}\) are negatively correlated (of course, correlations would be trivially reversed if we computed \(\bar{X_2}-\bar{X_1}\) instead of \(\bar{X_1}-\bar{X_2}\)).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.2.png}

\setlength\parindent{0pt}\emph{Figure SM3.2} : \(S_j\) (\(j\)=1,2) as a function of \(\bar{X_j}\) (top plots) or \(\bar{X_1}-\bar{X_2}\) (bottom plots), when samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\)).

\newpage

\setlength\parindent{24pt}One should also notice that both correlations between \(S_j\) and \(\bar{X_1}-\bar{X_2}\) are equal, in absolute terms (possible tiny differences might be observed due to sampling error in our simulations). As a consequence, when computing a standardizer taking both \(S_1\) and \(S_2\) into account, it results in a standardizer that is uncorrelated with \(\bar{X_1}-\bar{X_2}\) (see Figure SM3.3).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.3.png}

\setlength\parindent{0pt}\emph{Figure SM3.3} : \(S_{Cohen's \; d}\), \(S_{Shieh's \; d}\) and \(S_{Cohen's \; d^*}\) as a function of the mean difference (\(\bar{X_1}-\bar{X_2}\)), when samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\)).

\newpage

\setlength\parindent{24pt}On the other hand, when distributions are left-skewed, there is a \textbf{negative} correlation between \(\bar{X_j}\) and \(S_j\) (see the two top plots in Figure SM3.4), resulting in a \emph{negative} correlation between \(S_1\) and \(\bar{X_1}-\bar{X_2}\) and in a \emph{positive} correlation between \(S_2\) and \(\bar{X_1}-\bar{X_2}\) (see the two bottom plots in Figure SM3.4).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.4.png}

\setlength\parindent{0pt}\emph{Figure SM3.4} : \(S_j\) (\(j\)=1,2) as a function of \(\bar{X_j}\) (top plots) or \(\bar{X_1}-\bar{X_2}\) (bottom plots), when samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)).

\newpage

\setlength\parindent{24pt}Again, because correlations between \(S_j\) and \(\bar{X_1}-\bar{X_2}\) are similar in absolute terms, any standardizers taking both \(S_1\) and \(S_2\) into account will be uncorrelated with \(\bar{X_1}-\bar{X_2}\) (see Figure SM3.5).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.5.png}

\setlength\parindent{0pt}\emph{Figure SM3.5} : \(S_{Cohen's \; d}\), \(S_{Shieh's \; d}\) and \(S_{Cohen's \; d^*}\) as a function of the mean difference (\(\bar{X_1}-\bar{X_2}\)), when samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)).

\newpage

\hypertarget{when-equal-population-variances-are-estimated-based-on-unequal-sample-sizes-condition-b}{%
\subsubsection{When equal population variances are estimated based on unequal sample sizes (condition b)}\label{when-equal-population-variances-are-estimated-based-on-unequal-sample-sizes-condition-b}}

\setlength\parindent{24pt}When distributions are skewed, there are again non-null correlations between \(\bar{X_j}\) and \(S_j\), however \(cor(S_1,\bar{X_1}) \neq cor(S_2,\bar{X_2})\), because of the different sample sizes.

When distributions are skewed, one observes that the larger the sample size, the lower the correlation between \(S_j\) and \(\bar{X_j}\) (See Figures SM3.6 and SM3.7).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.6.png}
\setlength\parindent{0pt}\emph{Figure SM3.6} : Correlation between \(S_j\) and \(\bar{X_j}\) when \(n\) = 25, 50, 75 or 100 and samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\)).

\newpage

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.7.png}
\emph{Figure SM3.7} : Correlation between \(S_j\) and \(\bar{X_j}\) when \(n\) = 25, 50, 75 or 100 and samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)).

\newpage

\setlength\parindent{24pt}This might explain that the magnitude of the correlation between \(S_j\) and \(\bar{X_1}-\bar{X_2}\) is lower in the larger sample (see bottom plots in Figures SM3.8 and SM3.9). With no surprise, there is a positive (negative) correlation between \(S_1\) and \(\bar{X_1}-\bar{X_2}\) and a negative (positive) correlation between \(S_2\) and \(\bar{X_1}-\bar{X_2}\) when distributions are right-skewed (left-skewed), as illustrated in the two bottom plots of Figures SM3.8 and SM3.9.

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.8.png}
\setlength\parindent{0pt}\emph{Figure SM3.8} : \(S_j\) (\(j\)=1,2) as a function of \(\bar{X_j}\) (top plots) or \(\bar{X_1}-\bar{X_2}\) (bottom plots), when samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\)), with \(n_1\)=20 and \(n_2\)=100.

\newpage

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.9.png}
\emph{Figure SM3.9} : \(S_j\) (\(j\)=1,2) as a function of \(\bar{X_j}\) (top plots) or \(\bar{X_1}-\bar{X_2}\) (bottom plots), when samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)), with \(n_1\)=20 and \(n_2\)=100.

\setlength\parindent{24pt}This might also explain that standardizers of Shieh's \(d\) and Cohen's \(d^*\) are \textbf{correlated} with \(\bar{X_1}-\bar{X_2}\) (see Figures SM3.10 and SM3.11) :\\
- When computing \(S_{Cohen's \; d^*}\), the same weight is given to both \(S_1\) and \(S_2\). Therefore, it does not seem surprising that the sign of the correlation between \(S_{Cohen's \; d^*}\) and \(\bar{X_1}-\bar{X_2}\) is the same as the size of the correlation between \(\bar{X_1}-\bar{X_2}\) and the \(SD\) of the smallest sample;\\
- When computing \(S_{Shieh's \; d}\), more weight is given to the \(SD\) of the smallest sample, it is therefore not really surprising to observe that the correlation between \(S_{Shieh's \; d}\) and \(\bar{X_1}-\bar{X_2}\) is closer of the correlation between the \(SD\) of the smallest group and \(\bar{X_1}-\bar{X_2}\) (i.e.~\(|cor(S_{Shieh's \; d},\bar{X_1}-\bar{X_2})| > |cor(S_{Cohen's \; d^*},\bar{X_1}-\bar{X_2})|\));\\
- When computing \(S_{Cohen's \; d}\), more weight is given to the \(SD\) of the largest sample, which by compensation effect brings the correlation very close to 0.

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.10.png}
\setlength\parindent{0pt}\emph{Figure SM3.10} : \(S_{Cohen's \; d}\), \(S_{Shieh's \; d}\) and \(S_{Cohen's \; d^*}\) as a function of the mean difference (\(\bar{X_1}-\bar{X_2}\)), when samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\), with \(n_1\)=20 and \(n_2\)=100).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.11.png}
\emph{Figure SM3.11} : \(S_{Cohen's \; d}\), \(S_{Shieh's \; d}\) and \(S_{Cohen's \; d^*}\) as a function of the mean difference (\(\bar{X_1}-\bar{X_2}\)), when samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)), with \(n_1\)=20 and \(n_2\)=100.

\setlength\parindent{24pt}The correlation between \(\bar{X_1}-\bar{X_2}\) and respectively \(S_1\), \(S_2\), the standardizer of Cohen's \(d^*\), the standardizer of Shieh's \(d\) and the standardizer of Cohen's \(d\) are summarized in Table 1.

\newpage

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.29}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.32}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.39}}@{}}
\caption{Correlation between standardizers (\(S_1\), \(S_2\), \(S_{Cohen's \; d}\), \(S_{Shieh's \; d}\) and \(S_{Cohen's \; d^*}\)) and \(\bar{X_1}-\bar{X_2}\), when samples are extracted from skewed distributions with equal variances, and \(n_1=n_2\) (condition a) or \(n_1 \neq n_2\) (condition b)}\tabularnewline
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{\textbf{population distribution}}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{\textbf{population distribution}}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
& \emph{right-skewed} & \emph{left-skewed} \\
& ---------------------------------- & ----------------------------------------- \\
When \(n_1=n_2\) & \(S_1\) : \emph{positive} & \(S_1\) : \emph{negative} \\
& \(S_2\) : \emph{negative} & \(S_2\) : \emph{positive} \\
& \(S_{Cohen's \; d}\) : \emph{null} & \(S_{Cohen's \; d}\) : \emph{null} \\
& \(S_{Shieh's \; d}\) : \emph{null} & \(S_{Shieh's \; d}\) : \emph{null} \\
& \(S_{Cohen's \; d^*}\) : \emph{null} & \(S_{Cohen's \; d^*}\) : \emph{null} \\
& & \\
When \(n_1>n_2\) & \(S_1\) : \emph{positive} & \(S_1\) : \emph{negative} \\
& \(S_2\) : \emph{negative} & \(S_2\) : \emph{positive} \\
& \(S_{Cohen's \; d}\) : \emph{null} & \(S_{Cohen's \; d}\) : \emph{null} \\
& \(S_{Shieh's \; d}\) : \emph{negative} & \(S_{Shieh's \; d}\) : \emph{positive} \\
& \(S_{Cohen's \; d^*}\) : \emph{positive (but very small)} & \(S_{Cohen's \; d^*}\) : \emph{negative (but very small)} \\
& & \\
When \(n_1<n_2\) & \(S_1\) : \emph{positive} & \(S_1\) : \emph{negative} \\
& \(S_2\) : \emph{negative} & \(S_2\) : \emph{positive} \\
& \(S_{Cohen's \; d}\) : \emph{negative (but very small)} & \(S_{Cohen's \; d}\) : \emph{positive (but very small)} \\
& \(S_{Shieh's \; d}\) : \emph{positive} & \(S_{Shieh's \; d}\) : \emph{negative} \\
& \(S_{Cohen's \; d^*}\) : \emph{positive} & \(S_{Cohen's \; d^*}\) : \emph{negative} \\
\bottomrule
\end{longtable}

\hypertarget{when-unequal-population-variances-are-estimated-based-on-equal-sample-sizes-condition-c}{%
\subsubsection{When unequal population variances are estimated based on equal sample sizes (condition c)}\label{when-unequal-population-variances-are-estimated-based-on-equal-sample-sizes-condition-c}}

When distributions are skewed, there are again non-null correlations between \(\bar{X_j}\) and \(S_j\). As illustrated in Figures SM3.12 and SM3.13, the correlation remains the same for any population \(SD\) (\(\sigma\)). However, the magnitude of the correlation between \(S_j\) and \(\bar{X_1}-\bar{X_2}\) differs : it is stronger in the sample extracted from the larger population variance (see Figures SM3.14 and SM3.15).
\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.12.png}
\setlength\parindent{0pt}\emph{Figure SM3.12} : Correlation between \(S_j\) and \(\bar{X_j}\) when \(SD\) = 2, 4, 6 or 8 and samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\)).

\newpage

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.13.png}
\emph{Figure SM3.13} : Correlation between \(S_j\) and \(\bar{X_j}\) when \(SD\) = 2, 4, 6 or 8 and samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)).

\newpage

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.14.png}
\emph{Figure SM3.14} : \(S_j\) (\(j\)=1,2) as a function of \(\bar{X_j}\) (top plots) or \(\bar{X_1}-\bar{X_2}\) (bottom plots), when samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\)), with \(S_1\)=2 and \(S_2\)=4.

\newpage

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.15.png}
\emph{Figure SM3.15} : \(S_j\) (\(j\)=1,2) as a function of \(\bar{X_j}\) (top plots) or \(\bar{X_1}-\bar{X_2}\) (bottom plots), when samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)), with \(S_1\)=2 and \(S_2\)=4.

\newpage

\setlength\parindent{24pt}This also explains that when computing a standardizer that takes both \(S_1\) and \(S_2\) into account, it results in a standardizer that is correlated with \(\bar{X_1}-\bar{X_2}\) (see Figures SM3.16 and SM3.17). The correlation between the mean difference (\(\bar{X_1}-\bar{X_2}\)) and respectively the standardizer of Shieh's \(d\), Cohen's \(d^*\) and Cohen's \(d\) will have the same sign as the correlation between (\(\bar{X_1}-\bar{X_2}\)) and the larger \(SD\). Table 2 summarizes the sign of the correlation between \(\bar{X_1}-\bar{X_2}\) and respectively \(S_1\), \(S_2\) and the three standardizers taking both \(S_1\) and \(S_2\) into account (see ``Others'' in the Table).

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.16.png}
\emph{Figure SM3.16} : \(S_{Cohen's \; d}\), \(S_{Shieh's \; d}\) and \(S_{Cohen's \; d^*}\) as a function of the mean difference (\(\bar{X_1}-\bar{X_2}\)), when samples are extracted from right skewed distributions (\(\gamma_1 = 6.32\)), with \(S_1\)=2 and \(S_2\)=4.

\newpage

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM3chp4/Figure SM3.17.png}
\emph{Figure SM3.17} : \(S_{Cohen's \; d}\), \(S_{Shieh's \; d}\) and \(S_{Cohen's \; d^*}\) as a function of the mean difference (\(\bar{X_1}-\bar{X_2}\)), when samples are extracted from left skewed distributions (\(\gamma_1 = -6.32\)), with \(S_1\)=2 and \(S_2\)=4.

\newpage

\begin{longtable}[]{@{}lcc@{}}
\caption{Correlation between standardizers (\(S_1\), \(S_2\) and others) and \(\bar{X_1}-\bar{X_2}\), when samples are extracted from skewed distributions with equal sample sizes, as a function of the SD-ratio.}\tabularnewline
\toprule
& \textbf{\textbf{population distribution}} & \\
\midrule
\endfirsthead
\toprule
& \textbf{\textbf{population distribution}} & \\
\midrule
\endhead
& \emph{right-skewed} & \emph{left-skewed} \\
& --------------------------- & --------------------------- \\
When \(\sigma_1=\sigma_2\) & \(S_1\) : \emph{positive} & \(S_1\) : \emph{negative} \\
& \(S_2\) : \emph{negative} & \(S_2\) : \emph{positive} \\
& Others : \emph{null} & Others : \emph{null} \\
& & \\
When \(\sigma_1>\sigma_2\) & \(S_1\) : \emph{positive} & \(S_1\) : \emph{negative} \\
& \(S_2\) : \emph{negative} & \(S_2\) : \emph{positive} \\
& Others : \emph{positive} & Others : \emph{negative} \\
& & \\
When \(\sigma_1<\sigma_2\) & \(S_1\) : \emph{positive} & \(S_1\) : \emph{negative} \\
& \(S_2\) : \emph{negative} & \(S_2\) : \emph{positive} \\
& Others : \emph{negative} & Others : \emph{positive} \\
& & \\
\bottomrule
\end{longtable}

\newpage

\hypertarget{supplemental-material-4-reminder-about-confidence-intervals}{%
\section{Supplemental Material 4 : Reminder about Confidence Intervals}\label{supplemental-material-4-reminder-about-confidence-intervals}}

\hypertarget{how-to-compute-a-confidence-interval-around-a-point-estimator}{%
\subsection{How to compute a confidence interval around a point estimator}\label{how-to-compute-a-confidence-interval-around-a-point-estimator}}

As illustration, we will explain how to compute a confidence interval around Cohen's \(d\) (the explanation would be very similar for all other estimators). Under the assumption of \(iid\) normal distribution of residuals with equal population variances across groups, in order to test the null hypothesis that \(\mu_1-\mu_2= (\mu_1-\mu_2)_0\), we can compute the following quantity :
\begin{equation*} 
t_{Student}=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}
\label{eq:tstudent}
\end{equation*}
with \(SE = S_{pooled} \times \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\), \(S_{pooled} = \sqrt{\frac{(n_1-1) \times S^2_1+(n_2-1)\times S^2_2}{n_1+n_2-2}}\) and \(S_j\) = the standard deviation of the \(j^{th}\) sample (\(j=1,2\)). Under the null hypothesis, this quantity will follow a central \emph{t}-distribution with \(n_1+n_2-2\) degrees of freedom. However, when the null hypothesis is false, the distribution of this quantity is not centered and a noncentral \emph{t}-distribution arises (Cumming \(\&\) Finch,2001), as illustrated in Figure SM4.1. Noncentral \emph{t}-distributions are described by two parameters: degrees of freedom (\(df\)) and noncentrality parameter (that we will call \(\Delta\); Cumming \(\&\) Finch,2001), the last being a function of the population effect size (\(\delta\)) and sample sizes (\(n_1\) and \(n_2\)) :
\begin{equation*}
\Delta = \frac{\delta}{\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}
\label{eq:ncp}
\end{equation*}
with \(\delta=\frac{(\mu_1-\mu_2)-(\mu_1-\mu_2)_0}{\sigma_{pooled}}\), \(\sigma_{pooled} = \sqrt{\frac{(n_1-1) \times \sigma^2_1+(n_2-1)\times \sigma^2_2}{n_1+n_2-2}}\) and \(\sigma_j\) = the standard deviation of the \(j^{th}\) population (\(j=1,2\)). Considering the link between \(\Delta\) and \(\delta\), it is possible to compute confidence limits for \(\Delta\) and multiply them by \(\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\) in order to have confidence limits for \(\delta\). In other words, we first need to determine the noncentrality parameters of the \emph{t}-distributions for which \(t_{Student}\) corresponds respectively to the quantiles \(\left(1-\frac{\alpha}{2}\right)\) and \(\frac{\alpha}{2}\) :
\[P[t_{df, \Delta_L} \geq t_{Student}] = \frac{\alpha}{2} \]
\[P[t_{df, \Delta_U} \leq t_{Student}] = \frac{\alpha}{2} \]
with \(df = n_1+n_2-2\). Second, we multiply \(\Delta_L\) and \(\Delta_U\) by \(\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\) in order to define \(\delta_L\) and \(\delta_U\) :
\[\delta_L = \Delta_L \times \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\]
\[\delta_U = \Delta_U \times \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\]

\includegraphics{C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 4/SupMat/SM4chp4/Figure SM4.1.png}

\setlength\parindent{0pt}\emph{Figure SM4.1} : Sampling distribution of \(t=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}\) when the null hypothesis is true (in grey) and when the null hypothesis is false, with \((\mu_1-\mu_2)-(\mu_1-\mu_2)_0=4\) and \(SE=5\) (in red).


\end{document}
