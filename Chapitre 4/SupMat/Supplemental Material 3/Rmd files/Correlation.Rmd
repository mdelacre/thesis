---
title             : "Correlations between the sample mean difference and standardizers of all estimators, and implications on biases and variances of all estimators"
shorttitle        : "Correlation between sample means and SD"

author: 
  - name          : "Marie Delacre" 
    affiliation   : "1"
    corresponding : yes    
    address       : "CP191, avenue F.D. Roosevelt 50, 1050 Bruxelles"
    email         : "marie.delacre@ulb.be"
  - name          : "Daniel Lakens"
    affiliation   : "2"
  - name          : "Christophe Ley"
    affiliation   : "3"
  - name          : "Limin Liu"
    affiliation   : "3"
  - name          : "Christophe Leys"
    affiliation   : "1"
    
affiliation:
  - id            : "1"
    institution   : "Universit√© Libre de Bruxelles, Service of Analysis of the Data (SAD), Bruxelles, Belgium"
  - id            : "2"
    institution   : "Eindhoven University of Technology, Human Technology Interaction Group, Eindhoven, the Netherlands"
  - id            : "3"
    institution   : "Universiteit Gent, Department of Applied Mathematics, Computer Science and Statistics, Gent, Belgium"

wordcount         : "1434 words"

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : yes
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf

header-includes:
  - \usepackage{bm}
---

```{r setup, include = FALSE}
library("PearsonDS")
```

# Introduction

The *d*-family effect sizes are commonly used with between-subject designs where individuals are randomly assigned into one of two independent groups and group means are compared. The population effect size is defined as 
\begin{equation*} 
\delta = \frac{\mu_{1}-\mu_{2}}{\sigma} 
(\#eq:Cohendelta)
\end{equation*} 
where both populations follow a normal distribution with mean $\mu_j$ in the $j^{th}$ population ($j$=1,2) and common standard deviation $\sigma$. There exist different estimators of this population effect size, varying as a function of the chosen standardizer. When the equality of variances assumption is met, $\sigma$ is estimated by pooling both sample standard deviations ($S_1$ and $S_2$):
\begin{equation*} 
S_{Cohen's \; d} = \sqrt{\frac{(n_1-1) \times S_1^2+(n_2-1) \times S_2^2}{n_1+n_2-2}}
(\#eq:Cohends)
\end{equation*}   
When the equality of variances assumption is not met, we are considering three alternative estimates:   
- Using the standard deviation of the control group ($S_c$) as standardizer: 
\begin{equation*} 
S_{Glass's \; d} = S_{c}
(\#eq:Glassds)
\end{equation*}   
- Using a standardizer that takes the sample sizes allocation ratio $\left( \frac{n_2}{n_1}\right)$ into account:   
\begin{equation*}  
S_{Shieh's \; d} = \sqrt{S_1^2/q_1+S_2^2/q_2}; \;\;\; q_j=\frac{n_j}{N} (j=1,2)
(\#eq:Shiehds)
\end{equation*}   
- Or using the square root of the non pooled average of both variance estimates ($S^2_1$ and $S^2_2$) as standardizer:
\begin{equation*} 
S_{Cohen's \; d^*} = \sqrt{\frac{\left(S^2_{1}+S^2_{2} \right)}{2}}
(\#eq:cohenprimeds)
\end{equation*}   
As we previously mentioned, the use of these formulas requires to meet the assumption of normality. Using them when distributions are not normal will have consequences on both bias and variance of all estimators. More specifically, when samples are extracted from skewed distributions, correlations might occur between the sample mean difference ($\bar{X_1}-\bar{X_2}$) and standardizers ($S$). Throughout this Supplemental Material, we will study when these correlations occur. To this end, we will distinguish 3 situations:   
- when $\sigma_1=\sigma_2$ and $n_1=n_2$ (condition a);  
- when $\sigma_1=\sigma_2$ and $n_1\neq n_2$ (condition b);  
- when $\sigma_1 \neq \sigma_2$ and $n_1 = n_2$ (condition c).

Before studying conditions a, b and c, we will briefly introduce the impact of these correlations on the bias. Note that we will compute correlations using the coefficient of Spearman's $\rho$. We decided to use Spearman's $\rho$ instead of Pearson's $\rho$ because some plots revealed non-perfectly linear relations. 

# How correlations between the mean difference ($\bm{\bar{X_1}-\bar{X_2}}$) and standardizers affect the bias of estimators.

When population distributions are right-skewed, there is a positive (negative) correlation between $S_1$ ($S_2$) and ($\bar{X_1}-\bar{X_2}$). When distributions are left-skewed, there is a negative (positive) correlation between $S_1$ ($S_2$) and ($\bar{X_1}-\bar{X_2}$).  When the population mean difference ($\mu_1-\mu_2$) is positive (like in our simulations), all other parameters being equal, an estimator is always less biased and variable when choosing a standardizer that is positively correlated with $\bar{X_1}-\bar{X_2}$ than when choosing an estimator that is negatively correlated with $\bar{X_1}-\bar{X_2}$. When the population mean difference is negative, the reverse is true. 

"All other parameters being equal" is mentioned because it is always possible that other factors in action have an opposite effect on bias and variance in order that increasing the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ does not necessarily reduce the bias and the variance. For example, when population variances are equal across groups and sample sizes are unequal, we will see below that the lower $n_j$, the larger the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$. When the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ is positive, the smaller the sample size, the larger the positive correlation. At the same time, we know that increasing the sample size decreases the bias. This is a nice example of situations where two factors might have an opposite action on bias. 

# Correlations between the mean difference ($\bm{\bar{X_1}-\bar{X_2}}$) and all standardizers

## When equal population variances are estimated based on equal sample sizes (condition a)

```{r Hombal,include=FALSE}

n1 <- 20
n2 <- 20
n <- n1
N <- n1+n2

corrHombal=function(sd,nSims=10000,m1=1,m2=0,n,skew,kurt=95.75){
   
   sd1<-rep(0,nSims)
   sd2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(1,nSims)
   mean2<-rep(0,nSims)
   
   y=rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
   
   for (i in 1:nSims){

      y1 <- sample(y,size=n,replace=TRUE)
      y2 <- sample(y,size=n,replace=TRUE)
      sd1[i] <- sd(y1)
      sd2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
   }
   
   return(data.frame(sd1,sd2,meandiff,mean1,mean2))
}

Hombal_sym <- corrHombal(sd=2,n=n,skew=0)
Hombal_rightskew <- corrHombal(sd=2,n=n,skew=6.32)
Hombal_leftskew <- corrHombal(sd=2,n=n,skew=-6.32)
```

```{r corHombalsym,echo=FALSE}
Hombal_sym_mean1 <- Hombal_sym$mean1
Hombal_sym_mean2 <- Hombal_sym$mean2
Hombal_sym_meandiff <- Hombal_sym$meandiff
Hombal_sym_sd1 <- Hombal_sym$sd1
Hombal_sym_sd2 <- Hombal_sym$sd2
Hombal_sym_sdCohen <- sqrt(((n1-1)*Hombal_sym_sd1^2+(n2-1)*Hombal_sym_sd2^2)/(N-2))
Hombal_sym_sdShieh <-  sqrt(Hombal_sym_sd1^2/(n1/N)+Hombal_sym_sd2^2/(n2/N))
Hombal_sym_sdCohenprime <- sqrt((Hombal_sym_sd1^2+Hombal_sym_sd2^2)/2)
```

```{r corHombalrightskew,echo=FALSE}
Hombal_rightskew_mean1 <- Hombal_rightskew$mean1
Hombal_rightskew_mean2 <- Hombal_rightskew$mean2
Hombal_rightskew_meandiff <- Hombal_rightskew$meandiff
Hombal_rightskew_sd1 <- Hombal_rightskew$sd1
Hombal_rightskew_sd2 <- Hombal_rightskew$sd2
Hombal_rightskew_sdCohen <- sqrt(((n1-1)*Hombal_rightskew_sd1^2+(n2-1)*Hombal_rightskew_sd2^2)/(N-2))
Hombal_rightskew_sdShieh <-  sqrt(Hombal_rightskew_sd1^2/(n1/N)+Hombal_rightskew_sd2^2/(n2/N))
Hombal_rightskew_sdCohenprime <- sqrt((Hombal_rightskew_sd1^2+Hombal_rightskew_sd2^2)/2)
```

```{r corHomballeftskew,echo=FALSE}
Hombal_leftskew_mean1 <- Hombal_leftskew$mean1
Hombal_leftskew_mean2 <- Hombal_leftskew$mean2
Hombal_leftskew_meandiff <- Hombal_leftskew$meandiff
Hombal_leftskew_sd1 <- Hombal_leftskew$sd1
Hombal_leftskew_sd2 <- Hombal_leftskew$sd2
Hombal_leftskew_sdCohen <- sqrt(((n1-1)*Hombal_leftskew_sd1^2+(n2-1)*Hombal_leftskew_sd2^2)/(N-2))
Hombal_leftskew_sdShieh <-  sqrt(Hombal_leftskew_sd1^2/(n1/N)+Hombal_leftskew_sd2^2/(n2/N))
Hombal_leftskew_sdCohenprime <- sqrt((Hombal_leftskew_sd1^2+Hombal_leftskew_sd2^2)/2)
```

```{r pltSDMEANHombalsym,fig.cap="$S_j$ as a function of $\\bar{X_j}$ ($j$=1,2), when samples are extracted from symmetric distributions ($\\gamma_1 = 0$)",echo=FALSE}
par(mfrow=c(1,2),mar=c(5,5,5,2))
plot(Hombal_sym_sd1,Hombal_sym_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(Hombal_sym_sd1,Hombal_sym_mean1,method="spearman"),2)))
plot(Hombal_sym_sd2,Hombal_sym_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(Hombal_sym_sd2,Hombal_sym_mean2,method="spearman"),2)))
```

While $\bar{X_j}$ and $S_j$ ($j$=1,2) are uncorrelated when samples are extracted from symmetric distributions (see Figure \ref{fig:pltSDMEANHombalsym}), there is a non-null correlation between $\bar{X_j}$ and $S_j$ when distributions are skewed (Zhang, 2007). 

```{r pltSDHombalRskew,fig.cap="$S_j$ ($j$=1,2) as a function of $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(Hombal_rightskew_sd1,Hombal_rightskew_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(Hombal_rightskew_sd1,Hombal_rightskew_mean1,method="spearman"),2)))
plot(Hombal_rightskew_sd2,Hombal_rightskew_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(Hombal_rightskew_sd2,Hombal_rightskew_mean2,method="spearman"),2)))

plot(Hombal_rightskew_sd1,Hombal_rightskew_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_rightskew_sd1,Hombal_rightskew_meandiff),2)))
plot(Hombal_rightskew_sd2,Hombal_rightskew_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_rightskew_sd2,Hombal_rightskew_meandiff),2)))
```

More specifically, when distributions are right-skewed, there is a **positive** correlation between $\bar{X_j}$ and $S_j$ (see the two top plots in Figure \ref{fig:pltSDHombalRskew}), resulting in a *positive* correlation between $S_1$ and $\bar{X_1}-\bar{X_2}$ and in a *negative* correlation between $S_2$ and $\bar{X_1}-\bar{X_2}$ (see the two bottom plots in Figure \ref{fig:pltSDHombalRskew}). This can be explained by the fact that $\bar{X_1}$ and $\bar{X_1}-\bar{X_2}$ are positively correlated while $\bar{X_2}$ and $\bar{X_1}-\bar{X_2}$ are negatively correlated (of course, correlations would be trivially reversed if we computed $\bar{X_2}-\bar{X_1}$ instead of $\bar{X_1}-\bar{X_2}$).

```{r pltStdzrHombalRskew,fig.cap="$S_{Cohen's \\; d}$, $S_{Shieh's \\; d}$ and $S_{Cohen's \\; d^*}$ as a function of the mean difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(Hombal_leftskew_sdCohen,Hombal_leftskew_meandiff,ylab=expression(S["Cohen's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_leftskew_sdCohen,Hombal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hombal_leftskew_sdShieh,Hombal_leftskew_meandiff,ylab=expression(S["Shieh's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_leftskew_sdShieh,Hombal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hombal_leftskew_sdCohenprime,Hombal_leftskew_meandiff,ylab=expression(S["Cohen's d*"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_leftskew_sdCohenprime,Hombal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
```

One should also notice that both correlations between $S_j$ and $\bar{X_1}-\bar{X_2}$ are equal, in absolute terms (possible tiny differences might be observed due to sampling error in our simulations). As a consequence, when computing a standardizer taking both $S_1$ and $S_2$ into account, it results in a standardizer that is uncorrelated with $\bar{X_1}-\bar{X_2}$ (see Figure \ref{fig:pltStdzrHombalRskew}).

```{r pltSDHombalLskew,fig.cap="$S_j$ ($j$=1,2) as a function of $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(Hombal_leftskew_sd1,Hombal_leftskew_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(Hombal_leftskew_sd1,Hombal_leftskew_mean1,method="spearman"),2)))
plot(Hombal_leftskew_sd2,Hombal_leftskew_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(Hombal_leftskew_sd2,Hombal_leftskew_mean2,method="spearman"),2)))

plot(Hombal_leftskew_sd1,Hombal_leftskew_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_leftskew_sd1,Hombal_leftskew_meandiff),2)))
plot(Hombal_leftskew_sd2,Hombal_leftskew_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_leftskew_sd2,Hombal_leftskew_meandiff),2)))
```

On the other hand, when distributions are left-skewed, there is a **negative** correlation between $\bar{X_j}$ and $S_j$ (see the two top plots in Figure \ref{fig:pltSDHombalLskew}), resulting in a *negative* correlation between $S_1$ and $\bar{X_1}-\bar{X_2}$ and in a *positive* correlation between $S_2$ and $\bar{X_1}-\bar{X_2}$ (see the two bottom plots in Figure \ref{fig:pltSDHombalLskew}).

```{r pltStdzrHombalLskew,fig.cap="$S_{Cohen's \\; d}$, $S_{Shieh's \\; d}$ and $S_{Cohen's \\; d^*}$ as a function of the mean difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(Hombal_rightskew_sdCohen,Hombal_rightskew_meandiff,ylab=expression(S["Cohen's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_rightskew_sdCohen,Hombal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hombal_rightskew_sdShieh,Hombal_rightskew_meandiff,ylab=expression(S["Shieh's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_rightskew_sdShieh,Hombal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hombal_rightskew_sdCohenprime,Hombal_rightskew_meandiff,ylab=expression(S["Cohen's d*"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hombal_rightskew_sdCohenprime,Hombal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
```

Again, because correlations between $S_j$ and $\bar{X_1}-\bar{X_2}$ are similar in absolute terms, any standardizers taking both $S_1$ and $S_2$ into account will be uncorrelated with $\bar{X_1}-\bar{X_2}$ (see Figure \ref{fig:pltStdzrHombalLskew}).

## When equal population variances are estimated based on unequal sample sizes (condition b)

```{r Homunbal,include=FALSE}

n1 <- 20
n2 <- 100
N <- n1+n2

corrHomunbal=function(sd,nSims=10000,m1=1,m2=0,n1,n2,skew,kurt=95.75){
   
   sd1<-rep(0,nSims)
   sd2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(0,nSims)
   mean2<-rep(0,nSims)
   
   # I generate a population using N in order to be sure that both samples are extacted from equal population skewness and kurtosis
   N <- n1+n2
   y=rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))  
   
   for (i in 1:nSims){

      y1 <- sample(y,size=n1,replace=TRUE)
      y2 <- sample(y,size=n2,replace=TRUE)
      sd1[i] <- sd(y1)
      sd2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
   }
   
   return(data.frame(sd1,sd2,meandiff,mean1,mean2))
}

Homunbal_sym <- corrHomunbal(sd=2,n1=n1,n2=n2,skew=0)
Homunbal_rightskew <- corrHomunbal(sd=2,n1=n1,n2=n2,skew=6.32)
Homunbal_leftskew <- corrHomunbal(sd=2,n1=n1,n2=n2,skew=-6.32)

```

```{r corHomunbalsym,echo=FALSE}
Homunbal_sym_mean1 <- Homunbal_sym$mean1
Homunbal_sym_mean2 <- Homunbal_sym$mean2
Homunbal_sym_meandiff <- Homunbal_sym$meandiff
Homunbal_sym_sd1 <- Homunbal_sym$sd1
Homunbal_sym_sd2 <- Homunbal_sym$sd2
Homunbal_sym_sdCohen <- sqrt(((n1-1)*Homunbal_sym_sd1^2+(n2-1)*Homunbal_sym_sd2^2)/(N-2))
Homunbal_sym_sdShieh <-  sqrt(Homunbal_sym_sd1^2/(n1/N)+Homunbal_sym_sd2^2/(n2/N))
Homunbal_sym_sdCohenprime <- sqrt((Homunbal_sym_sd1^2+Homunbal_sym_sd2^2)/2)
```

```{r corHomunbalrightskew,echo=FALSE}
Homunbal_rightskew_mean1 <- Homunbal_rightskew$mean1
Homunbal_rightskew_mean2 <- Homunbal_rightskew$mean2
Homunbal_rightskew_meandiff <- Homunbal_rightskew$meandiff
Homunbal_rightskew_sd1 <- Homunbal_rightskew$sd1
Homunbal_rightskew_sd2 <- Homunbal_rightskew$sd2
Homunbal_rightskew_sdCohen <- sqrt(((n1-1)*Homunbal_rightskew_sd1^2+(n2-1)*Homunbal_rightskew_sd2^2)/(N-2))
Homunbal_rightskew_sdShieh <-  sqrt(Homunbal_rightskew_sd1^2/(n1/N)+Homunbal_rightskew_sd2^2/(n2/N))
Homunbal_rightskew_sdCohenprime <- sqrt((Homunbal_rightskew_sd1^2+Homunbal_rightskew_sd2^2)/2)
```

```{r corHomunballeftskew,echo=FALSE}
Homunbal_leftskew_mean1 <- Homunbal_leftskew$mean1
Homunbal_leftskew_mean2 <- Homunbal_leftskew$mean2
Homunbal_leftskew_meandiff <- Homunbal_leftskew$meandiff
Homunbal_leftskew_sd1 <- Homunbal_leftskew$sd1
Homunbal_leftskew_sd2 <- Homunbal_leftskew$sd2
Homunbal_leftskew_sdCohen <- sqrt(((n1-1)*Homunbal_leftskew_sd1^2+(n2-1)*Homunbal_leftskew_sd2^2)/(N-2))
Homunbal_leftskew_sdShieh <-  sqrt(Homunbal_leftskew_sd1^2/(n1/N)+Homunbal_leftskew_sd2^2/(n2/N))
Homunbal_leftskew_sdCohenprime <- sqrt((Homunbal_leftskew_sd1^2+Homunbal_leftskew_sd2^2)/2)
```

When distributions are skewed, there are again non-null correlations between $\bar{X_j}$ and $S_j$, however $cor(S_1,\bar{X_1}) \neq cor(S_2,\bar{X_2})$, because of the different sample sizes. 

```{r Homunbalcorasafctofn1,fig.cap="Correlation between $S_j$ and $\\bar{X_j}$ when $n$ = 25, 50, 75 or 100 and samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}

nSims=10000
m1=0
n1 <- 25
n2 <- 50
n3 <- 75
n4 <- 100
sd <- 2
kurt=95.75
skew=6.32

N <- n1+n2+n3+n4

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

sd1 <- NULL
sd2 <- NULL
sd3 <- NULL
sd4 <- NULL

y <- rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(y,size=n1,replace=TRUE)
     y2 <- sample(y,size=n2,replace=TRUE)
     y3 <- sample(y,size=n3,replace=TRUE)
     y4 <- sample(y,size=n4,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   sd1 <- c(sd1,sd(y1))
   sd2 <- c(sd2,sd(y2))
   sd3 <- c(sd3,sd(y3))
   sd4 <- c(sd4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,sd1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when n=",n1,", cor(mean,SD)=",round(cor(mean1,sd1,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean2,sd2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when n=",n2,", cor(mean,SD)=",round(cor(mean2,sd2,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean3,sd3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when n=",n3,", cor(mean,SD)=",round(cor(mean3,sd3,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean4,sd4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when n=",n4,", cor(mean,SD)=",round(cor(mean4,sd4,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          

```

```{r Homunbalcorasafctofn2,fig.cap="Correlation between $S_j$ and $\\bar{X_j}$ when $n$ = 25, 50, 75 or 100 and samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}

nSims=10000
m1=0
n1 <- 25
n2 <- 50
n3 <- 75
n4 <- 100
sd <- 2
kurt=95.75
skew=-6.32

N <- n1+n2+n3+n4

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

sd1 <- NULL
sd2 <- NULL
sd3 <- NULL
sd4 <- NULL

y <- rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(y,size=n1,replace=TRUE)
     y2 <- sample(y,size=n2,replace=TRUE)
     y3 <- sample(y,size=n3,replace=TRUE)
     y4 <- sample(y,size=n4,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   sd1 <- c(sd1,sd(y1))
   sd2 <- c(sd2,sd(y2))
   sd3 <- c(sd3,sd(y3))
   sd4 <- c(sd4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,sd1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when n=",n1,", cor(mean,SD)=",round(cor(mean1,sd1,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean2,sd2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when n=",n2,", cor(mean,SD)=",round(cor(mean2,sd2,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean3,sd3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when n=",n3,", cor(mean,SD)=",round(cor(mean3,sd3,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean4,sd4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when n=",n4,", cor(mean,SD)=",round(cor(mean4,sd4,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          

```

When distributions are skewed, one observes that the larger the sample size, the lower the correlation between $S_j$ and $\bar{X_j}$ (See Figures \ref{fig:Homunbalcorasafctofn1} and \ref{fig:Homunbalcorasafctofn2}).

```{r pltSDHomunbalRskew,fig.cap="$S_j$ ($j$=1,2) as a function of $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$), with $n_1$=20 and $n_2$=100",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(Homunbal_rightskew_sd1,Homunbal_rightskew_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(Homunbal_rightskew_sd1,Homunbal_rightskew_mean1,method="spearman"),2)))
plot(Homunbal_rightskew_sd2,Homunbal_rightskew_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(Homunbal_rightskew_sd2,Homunbal_rightskew_mean2,method="spearman"),2)))

plot(Homunbal_rightskew_sd1,Homunbal_rightskew_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_rightskew_sd1,Homunbal_rightskew_meandiff),2)))
plot(Homunbal_rightskew_sd2,Homunbal_rightskew_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_rightskew_sd2,Homunbal_rightskew_meandiff),2)))
```

```{r pltSDHomunbalLskew,fig.cap="$S_j$ ($j$=1,2) as a function of $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$), with $n_1$=20 and $n_2$=100",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(Homunbal_leftskew_sd1,Homunbal_leftskew_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(Homunbal_leftskew_sd1,Homunbal_leftskew_mean1,method="spearman"),2)))
plot(Homunbal_leftskew_sd2,Homunbal_leftskew_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(Homunbal_leftskew_sd2,Homunbal_leftskew_mean2,method="spearman"),2)))

plot(Homunbal_leftskew_sd1,Homunbal_leftskew_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_leftskew_sd1,Homunbal_leftskew_meandiff),2)))
plot(Homunbal_leftskew_sd2,Homunbal_leftskew_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_leftskew_sd2,Homunbal_leftskew_meandiff),2)))
```

This might explain that the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ is lower in the larger sample (see bottom plots in Figures \ref{fig:pltSDHomunbalRskew} and \ref{fig:pltSDHomunbalLskew}). With no surprise, there is a positive (negative) correlation between $S_1$ and $\bar{X_1}-\bar{X_2}$ and a negative (positive) correlation between $S_2$ and $\bar{X_1}-\bar{X_2}$ when distributions are right-skewed (left-skewed), as illustrated in the two bottom plots of Figures \ref{fig:pltSDHomunbalRskew} and \ref{fig:pltSDHomunbalLskew}. 

```{r pltStdzrHomunbalRskew,fig.cap="$S_{Cohen's \\; d}$, $S_{Shieh's \\; d}$ and $S_{Cohen's \\; d^*}$ as a function of the mean difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$, with $n_1$=20 and $n_2$=100)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(Homunbal_rightskew_sdCohen,Homunbal_rightskew_meandiff,ylab=expression(S["Cohen's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_rightskew_sdCohen,Homunbal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Homunbal_rightskew_sdShieh,Homunbal_rightskew_meandiff,ylab=expression(S["Shieh's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_rightskew_sdShieh,Homunbal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Homunbal_rightskew_sdCohenprime,Homunbal_rightskew_meandiff,ylab=expression(S["Cohen's d*"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_rightskew_sdCohenprime,Homunbal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
```

```{r pltStdzrHomunbalLskew,fig.cap="$S_{Cohen's \\; d}$, $S_{Shieh's \\; d}$ and $S_{Cohen's \\; d^*}$ as a function of the mean difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$), with $n_1$=20 and $n_2$=100",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(Homunbal_leftskew_sdCohen,Homunbal_leftskew_meandiff,ylab=expression(S["Cohen's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_leftskew_sdCohen,Homunbal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Homunbal_leftskew_sdShieh,Homunbal_leftskew_meandiff,ylab=expression(S["Shieh's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_leftskew_sdShieh,Homunbal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Homunbal_leftskew_sdCohenprime,Homunbal_leftskew_meandiff,ylab=expression(S["Cohen's d*"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Homunbal_leftskew_sdCohenprime,Homunbal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
```

This might also explain that standardizers of Shieh's $d$ and Cohen's $d^*$ are **correlated** with $\bar{X_1}-\bar{X_2}$ (see Figures \ref{fig:pltStdzrHomunbalRskew} and \ref{fig:pltStdzrHomunbalLskew}):  
- When computing $S_{Cohen's \; d^*}$, the same weight is given to both $S_1$ and $S_2$. Therefore, it does not seem surprising that the sign of the correlation between $S_{Cohen's \; d^*}$ and $\bar{X_1}-\bar{X_2}$ is the same as the size of the correlation between $\bar{X_1}-\bar{X_2}$ and the $SD$ of the smallest sample;   
- When computing $S_{Shieh's \; d}$, more weight is given to the $SD$ of the smallest sample, it is therefore not really surprising to observe that the correlation between $S_{Shieh's \; d}$ and $\bar{X_1}-\bar{X_2}$  is closer of the correlation between the $SD$ of the smallest group and $\bar{X_1}-\bar{X_2}$ (i.e. $|cor(S_{Shieh's \; d},\bar{X_1}-\bar{X_2})| > |cor(S_{Cohen's \; d^*},\bar{X_1}-\bar{X_2})|$);    
- When computing $S_{Cohen's \; d}$, more weight is given to the $SD$ of the largest sample,  which by compensation effect brings the correlation very close to 0.  

The correlation between $\bar{X_1}-\bar{X_2}$ and respectively $S_1$, $S_2$, the standardizer of Cohen's $d^*$, the standardizer of Shieh's $d$ and the standardizer of Cohen's $d$ are summarized in Table 1.


|                               |           __**population distribution**__                                  |             
|-------------------------------|:--------------------------------:|:---------------------------------------:|
|                               |      *right-skewed*              |         *left-skewed*                   |
|                               |----------------------------------|-----------------------------------------|
|        When $n_1=n_2$         | $S_1$: *positive*               | $S_1$: *negative*                      |
|                               | $S_2$: *negative*               | $S_2$: *positive*                      |
|                               | $S_{Cohen's \; d}$: *null*     | $S_{Cohen's \; d}$: *null*            |
|                               | $S_{Shieh's \; d}$: *null*     | $S_{Shieh's \; d}$: *null*            |
|                               | $S_{Cohen's \; d^*}$: *null*    | $S_{Cohen's \; d^*}$: *null*           |
|                               |                                  |                                         |
|        When $n_1>n_2$         | $S_1$: *positive*               | $S_1$: *negative*                      |
|                               | $S_2$: *negative*               | $S_2$: *positive*                      |
|                               | $S_{Cohen's \; d}$: *null*     | $S_{Cohen's \; d}$: *null*            |
|                               | $S_{Shieh's \; d}$: *negative* | $S_{Shieh's \; d}$: *positive*        |
|                               | $S_{Cohen's \; d^*}$: *positive (but very small)*| $S_{Cohen's \; d^*}$: *negative (but very small)*       |
|                               |                                  |                                         |
|        When $n_1<n_2$         | $S_1$: *positive*               | $S_1$: *negative*                      |
|                               | $S_2$: *negative*               | $S_2$: *positive*                      |
|                               | $S_{Cohen's \; d}$: *negative (but very small)*     | $S_{Cohen's \; d}$: *positive (but very small)*            |
|                               | $S_{Shieh's \; d}$: *positive* | $S_{Shieh's \; d}$: *negative*        |
|                               | $S_{Cohen's \; d^*}$: *positive*| $S_{Cohen's \; d^*}$: *negative*       |

Table: Correlation between standardizers ($S_1$, $S_2$, $S_{Cohen's \; d}$,  $S_{Shieh's \; d}$ and $S_{Cohen's \; d^*}$) and $\bar{X_1}-\bar{X_2}$, when samples are extracted from skewed distributions with equal variances, and $n_1=n_2$ (condition a) or $n_1 \neq n_2$ (condition b)  


## When unequal population variances are estimated based on equal sample sizes (condition c)

```{r Hetbal,include=FALSE}

n <- 20
n1 <- n
n2 <- n
N <- n1+n2
sd1 <- 2
sd2 <- 4


Hetbal=function(sd1,sd2,nSims=10000,m1=1,m2=0,n,skew,kurt=95.75){
   
   SD1<-rep(0,nSims)
   SD2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(0,nSims)
   mean2<-rep(0,nSims)
   
   Y1=rpearson(1000000,moments=c(m1,sd1^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
   Y2=rpearson(1000000,moments=c(m1,sd2^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
   
   for (i in 1:nSims){
      
      y1 <- sample(Y1,size=n,replace=TRUE)
      y2 <- sample(Y2,size=n,replace=TRUE)
      SD1[i] <- sd(y1)
      SD2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
   }
   
   return(data.frame(SD1,SD2,meandiff,mean1,mean2))
}

Hetbal_sym <- Hetbal(sd1=sd1,sd2=sd2,n=n,skew=0)
Hetbal_rightskew <- Hetbal(sd1=sd1,sd2=sd2,n=n,skew=6.32)
Hetbal_leftskew <- Hetbal(sd1=sd1,sd2=sd2,n=n,skew=-6.32)

```

```{r corHetbalsym,echo=FALSE}
Hetbal_sym_mean1 <- Hetbal_sym$mean1
Hetbal_sym_mean2 <- Hetbal_sym$mean2
Hetbal_sym_meandiff <- Hetbal_sym$meandiff
Hetbal_sym_sd1 <- Hetbal_sym$SD1
Hetbal_sym_sd2 <- Hetbal_sym$SD2
Hetbal_sym_sdCohen <- sqrt(((n1-1)*Hetbal_sym_sd1^2+(n2-1)*Hetbal_sym_sd2^2)/(N-2))
Hetbal_sym_sdShieh <-  sqrt(Hetbal_sym_sd1^2/(n1/N)+Hetbal_sym_sd2^2/(n2/N))
Hetbal_sym_sdCohenprime <- sqrt((Hetbal_sym_sd1^2+Hetbal_sym_sd2^2)/2)
```

```{r corHetbalrightskew,echo=FALSE}
Hetbal_rightskew_mean1 <- Hetbal_rightskew$mean1
Hetbal_rightskew_mean2 <- Hetbal_rightskew$mean2
Hetbal_rightskew_meandiff <- Hetbal_rightskew$meandiff
Hetbal_rightskew_sd1 <- Hetbal_rightskew$SD1
Hetbal_rightskew_sd2 <- Hetbal_rightskew$SD2
Hetbal_rightskew_sdCohen <- sqrt(((n1-1)*Hetbal_rightskew_sd1^2+(n2-1)*Hetbal_rightskew_sd2^2)/(N-2))
Hetbal_rightskew_sdShieh <-  sqrt(Hetbal_rightskew_sd1^2/(n1/N)+Hetbal_rightskew_sd2^2/(n2/N))
Hetbal_rightskew_sdCohenprime <- sqrt((Hetbal_rightskew_sd1^2+Hetbal_rightskew_sd2^2)/2)
```

```{r corHetballeftskew,echo=FALSE}
Hetbal_leftskew_mean1 <- Hetbal_leftskew$mean1
Hetbal_leftskew_mean2 <- Hetbal_leftskew$mean2
Hetbal_leftskew_meandiff <- Hetbal_leftskew$meandiff
Hetbal_leftskew_sd1 <- Hetbal_leftskew$SD1
Hetbal_leftskew_sd2 <- Hetbal_leftskew$SD2
Hetbal_leftskew_sdCohen <- sqrt(((n1-1)*Hetbal_leftskew_sd1^2+(n2-1)*Hetbal_leftskew_sd2^2)/(N-2))
Hetbal_leftskew_sdShieh <-  sqrt(Hetbal_leftskew_sd1^2/(n1/N)+Hetbal_leftskew_sd2^2/(n2/N))
Hetbal_leftskew_sdCohenprime <- sqrt((Hetbal_leftskew_sd1^2+Hetbal_leftskew_sd2^2)/2)
```

```{r Hetbalcorasafctofn1,fig.cap="Correlation between $S_j$ and $\\bar{X_j}$ when $SD$ = 2, 4, 6 or 8 and samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}

nSims=10000
m1=0
sd1 <- 2
sd2 <- 4
sd3 <- 6
sd4 <- 8
n <- 20
kurt=95.75
skew=6.32

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

SD1 <- NULL
SD2 <- NULL
SD3 <- NULL
SD4 <- NULL

Y1 <- rpearson(1000000,moments=c(m1,sd1^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y2 <- rpearson(1000000,moments=c(m1,sd2^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y3 <- rpearson(1000000,moments=c(m1,sd3^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y4 <- rpearson(1000000,moments=c(m1,sd4^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(Y1,size=n,replace=TRUE)
     y2 <- sample(Y2,size=n,replace=TRUE)
     y3 <- sample(Y3,size=n,replace=TRUE)
     y4 <- sample(Y4,size=n,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   SD1 <- c(SD1,sd(y1))
   SD2 <- c(SD2,sd(y2))
   SD3 <- c(SD3,sd(y3))
   SD4 <- c(SD4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,SD1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when sd=",sd1,",cor(mean,SD)=",round(cor(mean1,SD1,method="spearman"),3)))          
   plot(mean2,SD2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when sd=",sd2,",cor(mean,SD)=",round(cor(mean2,SD2,method="spearman"),3)))          
   plot(mean3,SD3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when sd=",sd3,",cor(mean,SD)=",round(cor(mean3,SD3,method="spearman"),3)))          
   plot(mean4,SD4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when sd=",sd4,",cor(mean,SD)=",round(cor(mean4,SD4,method="spearman"),3)))          
```

```{r Hetbalcorasafctofn2,fig.cap="Correlation between $S_j$ and $\\bar{X_j}$ when $SD$ = 2, 4, 6 or 8 and samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}

nSims=10000
m1=0
sd1 <- 2
sd2 <- 4
sd3 <- 6
sd4 <- 8
n <- 20
kurt=95.75
skew=-6.32

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

SD1 <- NULL
SD2 <- NULL
SD3 <- NULL
SD4 <- NULL

Y1 <- rpearson(1000000,moments=c(m1,sd1^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y2 <- rpearson(1000000,moments=c(m1,sd2^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y3 <- rpearson(1000000,moments=c(m1,sd3^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y4 <- rpearson(1000000,moments=c(m1,sd4^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(Y1,size=n,replace=TRUE)
     y2 <- sample(Y2,size=n,replace=TRUE)
     y3 <- sample(Y3,size=n,replace=TRUE)
     y4 <- sample(Y4,size=n,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   SD1 <- c(SD1,sd(y1))
   SD2 <- c(SD2,sd(y2))
   SD3 <- c(SD3,sd(y3))
   SD4 <- c(SD4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,SD1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when sd=",sd1,",cor(mean,SD)=",round(cor(mean1,SD1,method="spearman"),3)))          
   plot(mean2,SD2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when sd=",sd2,",cor(mean,SD)=",round(cor(mean2,SD2,method="spearman"),3)))          
   plot(mean3,SD3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when sd=",sd3,",cor(mean,SD)=",round(cor(mean3,SD3,method="spearman"),3)))          
   plot(mean4,SD4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when sd=",sd4,",cor(mean,SD)=",round(cor(mean4,SD4,method="spearman"),3)))          
```

```{r pltSDHetbalRskew,fig.cap="$S_j$ ($j$=1,2) as a function of $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$), with $S_1$=2 and $S_2$=4",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(Hetbal_rightskew_sd1,Hetbal_rightskew_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(Hetbal_rightskew_sd1,Hetbal_rightskew_mean1,method="spearman"),2)))
plot(Hetbal_rightskew_sd2,Hetbal_rightskew_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(Hetbal_rightskew_sd2,Hetbal_rightskew_mean2,method="spearman"),2)))

plot(Hetbal_rightskew_sd1,Hetbal_rightskew_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_rightskew_sd1,Hetbal_rightskew_meandiff),2)))
plot(Hetbal_rightskew_sd2,Hetbal_rightskew_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_rightskew_sd2,Hetbal_rightskew_meandiff),2)))
```

```{r pltSDHetbalLskew,fig.cap="$S_j$ ($j$=1,2) as a function of $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$), with $S_1$=2 and $S_2$=4",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(Hetbal_leftskew_sd1,Hetbal_leftskew_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(Hetbal_leftskew_sd1,Hetbal_leftskew_mean1,method="spearman"),2)))
plot(Hetbal_leftskew_sd2,Hetbal_leftskew_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(Hetbal_leftskew_sd2,Hetbal_leftskew_mean2,method="spearman"),2)))

plot(Hetbal_leftskew_sd1,Hetbal_leftskew_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_leftskew_sd1,Hetbal_leftskew_meandiff),2)))
plot(Hetbal_leftskew_sd2,Hetbal_leftskew_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_leftskew_sd2,Hetbal_leftskew_meandiff),2)))
```

When distributions are skewed, there are again non-null correlations between $\bar{X_j}$ and $S_j$. As illustrated in Figures \ref{fig:Hetbalcorasafctofn1} and \ref{fig:Hetbalcorasafctofn2}, the correlation remains the same for any population $SD$ ($\sigma$). However, the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ differs: it is stronger in the sample extracted from the larger population variance (see Figures \ref{fig:pltSDHetbalRskew} and \ref{fig:pltSDHetbalLskew}). 

```{r pltStdzrHetbalRskew,fig.cap="$S_{Cohen's \\; d}$, $S_{Shieh's \\; d}$ and $S_{Cohen's \\; d^*}$ as a function of the mean difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$), with $S_1$=2 and $S_2$=4",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(Hetbal_rightskew_sdCohen,Hetbal_rightskew_meandiff,ylab=expression(S["Cohen's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_rightskew_sdCohen,Hetbal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hetbal_rightskew_sdShieh,Hetbal_rightskew_meandiff,ylab=expression(S["Shieh's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_rightskew_sdShieh,Hetbal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hetbal_rightskew_sdCohenprime,Hetbal_rightskew_meandiff,ylab=expression(S["Cohen's d*"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_rightskew_sdCohenprime,Hetbal_rightskew_meandiff,method="spearman"),2)),cex.lab=1.5)
```

```{r pltStdzrHetbalLskew,fig.cap="$S_{Cohen's \\; d}$, $S_{Shieh's \\; d}$ and $S_{Cohen's \\; d^*}$ as a function of the mean difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$), with $S_1$=2 and $S_2$=4",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(Hetbal_leftskew_sdCohen,Hetbal_leftskew_meandiff,ylab=expression(S["Cohen's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_leftskew_sdCohen,Hetbal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hetbal_leftskew_sdShieh,Hetbal_leftskew_meandiff,ylab=expression(S["Shieh's d"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_leftskew_sdShieh,Hetbal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(Hetbal_leftskew_sdCohenprime,Hetbal_leftskew_meandiff,ylab=expression(S["Cohen's d*"]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(Hetbal_leftskew_sdCohenprime,Hetbal_leftskew_meandiff,method="spearman"),2)),cex.lab=1.5)
```

This also explains that when computing a standardizer that takes both $S_1$ and $S_2$ into account, it results in a standardizer that is correlated with $\bar{X_1}-\bar{X_2}$ (see Figures \ref{fig:pltStdzrHetbalRskew} and \ref{fig:pltStdzrHetbalLskew}). The correlation between the mean difference ($\bar{X_1}-\bar{X_2}$) and respectively the standardizer of Shieh's $d$, Cohen's $d^*$ and Cohen's $d$ will have the same sign as the correlation between ($\bar{X_1}-\bar{X_2}$) and the larger $SD$. Table 2 summarizes the sign of the correlation between $\bar{X_1}-\bar{X_2}$ and respectively $S_1$, $S_2$ and the three standardizers taking both $S_1$ and $S_2$ into account (see "Others" in the Table).


|                               |           __**population distribution**__             |             
|-------------------------------|:-------------------------:|:-------------------------:|
|                               |      *right-skewed*       |         *left-skewed*     |
|                               |---------------------------|---------------------------|
|   When $\sigma_1=\sigma_2$    | $S_1$: *positive*        | $S_1$: *negative*        |
|                               | $S_2$: *negative*        | $S_2$: *positive*        |
|                               | Others: *null*            | Others: *null*            |
|                               |                           |                           |
|   When $\sigma_1>\sigma_2$    | $S_1$: *positive*        | $S_1$: *negative*        |
|                               | $S_2$: *negative*        | $S_2$: *positive*        |
|                               | Others: *positive*        | Others: *negative*        |
|                               |                           |                           |
|   When $\sigma_1<\sigma_2$    | $S_1$: *positive*        | $S_1$: *negative*        |
|                               | $S_2$: *negative*        | $S_2$: *positive*        |
|                               | Others: *negative*        | Others: *positive*        |
|                               |                           |                           |

Table: Correlation between standardizers ($S_1$, $S_2$ and others) and $\bar{X_1}-\bar{X_2}$, when samples are extracted from skewed distributions with equal sample sizes, as a function of the SD-ratio.  