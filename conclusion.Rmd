---
title: ''
output: pdf_document

header-includes:
  - \usepackage{xcolor}
  - \usepackage{bm}
---

# Discussion générale et conclusion

A travers cette thèse, nos objectifs de départ étaient (1)	d’identifier des manquements dans les pratiques actuelles des chercheurs, via des analyses d’articles publiés dans des revues de psychologie ;  (2)	de réaliser des simulations, en vue de montrer l’impact de ces pratiques et (3)	de proposer des recommandations pour les améliorer.  

Dans un premier temps, nous nous sommes focalisés sur l'usage des statistiques $t$ de Student, $F$ de Fisher et $d$ de Cohen, soit des mesures communément utilisées par les chercheurs en psychologie, en vue de comparer les moyennes de deux ou plusieurs groupes de sujets indépendants, et qui reposent sur les conditions que des résidus, indépendants et identiquement distribués, soient extraits d'une distribution normale et que les variances des populations dont sont extraits chaque groupe soient identiques (soit la condition d'homogénéité des variances). Comme nous l'avons théoriquement décrit, il existe toute une série d'arguments qui permettent de remettre en cause la crédibilité des conditions statistiques de normalité (comme la présence de sous-populations définies par des facteurs non identifiés dans le design, l'étude de mesures bornées, telle(s?) que le temps qui ne peut prendre des valeurs négatives, ou encore le fait qu'un traitement est susceptible de modifier la forme des distributions étudiées) et d'homogénéité des variances (comme l'étude de groupes pré-existants à l'expérience, définis par des variables telles que le genre ou l'origine ethnique\footnote{Dans ce cas, les sujets ne sont pas répartis aléatoirement entre les groupes. L'hétérogénéité des variances entre les groupes et dès lors le résultats de la violation de la condition méthodologique d'indépendance des résidus.}, ou encore le fait qu'un traitement, qu'il soit expérimental ou quasi-expérimental, est susceptible d'agir sur tous les paramètres d'une distribution, incluant sa variance), dans de nombreux domaines de la psychologie. Conformément à nos objectifs de départ, nous avons réalisé des simulations Monte Carlo en vue de montrer les conséquences réelles de la violation des conditions de normalité et d'homogénéité des variances, et de comparer respectivement les statistiques $t$ de Student (chapitre 2), $F$ de Fisher (chapitre 3) et $d$ de Cohen (chapitre 4) à des alternatives plus robustes en cas de violation de la condition d'homogénéité des variances\footnote{Comme expliqué en introduction, nous nous sommes principalement focalisés sur la condition d'homogénéité des variances compte tenu, d'abord, de la forte résistance de la part des chercheurs à l'égard des tests comparant d'autres indicateurs de tendance centrale que la moyenne et ensuite, du fait qu'un écart à la condition d'homogénéité des variances affectera bien plus les taux d'erreur de type $I$ et $II$ des statistiques $t$ de Student, $F$ de Fisher et $d$ de Cohen qu'un écart à la condition de normalité.}. De manière consistante avec nos attentes théoriques, lorsque les deux échantillons comparés sont de même taille, le test $t$ de Student est robuste aux violations de la condition d'homogénéité des variances. Par contre, il en est différemment lorsque les échantillons sont de tailles différentes: sur le long terme, la probabilité de rejeter l'hypothèse nulle avec ce test sera supérieure aux attentes théoriques lorsque le plus petit échantillon est extrait de la population ayant la plus grande variance, et sera inférieure aux attentes théoriques lorsque le plus petit échantillon est extrait de la population ayant la plus petite variance. Au contraire, le test $t$ de Welch ne dépend pas de la condition d'homogénéité des variances. Il est souvent recommandé aux chercheurs de tester préalablement la condition d'homogénéité des variances, via un test de Levene par exemple, et ensuite d'utiliser soit le test $t$ de Student soit le test $t$ de Welch, suivant que la condition d'homogénéité des variances soit ou non respectée. Cependant, dans la mesure où la condition d'homogénéité des variances est plus souvent l'exception que la norme et qu'il est parfois très difficile (voire impossible) de détecter les écarts à cette condition à travers des tests, nous recommandons l'usage du test $t$ de Welch par défaut, au moins lorsque les échantillons sont de taille différente. En effet, ce test est pratiquement aussi puissant que le test $t$ de Student lorsque la condition d'homogénéité des variances est respectée, et contrôle bien mieux les taux d'erreur de type $I$ et $II$ lorsqu'elle ne l'est pas. Par ailleurs, il est disponible dans presque tous les logiciels statistiques courants ($R$, Minitab, Jamovi, SPSS, etc.). Dans la mesure où l'ANOVA $F$ de Fisher est une généralisation du test $t$ de Student\footnote{L'ANOVA $F$ de Fisher peut être utilisée lorsqu'on compare deux ou plus de deux échantillons indépendants sur base de leur moyenne. Lorsqu'on compare exactement deux groupes, le test $t$ de Student et l'ANOVA $F$ de Fisher sont strictement équivalents. En effet, ils entretiennent la relation mathématique suivante: $F(1,x) = t^2(x)$.}, il n'est pas surprenant que nos simulations relatives à l'ANOVA $F$ de Fisher amènent à des constats semblables à ceux obtenus sur base de nos simulations relativement au test $t$ de Student. Ces simulations nous ont cependant permis de tirer deux conclusions supplémentaires: d'abord, plus le nombre d'échantillons comparés est importants, plus les écarts entre les proportions réelles et théoriques de simulations qui amènent à conclure au rejet de l'hypothèse nulle augmentent. Ensuite, lorsqu'on compare plus de deux groupes, l'ANOVA $F$ de Fisher sera affectée par la présence d'hétérogénéité des variances, même lorsque tous les échantillons sont de tailles identiques (dans ce cas, le test amènera à rejeter l'hypothèse nulle plus souvent qu'attendu théoriquement, sur le long terme). En cas d'homogénéité des variances, le test $W$ de Welch est très légèrement inférieur aux tests $F^*$ de Brown-Forsythe que $F$ de Fisher, tant en termes de contrôle des erreurs de type $I$ et $II$ qu'en termes de consistances entre les puissances théoriques et observées. Par contre, il leur est bien supérieur en cas d'hétérogénéité des variances. Pour les mêmes raisons que celles qui nous amènent à privilégier le test $t$ de Welch par défaut, nous recommandons de privilégier systématiquement le test $W$ de Welch lorsqu'on compare plus de deux groupes de sujets indépendants sur base de leur moyenne. Tout comme le test $t$ de Welch, le test $W$ de Welch est disponible dans la plupart des logiciels statistiques fréquemment utilisés par les chercheurs en psychologie ($R$, Minitab, Jamovi, SPSS, etc.). Au sein du chapitre 4, nous avons mis deux éléments principaux en évidence concernant la mesure $d$ de Cohen. D'abord, il s'agit d'une mesure toujours biaisée, même lorsque toutes les conditions dont elle dépend sont respectées. Heureusement, une transformation de cette mesure existe telle que le biais devient nul lorsque la condition de normalité des résidus est respectée. Cette transformation a été proposée par Hedges et porte dès lors son nom: il s'agit de la mesure $g$ de Hedges. Ensuite, une violation de la condition d'homogénéité des variances amènera à une forte augmentation de la variance des estimateurs $d$ de Cohen et $g$ de Hedges, même lorsque les deux échantillons sont de taille identique. Différents estimateurs ont été proposés dans la littérature en vue de remplacer le $d$ de Cohen (et le $g$ de Hedges) en cas de violation de la condition d'homogénéité des variances. Parmis ceux-ci, on retrouve fréquemment le $d$ de Glass (qui devient le $g$ de Glass, après transformation en vue de supprimer le biais lorsque les résidus se distribuent normalement). Nos simulations ont révélé que le biais et la variance de ces estimateurs dépendent fortement de paramètres que l'on ne peut contrôler, ce qui nous amène à décourager l'usage de ces mesures. Dans la litérature, on retrouve également la mesure $d$ de Shieh, qui entretient une relation mathématique directe avec le $t$ de Welch, ainsi que la mesure $d^*$ de Cohen qui, contrairement au $d$ de Cohen classique, implique le calcul de la moyenne *non poolée* des variances de chaque groupe. De même que pour les estimateurs précédemment cités, il est possible de transformer ces mesures en vue de supprimer le biais lorsque la condition de normalité des résidus est respectée. Cela donne respectivement lieu aux mesures $g^*$ de Hedges et $g$ de Shieh. Grâce à nos simulations, nous avons révélé que le $g^*$ de Hedges est supérieur au $g$ de Shieh, non seulement d'un point de vue inférentiel (contrairement au $g$ de Shieh, le $g^*$ de Hedges est consistant, ce qui signifie que son biais et sa variance diminuent toujours lorsque les tailles d'échantillon augmentent) que d'un point de vue interprétatif (sa valeur est constante, peu importe que les deux échantillons soient de taille identique ou non). Lorsqu'on compare les mesures $g$ de Hedges et $g^*$ de Hedges, on constate que le $g^*$ de Hedges n'est très légèrement inférieur au $g$ de Hedges, en termes de biais et de variance, que lorsque des échantillons de tailles différentes sont extraits de population aux variances identiques. Il est tout aussi efficace que le $g$ de Hedges lorsque tant les tailles d'échantillons que les variances de population sont identiques. De plus, il reste valide lorsque la condition d'homogénéité des variances n'est pas respectée, contrairement au $g$ de Hedges. Pour les mêmes raisons que celles qui nous amènent à privilégier les tests $t$ de Welch et $F$ de Welch par défaut, nous recommandons de privilégier systématiquement le $g^*$ de Hedges. 

Dans un deuxième temps, nous nous sommes concentrés sur la tendance des chercheurs à définir par défaut, comme hypothèse nulle, une hypothèse d'absence d'effet. Nous avons souligné que même lorsque l'objectif est de prouver l'absence d'effet, c'est souvent sur base d'un non rejet d'une hypothèse d'absence d'effet que les chercheurs concluaient.

nous nous sommes focalisés sur la manière dont les chercheurs définissent l'hypothèse nulle de leur test en psychologie. 
*reprendre ici*
A travers les simulations qui nous ont permis d'étudier le $t$ de Student, le $F$ de Fisher, le $d$ de Cohen et leurs alte=rnatives robustes aux écarts à la condition d'homogénéité des variances, nous avons sytématiquement définis l'absence de différence entre les moyennes des groupes indépendants comme hypothèse nulle. Pourtant, il est théoriquement possible de définir n'importe quelle différence (ou intervalle de différences) de moyennes comme hypothèse nulle. Par exemple, en définissant comme hypothèse nulle l'ensemble des différences supérieures ou égales à la plus petite différence jugée pertinente, il est possible de fournir un soutien en faveur de l'absence d'effet jugé pertinent. C'est ce qui est fait lorsqu'on réaliste un test d'équivalence, dont fait partie le TOST. *lire article du TOST pour les arguments pour justifier qu'il est important de comparer le TOST à de nouvelles stratégies*.

## Apports théoriques et appliqués

### Apports théoriques

\color{blue} Afin d'aider les chercheurs à prendre conscience des conséquences néfastes de l'usage récurrent des tests $t$ de Student, $F$ de Fisher et du $d$ de Cohen, conformément à ce que nous avions annoncé en introduction, nous avons fourni des exemples concrets et issus de la psychologie expliquant en quoi la condition d’homoscédasticité n’était pas réaliste et les conséquences réelles de ces violations. 

*[Un consultant doit pouvoir parler de langage des psycs, c'est-à-dire décrire et expliquer les méthodes requises d'une manière compréhensible pour les clients [@golinski_expanding_2009]. Est-ce bien de demander à des mathématiciens/Statisticiens d'enseigner les stat aux psy's? Par forcément, car un psychologue spécialisé en méthodo quanti sera plsu à m^mee de comprendre les procédures et méthodes requises par les psys (ex. de la question de la taille d'effet qui n'intéresse pas vraiment les statisticiens; @golinski_expanding_2009)]*. 

Beaucoup avant nous ont tenté de faire passer ce message. L’idée de remplacer le test $t$ de Student par le test de Welch n’est pas récente. Mais les moyens informatiques actuels nous ont permis la réalisation de simulations intensives, avec 1,000,000 d’itérations pour un très grand nombre de scénarios qui varient en fonction du niveau d’hétéroscédasticité et du degré d’écart aux conditions d’application. En clair, nous avons remis au goût du jour une problématique ancienne, mais avec des simulations bcp plus poussées car pas possible avant. 

De plus, nous avons tout mis en oeuvre pour rendre notre recherche la plus tranparente possible et pour assurer une grande accessibilité pour tous les chercheurs en psychologie. 
- Premièrement, nous avons systématiquement diffusé des preprint, chaque fois que nous avons soumis un article. *Cette pratique a eu des retombées positives qui ont dépassé nos attentes, grâce à la collaborations d'éminents chercheurs (exemple du dernier article qui a été très longuement commenté par Cumming), *  
- Deuxièmement, nous avons systématiquement privilégié des revues Open Access (*International Review of Social Psychology*, *British Journal of Mathematical and Statistical Science*, *AMPPS*).   
- Troisièmement, nous avons utilisé Github.  
Nous semblons avoir relevé le défi de la diffusion et de l'impact théorique puisque nos articles ont un taux de citation élevé. Notre article sur le test $t$ de Welch (chapitre 2) a à ce jour été cité 387 fois. Notre article sur l'ANOVA de Welch (chapitre 3) a déjà été cité 61 fois et le preprint sur les mesures de taille d'effet a été source d'intérêt pour plusieurs chercheurs.

\color{brown} Bien que nos trois premiers articles soulignent des questions importantes, toutes les recommandations que nous y faisons permettent d'améliorer la fiabilité des résultats en cas de rejet de l'hypothèse nulle, mais n'apportent aucune solution pour démontrer l'*absence* d'effet. Contrairement au TOST. Ce test commence à gagner en popularité, notamment grâce aux travaux de Lakens. Même si ce test est simple et efficace, il est important de continuer à le comparer à de nouvelles techniques existantes. C'est ce que nous avons fait à travers notre article comparant le TOST et le SGPV *relire article pour écrire un petit paragraphe là-dessus). A nouveau, nous avons attaché beaucoup d'importance à la transparence et à l'accessibilité de ce travail. Nous avons cette fois encore réalisé un preprint et régulièrement mis à jour un lien Github permettant d'appréhender l'évolution de l'étude. L'article a été soumis dans la revue "MetaPsychology". Dans cette revue, même le processus re Reviewing est transparent!*

### Apports appliqués

\color{blue} Bien qu'il est très important d'aider les chercheurs à comprendre, théoriquement, en quoi les pratiques actuelles sont problématiques, il nous semble plus important encore de leur fournir des outils, afin de leur permettre de modifier ces pratiques. Or, un article méthodologique à lui seul suffit rarement à cela (d'après @mills_quantitative_2010, les chercheurs appliqués citent très peu les articles méthodologiques dans leur référence pour justifier leurs choix, ce qui pourrait être un signal du fait qu'ils basent peu leurs décisions sur ces articles). 

Dans l'autre sens, on constate que les articles méthodos sont généralement peu cités, et ils le sont encore 3 fois moins par les chercheurs appliqués que par les autres méthodologistes [@mills_quantitative_2010, p.56]. On est en droit de questionner l'impact réel des publications méthodologiques, pour 2 raisons, d'après @mills_quantitative_2010:  
(1) Les chercheurs appliqués sont noyés sous les articles dans leur domaine d'expertise si bien que cela limite le temps dont ils disposent pour se consacrer aux articles méthodologiques;   
(2) malgré que des nouvelles méthodes sont disponibles, les chercheurs continuent à opter pour des tests traditionnels et familiaux (mais souvent inappropriés).  

Par contre, les articles méthodos peuvent servir à la créations d'outils/logiciels qui seront très utiles aux chercheurs. *Retravailler cette partie sur l'importance des simulations et des logiciels mordernes pour enseigner les statistiques fréquentistes*:

On sait que les chercheurs tendent à privilégier les méthodes qui sont proposées par défaut dans des logiciels de clique bouton (comme SPSS). C'est en tout cas ce que dit @counsell_reporting_2017 dans le contexte de la gestion des données manquantes (mais je crois que c'est vrai pour tout). Une manière d'améliorer les pratiques serait d'améliorer les options proposées par défaut dans les logiciels de clic-bouton. C'est à ce genre de choses que j'aspire à travers mes articles. 

*[ Par manque de connaissances, les chercheurs se contentent souvent des informations fournies dans les logiciels clic/bouton. "for example, if software does not report a CI on Cohen's $d$, it is unlikely that a researcher will calculate one his or herself" (@counsell_reporting_2017). Une chance qu'on a, c'est Jamovi (regarder si Jamovi me cite)]*

*[Anecdote, pour quand je parlerai des logiciels et de leur intérêt: les chercheurs font souvent l'erreur de croire qu'il faut vérifier la normalité de la VD en faisant une régression. Dans SPSS, il est assez complexe de le faire car il faut d'abord calculer les résidus, ce qui implique de comprendre que les tests t et ANOVA sont des cas particuliers de régression, puis ensuite a posteriori représenter graphiquement les résidus. C'est chronophage et complexe. Dans Jamovi, par contre, la vérification de la normalité des résidus est automatiquement réalisée lorsqu'on fait un test t. Le rôle des méthodologistes, à mon sens, est de prémacher le travail, pour permettre à d'autres de créer des outils conçus pour améliorer les pratiques de recherche. à partir du moment où c'est automatiquement fait correctement, il devient moins problématique que les psychologues maîtrisent le détail. Débarassés de ces questions, ils pourront peut-être alors plus se focaliser sur l'important pour mieux comprendre et interpréter les résultats de leur tests: càd comprendre la distribution d'échantillonnage, dont pratiquement tt découle.]*

Malgré tout, un logiciel ne fait pas tout et après avoir utilisé le test adéquat, il est important d'être capable de l'interpréter correctement. Les tests font appel à des notions faussement simples telles que les p-valeurs et les distributions d'échantillonnage. A mon sens, le seul moyen d'enseigner correctement ces notions, c'est à travers des simulations. 

D'après Thompson (1999a, cité par @fraas_testing_2000), les chercheurs continuent à utiliser la nil nul hypothesis pour 2 raisons:  (1) la plupart des logiciels partent du postulat que c'est l'hypothèse nulle qu'utilisent les chercheurs et ne donnent pas la possibilité de faire autre chose  (2) les non nil-nul hypotheses incluent un niveau de complexité pas toujours possible à atteindre dans bcp de designs.  Fraas et Newman (2001) admettent que les chercheurs sont probablement plus enclins à utiliser des procédures si elles sont implémentées dans des logiciels "user friendly".

--> Concernant la raison (1), ce n'est plus tellement vrai en 2021. Jamovi, par exemple, contient un package "TOSTER" qui permet de faire des tests d'effets minimaux ET des tests d'équivalence. Il est très important que des logiciels le fassent, car comme disaient @fraas_testing_2000, "unless researchers are able to test non-nil null hypotheses with readily available computer software, they may continue to exclusively use nil null hypothesis" (p.4). 

Au final, nos articles ont souvent été utiles pour ce genre de choses:

- Pour le Welch, vérifier mais je crois que Daniel s’en était aussi servi pour le package TOSTER (ou en tout cas en parlait).
Nous avons créés des outils pour aider les autres chercheurs (packages R et Shiny App).
De plus, nos articles ont été utilisés par d’autres chercheurs qui ont eux-mêmes créés des outils utiles. Par exemple, notre dernier article sur l’ES a inspiré plusieurs chercheurs :
-	Aardon Caldwell s’est appuyé sur nos équations pour implémenter le g* de Hedges dans sa mise à jour du Package TOSTER (sur Jamovi)
-	Intrajeel Patil (twitter) et son collaborateur se sont servis de notre preprint pour corriger une erreur dans un package R qu’ils ont créés
	Ça met en lumière ce à quoi à mon avis les articles méthodos doivent servir : à inspirer la création de packages/fonctions dans des logiciels user friendly et communément utilisés pour que les chercheurs sachent concrètement comment agir ! (citer la source qui dit que bien souvent, les chercheurs ne vont utiliser des techniques que si elles sont implémentées dans ce type de logiciel)
Pour continuer à mettre à jour les logiciels, il est important de continuer à éprouver les méthodes existantes, et à les comparer aux nouvelles méthodes existantes. Comme la SGPV proposé par Blume était une nouvelle statistique qui semblait remplir les mêmes objectifs que le TOST, il était tout naturel de comparer ces deux techniques de la manière la plus détaillée possible. Peut-être que de nouvelles techniques avec un meilleur ratio coût/bénéfice seront mises en lumière et viendront dès lors naturellement remplacer les techniques existantes (c’est la magie de la science : un questionnement critique et des mises à jours régulières).
Par exemple, on a pu montrer effectivement que conformément à ce que certains auteurs suggéraient mais sans que ça soit entendu dans le mnode de la psychologie (car pas appliqué à une compréhension claire du fait que l’hétéroscédatisticité était presque inhérente au domaine, par exemple), il vaut mieux utiliser le test de Welch tt le tps.

Je peux aussi signaler que confirmer une hypothèse nulle, ce qui est important car il y a pas mal de domaines dans la psycho ou le but est de montrer les similitudes, surtout quand c’est mêlé à des questions sociétales (est-ce que les couples homosexuels sont aussi performants que les couples hétérosexuels ?)

## Limites

En recherche, on apprend constamment de nos erreurs. Pour chaque article, j’ai pu identifier des éléments que je ne reproduirais plus à l’identique avec dû recul. Par exemple, dans l’article sur le test de Welch (le premier) j’aurais dû fragmenter. Commencer par faire des simultions avec des distributions identiques dans tous les groupes (car ça permet de moins bien visualiser l’effet de compensation, ex . quand asymétrie positive et négative en mm tps). J’ai également identifié des erreurs dans certains articles 

Concernant le premier article sur le test $t$ de Welch (vérifier mais je crois que ça s’applique aussi au 2ème article en fait ces erreurs) Nous spécifions à plusieurs reprises que le test $t$ de Yuen contrôle moins bien le taux d'erreur de type $I$ que le test $t$ de Welch:  
- p.14: *"Yuen's $t$-test is not a good unconditional alternative because we observe an unacceptable departure from the nominal alpha risk of 5 percent for several shapes of distributions [...] particularly when we are studying asymmetric distributions of unequal shapes"*;  
- p.15: *"As it is explained in the additional file, Yuen's $t$-test is not a better test than Welch's $t$-test, since it often suffers high departure from the alpha risk of 5 percent"*.   

Ceci n'est pas exact d'un point de vue purement statistique. A travers le test de Yuen, on ne compare plus les moyennes de chaque groupe, mais les moyennes *trimmées* (soit les moyennes calculées sur les données après avoir écarté les 20$\%$ des scores les plus faibles ainsi que les 20$\%$ des scores les plus élevés). Or, à travers nos simulations, les scénarios créés en vue de tester le taux d'erreur de type $I$ (risque alpha) étaient systématiquement des scénarios dans lesquels les moyennes de chaque population étaient identiques. Lorsque la distribution d'une population est parfaitement symétrique, la moyenne et la moyenne trimmée seront identiques. Au contraire, lorsque la distribution d'une population est asymétrique, la moyenne et la moyenne trimmée diffèreront (la moyenne trimmée sera plus proche du mode de la distribution et donc, représentera mieux cette dernière). Notons malgré tout que d'un point de vue méthodologique, nous avons déjà relevé que la plupart du temps, les chercheurs définissent l'absence de différence entre les moyennes comme hypothèse nulle et nos simulations démontrent que dans ce contexte, le test de Yuen n'est pas approprié. En conclusion, le test de Yuen ne devrait être utilisé que par des chercheurs ayant pleinement conscience du fait que les tests $t$ de Student et de Welch ne reposent pas sur la même hypothèse que le test $t$ de Yuen.

*Revoir si je parle du fait que le kurtosis impacte la puissance du test de Welch dans l'article en tant que tel (je le fais en tout cas dans les annexes). Expliquer que j'avais fait une erreur en confondant kurtosis et sd (j'ai cru à tort que la mesure de dispersion de la double expo était le sd alors qu'en fait non). Ca m'a fait prendre conscience qu'il est SUPER important de toujours demander, dans les simulations, le calcul des descriptives, afin de vérifier que tout s'est bien passé (si la variance moyenne n'est pas égale à la variable théorique, par exemple, en tt cas qd la condition de normalité est ok, c'est qu'il y a eu un couac). En plus de permettre un contrôle des erreurs, ça peut être utile comme aide à l'interprétation. A partir de l'article 2, je l'ai systématiquement fait. Et pour me "rattraper", j'ai expliqué en détail la différence entre le kurtosis et le SD dans le 2ème article.*

### Commentaires divers

Dans les deux articles sur le Welch: (même si les pages relevées concerne le test $t$, c vrai aussi pour le suivant):
- p.9: nous décrivons 3 arguments en défaveur de l'usage du test de Levene. En troisième argument, nous mentionnons le manque de puissance du test de Levenne. Ceci est rappelé en conclusion de l'article présenté au sein du chapitre 2: *"Because the statistical power for this test is often low, researchers will inappropriately choose Student's $t$-test instead of more robust alternatives"*. Nous aurions pu ajouter le fait qu'utiliser le test $t$ de Student lorsque le test de Levene est non significatif revient à confondre le non rejet de l'hypothèse d'égalité des variances avec l'acceptation de l'hypothèse d'égalité des variances. Au sein du chapitre 5 sur les tests d'équivalence, il est démontré par simulation que même lorsqu'on s'assure d'avoir une puissance suffisante pour détecter une différence attendue, la stratégie qui consiste à interpréter le non rejet de l'hypothèse nulle comme un soutien en faveur de l'hypothèse nulle n'est pas appropriée.  
- p.12: nous mentionnons ceci : *"When both variances and sample sizes are the same in each independent group, the $t$-values, degrees of freedom, and the $p$-values in Student's $t$-test and Welch's $t$-test are the same (see Table 1)*. Avec du recul, cette phrase peut porter à confusion. Par "variances" il faut comprendre "*sample* variances" ou "variances *estimates*". Nous ne sommes donc *pas* en train de dire que les deux statistiques, ainsi que les degrés de liberté et $p$-valeurs qui leur sont associées seront identiques lorsque la condition d'homogénéité des variances sera respectée au niveau de la population, mais bien lorsque les estimations de chaque variance de population seront identiques.

Limites dans le chapitre 4, on compare essentiellement les estimateurs sur base de leurs propriétés inférentielles. Nous avons tenté de prendre la dimension interprétative en compte, mais c'est parfois très compliqué. Cette dimension est d'ailleurs rarement prises en compte par les chercheurs. On constate que même si les mesures de taille d'effet sont de plus en plus fréquemment reportées, elles ne sont que rarement interprétées et incluses dans les discussions [@funder_evaluating_2019;@thompson_statistical_1997] par les chercheurs. Dans un tel contexte, il est particulièrement important d'ouvrir les débats sur cette question.

## Perspectives futures

\color{blue} *t-test*: "We do not include the bootstrapped $t$-test because it is known to fail in specific situations, such as when there are unequal sample sizes and standard deviations differ moderately"(p.8; Hayes & Cai, 2007): on s’est contenté de croire l’avis de machin qui dit que ça marche pas bien, mais on pourrait requestioner cela et dans les recherches futures le ré-investiguer la comparaison du test de welch classique avec sa version boostrappé. --> relire son artic pour voir dans quelles conditions ils ont étudié le t boostrappé. Pe pas les mêmes que nous! Dc ça pourrait être utile de refaire la même étude mais en comparant uniquement le $t$ de Welch à sa version boostrappée.

-	Je ne travaille pas avec des distributions discrètes.


Take home message :
Ce qu’il faut retenir de ma thèse : les recommandations en qlq points.  Parler ici du fait que j’ai créé des packages et les indiquer : donner le lien github vers ces packages (même si déjà donné ailleurs). 
