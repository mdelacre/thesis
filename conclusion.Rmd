---
title: ''
output: pdf_document

header-includes:
  - \usepackage{xcolor}
  - \usepackage{bm}
---

# Discussion générale et conclusion

A travers cette thèse, nos objectifs de départ étaient (1)	d’identifier des manquements dans les pratiques actuelles des chercheurs, via des analyses d’articles publiés dans des revues de psychologie ;  (2)	de réaliser des simulations, en vue de montrer l’impact de ces pratiques et (3)	de proposer des recommandations pour les améliorer.  

Dans un premier temps, nous nous sommes focalisés sur l'usage des tests $t$ de Student et $F$ de Fisher, soit des tests communément utilisés par les chercheurs en psychologie, en vue de comparer les moyennes de deux ou plusieurs groupes de sujets indépendants, et qui reposent sur les conditions que des résidus, indépendants et identiquement distribués, soient extraits d'une distribution normale et que les variances des populations dont sont extraits chaque groupe soient identiques (soit la condition d'homogénéité des variances). \color{blue}Bien que les enjeux des conditions statistiques de ces tests aient déjà été largement explorés par le passé, ils semblaient toujours largement ignorés par de nombreux chercheurs appliqués. Notre principale motivation à aborder cette thématique était dès lors d'ordre pédagogique: il nous semblait nécessaire de combler le fossé entre les méthodologistes et la majorité des chercheurs appliqués. Il nous est d'abord apparu que la litérature manquait d'articles expliquant de manière compréhensible les raisons pour lesquelles les conditions statistiques des tests $t$ de Student et $F$ de Fisher étaient peu réalistes. \color{black}Nous avons dès lors mis en évidence toute une série d'arguments qui permettent de remettre en cause la crédibilité, dans de nombreux domaines de la psychologie, des conditions statistiques de normalité (comme la présence de sous-populations définies par des facteurs non identifiés dans le design, l'étude de mesures bornées, telle(s?) que le temps qui ne peut prendre des valeurs négatives, ou encore le fait qu'un traitement est susceptible de modifier la forme des distributions étudiées) et d'homogénéité des variances (comme l'étude de groupes pré-existants à l'expérience, définis par des variables telles que le genre ou l'origine ethnique\footnote{Dans ce cas, les sujets ne sont pas répartis aléatoirement entre les groupes. L'hétérogénéité des variances entre les groupes et dès lors le résultats de la violation de la condition méthodologique d'indépendance des résidus.}, ou encore le fait qu'un traitement, qu'il soit expérimental ou quasi-expérimental, est susceptible d'agir sur tous les paramètres d'une distribution, incluant sa variance). \color{blue} Ensuite, grâce aux avancées informatiques récentes, nous ont pu étendre les travaux déjà engagés par de nombreux auteurs avant nous [voir par exemple @harwell_summarizing_1992], en vue de montrer les conséquences réelles de la violation des conditions de normalité et d'homogénéité des variances pour respectivement les tests statistiques $t$ de Student (chapitre 2) et $F$ de Fisher (chapitre 3). Nous avons à cette fin réalisé des simulations intensives, avec 1,000,000 d’itérations pour un nombre très vaste de scénarios, variant en fonction d'un ensemble de paramètres connus pour jouer un rôle clé sur les taux d'erreur de type I et II des test $t$ de Student et $F$ de Fisher. \color{black} Il est ressorti de nos simulations que de manière consistante avec nos attentes théoriques, lorsque les deux échantillons comparés sont de même taille, le test $t$ de Student est robuste aux violations de la condition d'homogénéité des variances. Par contre, il en est différemment lorsque les échantillons sont de tailles différentes: sur le long terme, la probabilité de rejeter l'hypothèse nulle avec ce test est supérieure aux attentes théoriques lorsque le plus petit échantillon est extrait de la population ayant la plus grande variance, et est inférieure aux attentes théoriques lorsque le plus petit échantillon est extrait de la population ayant la plus petite variance. Dans la mesure où l'ANOVA $F$ de Fisher est une généralisation du test $t$ de Student\footnote{L'ANOVA $F$ de Fisher peut être utilisée lorsqu'on compare deux ou plus de deux échantillons indépendants sur base de leur moyenne. Lorsqu'on compare exactement deux groupes, le test $t$ de Student et l'ANOVA $F$ de Fisher sont strictement équivalents. En effet, ils entretiennent la relation mathématique suivante: $F(1,x) = t^2(x)$.}, il n'est pas surprenant que nos simulations relatives à l'ANOVA $F$ de Fisher aient amené à des constats semblables à ceux obtenus sur base de nos simulations relativement au test $t$ de Student. En outre, ces simulations nous ont permis de faire deux constats supplémentaires: d'abord, lorsqu'on compare plus de deux groupes, l'ANOVA $F$ de Fisher est affectée par la présence d'hétérogénéité des variances, même lorsque tous les échantillons sont de tailles identiques. Dans ce cas, le test devient plus liberal, ce qui signifie qu'il amène à rejeter l'hypothèse nulle plus souvent qu'attendu théoriquement, sur le long terme. Ensuite, plus le nombre d'échantillons comparés est important, plus le test est affecté par les violations de la condition d'homogénéité des variances. \color{blue} Si la prise de conscience des limites d'une méthode est un premier pas très important, il est tout aussi important de savoir comment pallier ces limites. C'est pour cette raison que nos simulations incluaient également les résultats de tests théoriquement jugés comme constituant de bonnes alternatives, plus robustes en cas de violation de la condition d'homogénéité des variances, à savoir les tests $t$ de Welch, $F$ de Fisher et $F^*$ de Brown-Forsythe. Par ailleurs, il est souvent recommandé aux chercheurs de tester préalablement la condition d'homogénéité des variances, via un test de Levene par exemple, et ensuite d'utiliser soit le test $t$ de Student (ou $F$ de Fisher) soit une alternative plus robuste aux écarts à la condition d'homogénéité des variances, suivant que cette condition soit ou non respectée. \color{red} Nous avons dès lors également réalisé des simulations en vue d'éprouver le test de Levene. \color{black} Il est apparu que \color{red} ce dernier est souvent peu apte à détecter les écarts à la condition d'homogénéité des variances. \color{black} Il est également apparu que lorsqu'on compare les taux d'erreurs de type I et II des tests $t$ de Student et $t$ de Welch, on réalise que le test $t$ de Welch est pratiquement aussi puissant que le test $t$ de Student lorsque la condition d'homogénéité des variances est respectée, et contrôle bien mieux les taux d'erreur de type I et II lorsqu'elle ne l'est pas. De même, en cas d'homogénéité des variances, le test $W$ de Welch est très légèrement inférieur aux tests $F^*$ de Brown-Forsythe que $F$ de Fisher, tant en termes de contrôle des erreurs de type I et II qu'en termes de consistances entre les puissances théoriques et observées. Par contre, il leur est bien supérieur en cas d'hétérogénéité des variances. \color{blue}Enfin, il nous semblait indispensable de résumer le message clé de nos articles à travers des recommandations claires et précises. Cela nous a semblé d'autant plus important que bien souvent, les chercheurs appliqués sont noyés sous les articles dans leur domaine d'expertise si bien que cela limite le temps dont ils disposent pour se consacrer aux articles méthodologiques [@mills_quantitative_2010]. \color{red} Or, la formulation de directives précises permet de rentabiliser ce temps (checher référence là dessus). \color{black} Compte tenu du fait que la condition d’homogénéité des variances est plus souvent l’exception que la norme et qu’il est parfois très difficile (voire impossible) de détecter les écarts à cette condition à travers des tests, nous recommandons l’usage des test $t$ (ou $F$) de Welch par défaut. Cette recommandation s'applique au moins au cas où les échantillons sont de taille différente, lorsqu’on ne compare que deux groupes, et dans tous les cas lorsqu’on compare plus de deux groupes. \color{blue} Les choix de comparer les tests $t$ de Student et $F$ de Fisher respectivement aux tests de Welch et de Brown-Forsythe et finalement de recommander l'usage des tests de Welch ne se sont pas faits par hasard. Ils étaient fortement guidés par le désir de proposer des stratégies qui pourraient être facilement comprises et appliquées par la grande majorité des chercheurs. Comme nous l'avions déjà mentionné en introduction, il existe des tests qui sont plus robustes simultanément aux violations des conditions de normalité et d'homogénéité des variances, tels que les tests où l'on compare des moyennes trimmées [@wilcox_results_1994; @wilcox_how_1998] ou encore les tests non paramétriques. Cependant, ces tests étaient à nos yeux moins susceptibles de provoquer l'adhésion de la majorité des chercheurs, pour deux raisons essentielles. Premièrement, ces tests ne reposent pas sur la même hypothèse nulle que les tests $t$ de Student et $F$ de Fisher, puisqu'on n'y compare plus les moyennes de chaque groupe. L'usage des tests de Welch, au contraire, constituent un moyen simple d'améliorer les pratiques sans pour autant obliger à repenser la manière de définir l'hypothèse nulle. Deuxièmement, les tests de Welch sont déjà implémentés dans la plupart des logiciels courants tels qu'SPSS, Jamovi et R. C'est même la stratégie proposée par défaut dans Jamovi et R, ce qui est important compte tenu de la propension des chercheurs à privilégier les méthodes proposées par défaut dans les logiciels [@counsell_reporting_2017]. \color{blue}Nous ne voulons pas sous-entendre que les tests reposant sur les moyennes trimmées ou les tests non paramétriques sont à bannir (nous parlerons notamment du test de Yuen, que nous avons peut-être injustement sous-estimé au sein de l'article du chapitre 2, dans la section dédiée aux limites de cette thèse) \color{red} et encore moins qu'un outil statistique n'est pas digne d'intérêt s'il n'est pas déjà implémenté dans les logiciels courants (comme nous y reviendrons, il est de plus en plus abordable de proposer de nouveaux outils, par exemple via R). \color{blue}Nous pensons simplement qu'il était plus réaliste, dans un premier temps, de s'assurer que les hypothèses généralement définies par les chercheurs soient testées correctement, avant d'amener une réflexion sur la manière dont on peut améliorer leur définition. 

\color{black}Dans un deuxième temps, nous nous sommes intéressés à la significativité pratique des effets étudiés, au delà de leur significativité statistique. Nous avons noté que dans le contexte de la comparaison de deux groupes, lorsque les chercheurs calculent une mesure de taille d'effet, ils rapportent souvent le $d$ de Cohen, une mesure qui dépend exactement des mêmes conditions d'application que les tests $t$ de Student et $F$ de Fisher. [...] En ce qui concerne la mesure $d$ de Cohen, nous avons mis deux éléments principaux en évidence. D'abord, il s'agit d'une mesure toujours biaisée, mêmelorsque toutes les conditions dont elle dépend sont respectées. Heureusement, une transformation de cette mesure existe telle que le biais devient nul lorsque la condition de normalité des résidus est respectée. Cette transformation a été proposée par Hedges et porte dès lors son nom: la mesure $g$ de Hedges. Ensuite, une violation de la condition d'homogénéité des variances amènera à une forte augmentation de la variance des estimateurs $d$ de Cohen et $g$ de Hedges, même lorsque les deux échantillons sont de taille identique. Différents estimateurs ont été proposés dans la littérature en vue de remplacer le $d$ de Cohen (et le $g$ de Hedges) en cas de violation de la condition d'homogénéité des variances. Parmis ceux-ci, on retrouve fréquemment le $d$ de Glass, qui peut être transformé de sorte à obtenir le $g$ de Glass, théoriquement non biaisé lorsque les résidus se distribuent normalement. Nos simulations ont révélé que la variance du $g$ de Glass de même que son biais (lorsque les résidus sont extraits de populations qui ne se distribuent pas normalement) dépendent fortement de paramètres que l'on ne peut contrôler, ce qui nous amène à décourager l'usage de cette mesure. Dans la litérature, on retrouve également la mesure $d$ de Shieh, qui entretient une relation mathématique directe avec le $t$ de Welch, ainsi que la mesure $d^*$ de Cohen qui, contrairement au $d$ de Cohen classique, implique le calcul de la moyenne *non poolée* des variances de chaque groupe. De même que pour les estimateurs précédemment cités, il est possible de transformer ces mesures en vue de supprimer le biais lorsque la condition de normalité des résidus est respectée. Cela donne respectivement lieu aux mesures $g^*$ de Hedges et $g$ de Shieh. Grâce à nos simulations, nous avons révélé que le $g^*$ de Hedges est supérieur au $g$ de Shieh, non seulement d'un point de vue inférentiel (contrairement au $g$ de Shieh, le $g^*$ de Hedges est consistant, ce qui signifie que sa variance diminue toujours lorsque les tailles d'échantillon augmentent, de même que son biais lorsque les résidus sont extraits d'une population anormale) que d'un point de vue interprétatif (sa valeur est constante, peu importe que les deux échantillons soient de taille identique ou non). Finalement, lorsqu'on compare les mesures $g$ de Hedges et $g^*$ de Hedges, on constate que le $g^*$ de Hedges n'est très légèrement inférieur au $g$ de Hedges, en termes de biais et de variance, que lorsque des échantillons de tailles différentes sont extraits de population aux variances identiques. Il est tout aussi efficace que le $g$ de Hedges lorsque tant les tailles d'échantillons que les variances de population sont identiques. De plus, il reste valide lorsque la condition d'homogénéité des variances n'est pas respectée, contrairement au $g$ de Hedges. Pour les mêmes raisons que celles qui nous amènent à privilégier les tests $t$ de Welch et $F$ de Welch par défaut, nous recommandons de privilégier systématiquement le $g^*$ de Hedges. 

Dans un troisième temps, nous nous sommes concentrés sur la tendance des chercheurs à définir par défaut, comme hypothèse nulle, une hypothèse d'absence d'effet. Nous avons souligné que cette tendance persiste même lorsque l'objectif est de prouver une absence d'effet: c'est alors sur base d'un non rejet de l'hypothèse nulle que les chercheurs affirment pouvoir valider leur hypothèse. Pourtant, nous avons vu que ce n'est pas une stratégie adéquate puisque non seulement le test utilisé de cette manière présente de faibles propriétés asymptotiques, mais en plus, la probabilité que le test amène à conclure à l'absence d'effet augmente à mesure que l'erreur de mesure augmente. Nous avons également souligné qu'en réalité, il n'existe aucun test d'hypothèses qui permette de démontrer l'absence totale d'effet. Par contre, il est possible de démontrer qu'un effet observé ne s'éloigne pas de l'absence d'effet d'une quantité supérieure à une valeur définie (dit autrement, qu'il est *équivalent*), à condition de comprendre qu'il est théoriquement possible de définir n'importe quelle différence (ou intervalle de différences) de moyennes comme hypothèse nulle. C'est le principe sur lequel repose le TOST (Two One-Sided Tests), à travers lequel on conclut à l'équivalence à condition que l'intervalle de confiance à $(1-2\alpha)\%$ autour de l'effet étudié soit entièrement inclus à l'intérieur de la zone d'équivalence. Récemment, @blume_second-generation_2018 ont proposé un nouvel outil qui se nomme le SGPV (Second Generation P-Value) qu'ils définissent comme la proportion des valeurs de l'intervalle de confiance à $(1-\alpha)\%$  qui sont également compatibles avec l'hypothèse nulle (ou autrement dit, qui se situent à l'intérieur de la zone d'équivalence). Il nous a semblé pertinent de comparer le SPGV au TOST, dans la mesure où les deux stratégies reposent sur un principe similaire, à savoir la comparaison de l'intervalle de confiance de l'effet observé avec la zone d'équivalence. Cependant, notre comparaison n'a pas permis de mettre en évidence de réelle plus-value du SGPV par rapport au TOST. Bien que @blume_second-generation_2018 présentent le SGPV comme un outil permettant de déterminer à quel degré les données sont compatibles avec l'hypothèse d'équivalence, nous avons révélé au moins deux situations pour lesquelles cette définition ne tient pas: lorsque l'intervalle de confiance autour de l'effet observé recouvre les deux bornes de la zone d'équivalence tout en ayant une largeur moins de deux fois supérieure à celle de la zone d'équivalence, et lorsque les intervalles de confiance sont asymétriques, ce qui est le cas, par exemple, lorsqu'on étudie une corrélation $r$ de Pearson (tel que décrit dans l'article du chapitre 5) ou encore lorsqu'on étudie des mesures de taille d'effet standardisées de la famille $d$ (ces dernières ayant fait l'objet du chapitre 4). In fine, les seules situations pour lesquelles le SGPV permet de tirer une conclusion claire sont celles où sa valeur vaut exactement 0 ou 1. Or, les conclusions tirées dans ce cas sont similaires, mais moins précises, à celles que permettent de tirer le TOST. 
*Blume et ses collaborateurs ont introduit un outil qui n'apporte pas grand chose*: 
*1) les seules valeurs que l'on peut facilement interpréter, ce sont 0 et 1 (mais ces valeurs correspondent à une p-valeur de TOST respectivement > .975 ou < .025 dc ça ne fait rien de plus que le TOST lorsqu'on l'utilise d'un point de vue Neyman-Pearson, où l'on compare la p-valeur au risque alpha, et fait moins que le TOST lorsqu'on l'utilise du point de vue de Fisher car la p-valeur différencie là où le TOST vaut tj 0 ou 1)*.
*2) Il faut apporter une correction pour éviter une mauvaise interprétation quand l'IC est trop large (alors que pas besoin de correction avec le TOST). De plus, la correction exclut toute une série de situations (où l'IC chevauche les deux bornes de l'IC mais en étant moins que 2 fois plus grand que la zone d'équivalence). Et parfois, la correction apparaît quand ce n'est pas nécessaire, comme on le voit à travers la figure 13 de l'article du chapitre 5.* *3) Blume et al. sous-entendent que le SGPV permet d'éviter les correction spour comparaison multiples. Mais c'est faux, vu la correspondance parfaite entre TOST et SGPV quand il s'agit de décider si on a un soutien en faveur de l'équivalence ou pas, ça démontre bien que dans les 2 cas, on peut avoir une déformation des taux d'erreur de type I et II.*


### d de Cohen

Les simulations Monte Carlo présentées au sein du chapitre 4 ont été réalisées suite au constat d'un désaccord, de la part des chercheurs, quant à la mesure de taille d'effet la plus appropriée lorsqu'on compare deux groupes sur base de leur moyenne. A nouveau guidés par le désir d'être aussi pédagogiques que possibles, nous y avons exposé précisément les critères sur base desquels nous souhaitions comparer les différents estimateurs. Bien que l'article n'ait pas encore été accepté pour publication, il a généré des débats très enrichissants, grâce au preprint que nous avons diffusé en (date?). Essentiellement, le preprint a été à l'origine d'échanges très enrichissant avec Geoff Cumming (l'essentiel de ces échanges est reporté dans l'annexe C). L'apport théorique de ces échanges a été très riche, essentiellement parce que nous partions d'avis très divergents. Alors que nous mettions essentiellement l'accent sur les propriétés inférentielles des estimateurs à comparer, Cumming accordait beaucoup plus d'importance à la dimension interprétative. Cela nous a fait prendre conscience des certaines limites de l'article, tel qu'il est écrit actuellement, et également de pistes possibles d'amélioration. 

Il est finalement ressorti de ces échanges qu'il n'est pas approprié de donner plus de poids à la dimension inférentielle qu'à la dimension interprétative. Nous semblons pourtant sous-entendre par endroit que de bonnes propriétés inférentielles suffisent à compenser des difficultés en termes d'interprétation (notamment en ce qui concerne la présentation de la mesure $d$ de Shieh, puisque nous écrivons "..."). De tels propos risquent de réduire l'intérêt et la portée pédagogique de notre travail. Un estimateur non biaisé et présentant une faible variance n'a que peu d'intérêt s'il est difficilement interprétable. Cela devient surtout apparent lorsqu'on souhaite définir des hypothèses nulles autres que l'absence de différence, soit des hypothèses prenant en compte la significativité pratique des estimateurs, tel qu'abordé au sein du chapitre 5. Admettre cela ne signifie cependant pas pour autant qu'il ne faille accorder aucune importance aux propriétés inférentielles d'un estimateur: un estimateur ne peut être interprété correctement si sa valeur varie en fonction de facteurs non contrôlables, tels que la vraie corrélation entre la différence de moyenne et le standardiseur, cette dernière survenant lorsque les données sont extraites de populations qui se distribuent asymétriquement. Cela permet d'ouvrir le débats sur la manière de rendre un estimateur interprétable. *parler des auteurs qui proposent des benchmarks plus appropriées que celles de Cohen*.

Apport pratique: Mettre toute la partie où je signale qu'un article méthodo suffit raremetn à lui seul. Dans les deux articles d'avant, il n'étai tpas nécessaire de créer des outils dans la mesure où le Welch est déjà dans pleins de logiciels très utilisés. Mais ce n'est pas le cas pour le g* de Cohen. D'où les packages et Shiny App. 

### TOST vs. SGPV

Apport essentiellement théorique puisqu'on critique le SGPV. Il n'y a pas eu vraiment d'apport pratique puisque le TOST a déjà fait l'objet d'outils concrets par le premier auteur de l'article (Lakens).


# La plus-value de la transparence: parler de l'échange avec Cumming.

\color{blue} Tout au long du processus, de l'écriture à la soumission de nos articles, nous avons accordé beaucoup d'importance à la transparence et à l'accessibilité. Lors de la réalisation des deux premiers articles, nous avons effectué les démarches suivantes:  
1) Avant que l'article ne soit accepté pour publication, nous avons diffusé un preprint sur les réseaux sociaux (Facebook, Twitter...)  
2) Nous avons soumis l'article dans une revue Open Access (*l'International Review of Social Psychology*).          
3) Nous avons rendu disponible en ligne gratuitement et en open access tous les scripts de nos simulations et analyses. Nous l'avons fait dans un premier temps en utilisant la plateforme de l'OSF (Open Science Framework) et ensuite via Github (à définir?)  
Ces démarches semblent avoir porté leur fruit, compte tenu du taux de citation de l'article.

Lors de la réalisation des deux articles suivants, nous avons été un peu plus loin puisque nous avons rendu tout le développement du processus d'écriture visible, via Github. 

Expliquer en quoi toutes ces démarques ont été utiles pour nous.

**Limites**
nous avons porté un jugement trop sévère à l'égard du test de Yuen qui repose sur le principe de comparaison des moyennes trimmées. Ce jugement reposait sur une mauvaise compréhension de notre part des objectifs du test.

S'il est certain que les tests de Welch sont bien plus robustes que les tests $t$ de Student et $F$ de Fisher en cas d'hétérogénéité des variances, il est tout aussi certain qu'ils ne résolvent pas tous les problèmes. Par exemple, comme le révèlent les figures de l'article présenté au sein du chapitre 3, le test de Welch est sensible à certaines violations de la condition de normalité \footnote{Entre autre, la puissance du test est altérée (c'est-à-dire qu'elle est non conforme aux attentes théoriques) lorsque les données sont extraites de distributions fortement asymétriques ou qui ont des extrêmités très denses. }