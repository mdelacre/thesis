---
title: " "
output: 
  papaja::apa6_pdf:
    includes:
      extra_dependencies: ["float"]

header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
---
# Chapitre 5: Les tests d'équivalence

Lorsqu'on applique un test d'hypothèse, l'hypothèse nulle la plus couramment définie est celle d'absence d'effet ou de différence entre les groupes [@nickerson_null_2000]. Il arrive également parfois que les chercheurs définissent un intervalle de valeur comme hypothèse nulle, mais le plus souvent, cet intervalle est borné par la valeur 0 [@nickerson_null_2000], on parle alors d'hypothèse unilatérale. Avec cette stratégie, le rejet de l'hypothèse nulle constitue un soutien en faveur de la présence d'un effet non nul, par contre, le non-rejet de l'hypothèse nulle ne peut être interprété comme un soutien en faveur de l'absence d'effet. Pourtant, il arrive souvent que des chercheurs l'interprètent de la sorte [@anderson_theres_2016]. @finch_reporting_2001, par exemple, ont rapporté que parmi 150 articles publiés entre 1940 et 1999 dans le *JAP* (*Journal of Applied Psychology*), 38% interprétaient un résultat non significatif comme une acceptation de l'hypothèse nulle. Plus récemment, @lakens_equivalence_2017 a noté que l'expression "pas d'effet" a été utilisée dans 108 articles publiés dans *Social Psychological and Personality Science* avant août 2016 et que dans presque tous les cas, c'était sur base du non-rejet de l'hypothèse nulle que cette conclusion était tirée. Cette erreur d'interprétation est également fréquemment commise dans le cadre des études de réplication. @anderson_theres_2016, par exemple, ont analysé 50 réplications d'études publiées en 2013 dans PsycINFO. Ils ont noté que 14 études affirmaient avoir obtenu des effets "nuls" (interprété comme un échec à la réplication), et tous l'ont fait sur base de l'acceptation d'une hypothèse nulle d'absence d'effet. C'est par exemple de cette manière qu'on été réalisées la plupart des tentatives de réplications de la célèbre étude de Bem [Ritchie, Wiseman & French, 2012, cités par @anderson_theres_2016]. 

A travers ce chapitre, notre premier objectif sera d'expliquer pourquoi interpréter le non-rejet de l'hypothèse d'absence d'effet comme un soutien en faveur d'une absence d'effet n'est pas une bonne stratégie. Nous introduirons ensuite les tests d'équivalence qui permettent d'obtenir un soutien en faveur d'un effet jugé non pertinent, et plus particulièrement le TOST (Two One-sided test). Nous verrons que l'aspect le plus compliqué de la réalisation du TOST est la définition des bornes d'équivalence. Pour cette raison, notre troisième objectif sera de fournir quelques pistes en vue de définir ces bornes. Pour finir, nous présenterons un article dans lequel nous comparons le TOST à la SGPV (Second Generation *P*-Value), une stratégie récemment développée par @blume_second-generation_2018.

## Limites de l'approche traditionnelle

Lorsqu'on teste une hypothèse nulle, il y a deux conclusions possibles: soit ont la rejette, soit on ne la rejette pas. Si rejeter l'hypothèse nulle amène à conclure en faveur de l'hypothèse alternative, ne pas la rejeter ne permet pas de conclure en faveur de l'hypothèse nulle. Au mieux, cela nous montre que les données ne sont pas incompatibles avec l'hypothèse nulle, mais cela ne veut en aucun cas dire qu'elles ne sont compatibles avec aucune autre hypothèse.  Afin de l'illustrer, la Table 1 résume les résultats de simulations Monte Carlo pour un ensemble de 42 scénarios qui varient en fonction de la taille des échantillons ($n_j$) et de la différence entre les moyennes de population dont sont extraits les échantillons ($\mu_1-\mu_2$). Pour chaque scénario, à 100,000 reprises, nous avons généré aléatoirement une paire d'échantillons indépendants, réalisé un test $t$ de Student pour échantillons indépendants et extrait la $p$-valeur du test.\footnote{Rappelons que le test de Student repose sur la condition peu réaliste d'homogénéité des variances de population. Nous avons assuré le respect de cette condition en générant systématiquement deux échantillons extraits de populations aux variances identiques. Dans la mesure où le message-clé de cette illustration ne dépend pas du test utilisé, nous avons fait ce choix par facilité, en vue de simplifier l'équation de l'erreur standard de la différence de moyennes, qui sera ultérieurement mentionnée.} Ensuite, nous avons calculé la proportion d'itérations associées à une $p$-valeur supérieure à .05, nous amenant à ne pas rejeter l'hypothèse nulle lorsqu'on travaille avec un risque alpha de 5% [ce risque alpha étant communément accepté par la majorité des chercheurs, @meyners_equivalence_2012]. Lorsque l'hypothèse nulle est fausse (toutes les colonnes de la Table 1, à l'exception de la première), cette proportion correspond au taux d'erreur de type II (communément appelé $\beta$). 

```{r "simu", fig.align='left', fig.cap=NULL,echo=FALSE,out.width="95%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Illustration/Table1.png")
```
\newpage
Pour les scénarios de la première colonne, l'hypothèse nulle est vraie: il n'y a pas de différence entre les moyennes de population. Puisque les conditions d'application du test $t$ de Student sont toutes rencontrées, on est amené à rejeter l'hypothèse nulle, c'est-à-dire à commettre une erreur de type I dans une proportion d'itération égale à $\alpha = 5\%$. Par conséquent, on est amené à *ne pas* rejeter l'hypothèse nulle $95\%$ du temps (ce qui correspond à $(1-\alpha)\%$)\footnote{Nous observons en réalité des proportions qui varient de $94.8\%$ à $95\%$, à cause du hasard d'échantillonnage, mais sur le long terme, lorsque le nombre d'itérations tend vers l'infini, toutes les proportions de non-rejet de l'hypothèse nulle quand l'hypothèse nulle est vraie vont tendre vers $(1-\alpha) \%$.}. Pour les scénarios envisagés dans toutes les autres colonnes de la Table 1, une vraie différence entre les moyennes de population existe, si bien que le rejet de l'hypothèse nulle est la bonne décision. Pourtant, pour plusieurs scénarios, le nombre d'itérations amenant à conclure au non-rejet de l'hypothèse nulle est bien supérieur au nombre d'itérations amenant à conclure au rejet de l'hypothèse nulle, comme on peut le voir à travers les valeurs $\beta$. Par exemple, avec 100 sujets par groupes et considérant $\sigma_1=\sigma_2=1$, on ne détectera pas une différence de moyenne de .1 dans près de 90% des cas. Avec 700 sujets par groupe, cette différence ne sera toujours pas détectée plus d'une fois sur deux ($\approx 54 \%$ des itérations). En présence d'un effet non nul, cela se justifie par un manque de puissance des tests réalisés, ce qui démontre bien qu'un non-rejet de l'hypothèse nulle peut en fait signifier deux choses: soit qu'il n'y a vraiment pas de différence entre les moyennes des populations (ou autrement dit, que les différences observées sont dues au hasard), soit que le test n'est pas suffisamment puissant pour détecter la différence. Or, le manque de puissance des tests est récurrent dans la littérature, comme tendent à le montrer diverses méta-analyses [@button_power_2013; @bakker_rules_2012; @funder_improving_2014].

Pour éviter d'interpréter un test peu puissant comme un soutien en faveur de l'hypothèse nulle, l'approche de la puissance est devenue l'approche par défaut dans les années 80 pour tester l'équivalence [@meyners_equivalence_2012]. A travers cette approche qui est restée très populaire [@quertemont_how_2011], dans un premier temps, on définit ce que l'on considère comme étant la plus petite valeur d'intérêt (en anglais, le "SESOI" pour "Smaller Effect Size of Interest"), c'est-à-dire la taille d'effet minimale requise pour considérer qu'un effet est pertinent. Ensuite, on estime la puissance de notre test à détecter un effet de cette taille\footnote{On parle d'estimation et non de mesure, car la puissance du test dépend de $\sigma$, l'écart-type de la population, qu'on ne connait pas et devra donc estimer sur base de $S$, l'écart-type de l'échantillon (Schuirmann,1987).}, et si cette estimation atteint une valeur jugée satisfaisante (en général, 80%), alors on considère que l'on peut interpréter le non-rejet de l'hypothèse nulle d'absence d'effet comme soutien en faveur de l'équivalence [@quertemont_how_2011; @meyners_equivalence_2012; @schuirmann_comparison_1987]. L'idée sous-jacente est que si l'effet est au moins aussi grand que les bornes de la zone d'équivalence, sur le long terme, on devrait le plus souvent rejeter l'hypothèse nulle. Par conséquent, un non-rejet de l'hypothèse nulle devrait généralement signifier que l'effet n'atteint pas le SESOI et donc, que l'effet observé n'est pas pertinent. Bien que ce raisonnement puisse sembler tentant, de prime abord, il présente d'importantes limites. 

```{r "schuirman1", echo=FALSE}
# paramètres de départ
alpha <- .05
required_power <- .80
n <- 50
df <- 2*n-2
theta <- 20   

# Différences de moyennes testées 
theta1 <- -theta
theta2 <- theta
meandiff <- seq(theta1,theta2,.0001) # différence de moyennes

######  t critique  ######

## Valeur en deça de laquelle on va NRH0 pour l'approche de la puissance
t_crit <- qt(1-alpha/2,df) # on conclura au NRH0 si t < 2.228 
## Valeur au delà de laquelle on va conclure à l'équivalence avec le TOST
t_crit_TOST <- qt(1-alpha,df) # On conclut à l'équivalence  si l'IC à 1-2alpha est entièrement inclus 
                              # entre les limites de la zone d'équivalence (d'où "1-alpha" au lieu de "1-alpha/2")

######  SE maximum autorisé  ######

## Pour l'approche de la puissance: on n'interprétera un NRH0 comme "preuve" d'équivalence QUE SI on atteind

### une puissance minimale donnée (cf. required_power; e.g.: 80%). Compte tenu de cette contrainte, 
### quel sera le SE maximum autorisé?
library(pwr)

res <- pwr.t.test(n=n,sig.level=alpha,
                  power=required_power,
                  type="two.sample",
                  alternative="two.sided")
d_min <- res$d # taille d'effet standardisée qui permet d'atteindre la
               # puissance requise
S_max <- theta/d_min # d_min <- theta/S_max <--> S_max = theta/d_min
SE_max <- S_max*sqrt(2/n) # SE <- S*sqrt(2/N)

## Pour le TOST: jusqu'à quel SE pourra-t-on conclure à l'équivalence? 

### Pour rappel, IC autour de meandiff à (1-alpha)% = meandiff+-t_crit_TOST*SE
### Si meandiff = 0 (le meilleur des cas), t_crit_TOST*SE ne peut pas dépasser theta2 (=20)
### t_crit_TOST*SE < theta2 <--> SE < theta2/t_crit 
SE_max_TOST <- theta2/t_crit_TOST # 11.03472

######  SE critique  ######

## SE minimum qui amènera à conclure à NRH0, avec l'approche de la puissance

### En fonction de meandiff, le SE au delà duquel on conclura
### au NRH0 sera calculé comme suit:
crit_SE <- abs(meandiff)/t_crit # quand n1=n2=n
                                # t = meandiff/SE <--> SE = meandiff/t 
                                # avec SE = S*sqrt(2/n) et S = écart-type poolé
### Attenion: il faut garder à l'esprit la contrainte de la puissance (tout SE supérieure à "SE_max"
### N'amènera PAS à conclure au NRH0)

## SE maximum qui amènera à conclure à l'équivalence, avec le TOST

### Si -theta2 < meandiff < theta2, t_crit_TOST ne peut pas dépasser theta2-abs(meandiff)
##### ex.: si meandiff = 1, et que theta2 = 20, le demi intervalle de confiance ne 
##### peut pas valoir plus de 19 sinon la borne sup sortira de la zone d'équivalence.
##### idem si meandiff = -1.
#### t_crit_TOST*SE < theta2-abs(meandiff) <--> abs(meandiff) < theta2-t_crit_TOST*SE
# SE < [theta2-abs(meandiff)]/t_crit_TOST

crit_SE_TOST <- (theta2-abs(meandiff))/t_crit_TOST

# Représentation graphique:
#setwd("C:/Users/Admin/Documents/Github #projects/thesis/Chapitre 5/Illustration")
#setwd("D:/Documents/Github_projects/thesis/Chapitre #5/Illustration")
#setwd("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Illustration")
#png("Fig1.png",width=1500,height=1500, units = "px", res #= 300)
#ylimsup <- round(SE_max*3/100,1)*100 # pour que la lim supérieure soit un multiple de 10
# valant approximativement le double du SE_max
#plot(0,0,
#     pch=19,cex=.01,ylim=c(0,ylimsup),xlim=c(theta1,theta#2),col="white",
#     bty="n",
#     xlab=expression(paste(bar(X[1])," - ",bar(X[2]))),
#     ylab=expression(paste("s ",sqrt(paste(2,"/n"))))
#)
#axis(2,at=0:ylimsup,labels=rep("",length(0:ylimsup)),col.#ticks="grey")
#axis(2,at=seq(0,ylimsup,5),labels=rep("",length(seq(0,yli#msup,5))),col.ticks="black")

# Partie qui se rapporte à l'approche de la puissance
#SE_max_data<-max(crit_SE[crit_SE<=SE_max]) # dans le vecteur crit_SE, quel est celui qui  
                                           # se rapproche le plus du SE_max?
#segments(0,crit_SE[meandiff==0],meandiff[crit_SE==SE_max_#data],SE_max_data)
#segments(meandiff[crit_SE==SE_max_data][1],SE_max_data,
#         meandiff[crit_SE==SE_max_data][2],SE_max_data)
#x <- c(meandiff[crit_SE==SE_max_data][1],0,meandiff[crit_#SE==SE_max_data][2])
#y <- c(SE_max_data,0,SE_max_data)
#polygon(x, y,col="grey")

# Partie qui se rapporte au TOST
#SE_max_dataTOST<-max(crit_SE_TOST[crit_SE_TOST<=SE_max_TO#ST]) # dans le vecteur crit_SE, quel est celui qui  
#segments(theta1,0,theta2,0)
#segments(theta1,0,0,SE_max_dataTOST)
#segments(0,SE_max_dataTOST,theta2,0)
#x <- c(theta1,0,theta2)
#y <- c(0,SE_max_dataTOST,0)
#polygon(x, y,density=20)

#dev.off()
```

Premièrement, le test n'a pas de bonnes propriétés asymptotiques. Ceci est illustré au sein de la Table 2, dans laquelle nous envisageons les mêmes scénarios que dans la Table 1 et ajoutons une contrainte de puissance: nous décidons qu'on ne peut conclure à l'équivalence que si l'on atteint une puissance de 80% pour détecter une différence de moyenne de .3. On constate qu'avec 100 sujets par groupes, aucune itération n'amènera à conclure à l'équivalence, pas même lorsque la différence entre les moyennes de population vaut 0. Cela s'explique par le fait que l'on n'atteint jamais la puissance minimale de $80\%$.\footnote{Avec 100 sujets par groupe, on estime la puissance du test à $80\%$ lorsque l'estimation $d$ de Cohen vaut .3981. Par conséquent, un test sera susceptible de conclure à l'équivalence si les bornes de la zone d'équivalence, exprimée en mesure standardisée $d$ de Cohen, sont supérieures ou égales à .3981. Lorsqu'on fixe les bornes aux différences de moyennes $\pm .3$, cela n'est possible que si $S$ est inférieur ou égal à .7535. En effet, $d=\frac{\theta}{S} \leftrightarrow .3981 = \frac{.3}{S} \leftrightarrow S = \frac{.3}{.3981}=.7535$.  Or, avec 100 sujets par groupe, aucune estimation $S$ ne sera inférieure ou égale à .7535 lorsque $\sigma$ vaut 1.} Par contre, une fois les échantillons assez grands pour s'assurer une puissance de $80\%$ pour détecter une différence entre les moyennes des populations de .3, lorsque la différence entre les moyennes des populations est non nulle, la proportion d'itérations qui amènent à conclure à l'équivalence diminue à mesure que la taille des échantillons augmente. Par exemple, lorsque la différence de moyennes vaut .1 au niveau des populations, on conclura à l'équivalence dans $81\%$ des itérations avec 200 sujets par groupe, contre seulement $54\%$ des itérations avec 700 sujets par groupe \footnote{En comparant les Tables 1 et 2, on constate qu'avec 200 sujets par groupes, les proportions d'itérations de chaque scénario qui amènent à conclure à l'équivalence, dans la Table 2, sont inférieures aux proportions d'itérations de chaque scénarios qui amènent à ne pas rejeter l'hypothèse nulle, dans la Table 1. Plus les échantillons sont grands, moins on observera d'écart entre les 2 tables car la proportion d'itérations qui ne pourront amener à conclure à l'équivalence en raison d'un manque de puissance diminuera. En effet, plus la taille des échantillons sera grande, plus la valeur maximale de $S$ permettant d'assurer la puissance des $80\%$ sera élevée. Par exemple, avec 200 sujets par groupes, la valeur maximale autorisée pour $S$ sera de $\frac{.3}{.2808}=1.07$. Avec 300 sujets par groupes, la valeur maximale autorisée pour $S$ sera de $\frac{.3}{.2291}=1.31$. Par conséquent, plus les tailles d'échantillons seront grandes, moins il sera probable que $S$ dépasse le seuil autorisé.}. 

```{r "simu2", fig.align='left', fig.cap=NULL,echo=FALSE,out.width="95%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Illustration/Table2.png")
```

Deuxièmement, pour une taille d'échantillon donnée, plus l'erreur (la variabilité des scores au sein de chaque groupe) sera grande [@meyners_equivalence_2012; @schuirmann_comparison_1987], plus la probabilité de conclure à l'équivalence augmentera. Ce dernier point est illustré au sein de la Figure \ref{fig:schuirman2}, dans le contexte de la comparaison de deux moyennes. Sur l'axe des abscisses, on représente différentes estimations de la différence de moyenne ($\bar{X_1}-\bar{X_2}$) et sur l'axe des ordonnées, la précision des estimations $\bar{X_1}-\bar{X_2}$ ($S\sqrt{\frac{2}{n}}$ correspond à l'estimation de l'erreur standard de $\bar{X_1}-\bar{X_2}$, avec $S$ étant l'écart-type poolé et $n$ la taille de chaque échantillon, lorsque les échantillons ont tous les deux la même taille et sont extraits de population ayant la même variance)\footnote{Par facilité, à l'instar de Schuirman (1987), on envisage le cas où les échantillons sont de même taille et que l'on suppose que la condition d'homogénéité des variances est respectée. Notons cependant que d'après Schuirman, ce raisonnement peut être généralisé aux scénarios où les deux échantillons n'ont pas la même taille et sont extraits de population n'ayant pas la même variance.}. Le triangle grisé représente l'ensemble des combinaisons estimation/précision qui vont amener à conclure à l'équivalence, avec l'approche de la puissance, lorsqu'on travaille avec des échantillons de taille 50, en acceptant un risque $\alpha$ de 5% et en exigeant une puissance minimale de 80% pour détecter une différence de 20 unités ($|\theta_j=20|,\;j=1,2$). Dans cet exemple, pour toutes les valeurs de $S\sqrt{\frac{2}{n}}$ supérieures à `r round(SE_max,2)` aucune estimation de différence de moyennes ne permettra de conclure à l'équivalence (pas même 0) puisque la puissance du test à détecter une différence de 20 unités est inférieure à 80%.  Pour toutes les valeurs de $S\sqrt{\frac{2}{n}}$ inférieures à `r round(SE_max,2)`, on constate que plus notre estimation de $\bar{X_1}-\bar{X_2}$ est précise (lorsqu'on se déplace du haut vers le bas, sur l'axe des ordonnées), plus l'estimation doit être proche de 0 pour pouvoir conclure à l'équivalence. Comme on peut le voir à travers le triangle hachuré sur la Figure 10, cette propriété peu désirable n'est pas partagée par le TOST, un test d'équivalence que nous allons décrire ci-dessous [@schuirmann_comparison_1987]. 

```{r "schuirman2", fig.align='center', fig.cap="Région d'équivalence pour l'approche de la puissance (zone grisée) et pour le TOST (zone hachurée), pour l'exemple où $|\\theta$|=20, n = 50 et $\\alpha=.05$",echo=FALSE,out.width="40%",dpi=600, fig.width=4, fig.height=3}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Illustration/Fig1.png")
```

```{r "tauxbeta", echo=FALSE}

#simu_beta <- function(n){

#  mudiff <- seq(.1,1.9,.2)
#  NRH0 <- NULL
  
#  for(j in seq_len(length(mudiff))){

#    m2 <- mudiff[j]  
#    t <- NULL
#    pval <- NULL
    
#    for (i in seq_len(5000)){
#      G1 <- rnorm(n,0,1)
#      G2 <- rnorm(n,m2,1)
#      res <- t.test(G1,G2,var.equal=TRUE)
#      t <- c(t, res$statistic)  
#      pval <- c(pval,res $p.value)  
#    }

#    NRH0 <- c(NRH0,sum(pval>.05)/length(pval))
    
#}

# return(round(NRH0,3))
 
  
#}

#S1 <- simu_beta(10)
#S2 <- simu_beta(15)
#S3 <- simu_beta(20)
#S4 <- simu_beta(25)
#S5 <- simu_beta(50)
#S6 <- simu_beta(100)
#S7 <- simu_beta(200)

#df <- rbind(S1,S2,S3,S4,S5,S6,S7)
#colnames(df)=seq(.1,1.9,.2)
#rownames(df)=c(10,15,20,25,50,100,200)

#write.table(df,"df.txt",sep=";",dec=",")
```
\newpage
## Les tests d'équivalence

Avec les tests d'équivalence, il n'est pas possible de démontrer qu'un effet vaille exactement zéro [@meyners_equivalence_2012]. Il est par contre possible de montrer que l'effet observé est suffisamment petit pour être jugé non pertinent. Or, cela peut s'avérer précieux dans de nombreuses situations, par exemple pour justifier la décision de regrouper plusieurs groupes de sujets ensemble [@rogers_using_1993], pour contrôler qu'il n'y ait pas de différence trop importante entre les groupes sur base de critères autres que le (ou les) facteur(s) d'intérêts en cas de quasi-expérience  [@seaman_equivalence_1998] ou encore pour falsifier une théorie qui prônerait en faveur d'un effet dépassant une certaine taille [@lakens_equivalence_2017;@anderson_theres_2016].

Le point de départ des tests d'équivalence est de définir $\theta_1$ et $\theta_2$, les bornes inférieures et supérieures de la zone d'équivalence, cette dernière contenant l'ensemble des valeurs jugées trop petites pour être susceptibles de nous intéresser. Ces bornes peuvent être exprimées soit dans l'unité des données brutes, soit en terme standardisé, mais doivent être définies avant la récolte des données [@anderson_theres_2016; @lakens_equivalence_2018]. Il existe ensuite plusieurs approches pour démontrer que l'effet observé se situe dans la zone d'équivalence [voir @meyners_equivalence_2012, par exemple]. Parmis celles-ci, une approche très simple est celle du "Two one-sided tests" [@schuirmann_comparison_1987; @lakens_equivalence_2017], plus communément appelé le TOST \footnote{Il existe des alternatives au TOST qui sont très légèrement plus puissantes, mais le gain marginal en termes de puissance est contrebalancé par un niveau de complexité beaucoup plus élevé (Meyners, 2012).}. Le principe est de définir deux hypothèses nulles. La première est que l'effet observé est inférieur à la borne inférieure de la zone d'équivalence: $$\it H0_1: \theta < \theta_1, \; avec \; \theta_1 \neq 0$$ La deuxième est que l'effet observé est supérieur à la borne supérieure de la zone d'équivalence: $$\it H0_2: \theta > \theta_2, \; avec \; \theta_2 \neq 0$$ Lorsque les deux hypothèses nulles peuvent être simultanément rejetées, on peut conclure à l'équivalence [@seaman_equivalence_1998]. Cela équivaut, statistiquement parlant, à montrer que l'intervalle de confiance à $(1-2\times\alpha)\%$ est entièrement inclus dans la zone d'équivalence [@seaman_equivalence_1998; 
@lakens_equivalence_2017]. Notons qu'il n'est pas nécessaire de reporter les résultats des deux tests unilatéraux lorsqu'on réalise le TOST, il suffit de reporter les résultats du test associé à la plus petite valeur de statistique (et par conséquent, à la plus grande $p$-valeur). En effet, si ce test amène à conclure au rejet de l'hypothèse nulle, le second test amènera automatiquement à la même conclusion [@rogers_using_1993; @lakens_equivalence_2018]. Cette remarque reste vraie dans le cas particulier où les deux tests sont associés à la même valeur de statistique puisque dans ce cas, les deux tests mèneront à une conclusion identique [@rogers_using_1993]. Notons également qu'il n'est pas nécessaire de procéder à une correction du risque alpha due à la réalisation simultanée de deux tests. En effet, une erreur de type I (rejeter à tort l'hypothèse nulle) ne peut être commise que si l'hypothèse nulle est vraie. Or, les deux hypothèses nulles testées sont mutuellement exclusives: il n'est pas possible que $\theta$ soit simultanément inférieur à $\theta_1$ (ce qui correspond à $\it H0_1$) et supérieur à $\theta_2$ (ce qui correspond à $\it H0_2$).  

```{r, include=FALSE}
library(TOSTER)
m <- 144
mu <- 144.5
sd <- 500
n <- 1000000
low_eqbound = -2 
high_eqbound = 2 
alpha = 0.05
tost_res1 <- TOSTone.raw(m = m, 
                         mu = mu,
                         sd = sd, 
                         n = n, 
                         low_eqbound = low_eqbound, 
                         high_eqbound = high_eqbound, 
                         alpha = alpha
)

m <- 143.5
mu <- 144.5
sd <- 400
n <- 1000000
low_eqbound = -2 
high_eqbound = 2 
alpha = 0.05
tost_res2 <- TOSTone.raw(m = m, 
                         mu = mu,
                         sd = sd, 
                         n = n, 
                         low_eqbound = low_eqbound, 
                         high_eqbound = high_eqbound, 
                         alpha = alpha
)
m <- 148
mu <- 144.5
sd <- 500
n <- 1000000
low_eqbound = -2 
high_eqbound = 2 
alpha = 0.05
tost_res3 <- TOSTone.raw(m = m, 
                         mu = mu,
                         sd = sd, 
                         n = n, 
                         low_eqbound = low_eqbound, 
                         high_eqbound = high_eqbound, 
                         alpha = alpha
)

m <- 143.5
mu <- 144.5
sd <- 700
n <- 1000000
low_eqbound = -2 
high_eqbound = 2 
alpha = 0.05
tost_res4 <- TOSTone.raw(m = m, 
                         mu = mu,
                         sd = sd, 
                         n = n, 
                         low_eqbound = low_eqbound, 
                         high_eqbound = high_eqbound, 
                         alpha = alpha
)
```

Jusqu'il y a peu, le TOST n'était pas disponible dans la plupart des logiciels, à l'exception de Minitab, ce qui constituait un frein important à son usage. Pour cette raison, @lakens_20_2016 a créé le package R "TOSTER" et plus récemment encore, ce même package a été implémenté dans Jamovi \footnote{Jamovi est un logiciel clic-bouton entièrement gratuit qui gagne en popularité et qui présente, parmi ses nombreux avantages, le fait d'être particulièrement convivial. Dans la mesure où la plupart des chercheurs sont plus enclins à utiliser des procédures si elles sont implémentées dans ce type de logiciel (Fraas $\&$ Newman, 2000), cela constitue une excellente nouvelle pour le devenir du TOST dans la recherche en psychologie.}. Tant dans R que dans Jamovi, le package compare simultanément l'effet observé à l'absence d'effet (cela correspond au test traditionnel) ainsi qu'aux deux bornes de la zone d'équivalence (cela correspond au TOST). Il en découle 4 conclusions distinctes possibles [@lakens_equivalence_2017], qui sont illustrées dans la figure \ref{fig:equiv1} dans le contexte de la comparaison de deux moyennes indépendantes :   

```{r "equiv1", fig.align = "center", fig.cap= "Différence de moyennes ($\\bar{X_1}-\\bar{X_2}$) et IC à $1-2\\alpha\\%$ autour de la différence de moyennes ($\\bar{X_1}-\\bar{X_2}$) pour 4 scénarios distincts.", echo=FALSE, dpi=600,out.width="80%"}
plot(NA, 
     ylim = c(0, 1), 
     xlim = c(-3, 5),
     yaxt = "n",
     xaxt= "n",
     ylab = "",
     xlab = "Mean Difference")
axis(2, at = c(0.125,0.375,0.625,0.875), labels = c("D","C", "B", "A"), las = 1)
axis(1, at = c(-2,0,2), labels = c(expression(theta[1]),0,expression(theta[2])),las = 1)
abline(v = tost_res1$high_eqbound, 
       lty = 2)
abline(v = tost_res1$low_eqbound, 
       lty = 2)
abline(v = 0, 
       lty = 2, 
       col = "grey")

points(x = tost_res4$diff, 
       y = 0.125, 
       pch = 15, 
       cex = 1.5)
segments(tost_res4$LL_CI_TOST, 
         0.125, 
         tost_res4$UL_CI_TOST, 
         0.125, 
         lwd = 2)

points(x = tost_res3$diff, 
       y = 0.375, 
       pch = 15, 
       cex = 1.5)
segments(tost_res3$LL_CI_TOST, 
         0.375, 
         tost_res3$UL_CI_TOST, 
         0.375, 
         lwd = 2)

points(x = tost_res2$diff, 
       y = 0.625, 
       pch = 15, 
       cex = 1.5)
segments(tost_res2$LL_CI_TOST, 
         0.625, 
         tost_res2$UL_CI_TOST, 
         0.625, 
         lwd = 2)

points(x = tost_res1$diff, 
       y = 0.875, 
       pch = 15, 
       cex = 1.5)
segments(tost_res1$LL_CI_TOST, 
         0.875, 
         tost_res1$UL_CI_TOST, 
         0.875, 
         lwd = 2)
```

(1) La différence de moyenne observée diffère significativement des deux bornes d'équivalence, mais pas de 0 (scénario A, Figure \ref{fig:equiv1}): dans ce cas, on conclura à l'absence d'un effet au moins aussi grand que les bornes d'équivalence.  

(2) La différence de moyenne observée diffère significativement des deux bornes d'équivalence ainsi que de 0 (scénario B, Figure \ref{fig:equiv1}):  on conclura alors qu'il existe un effet non nul, mais qui ne dépasse pas une certaine taille fixée par les bornes. C'est ce qui arrive typiquement lorsqu'on travaille avec de très grands échantillons, si bien que le test traditionnel est très puissant, même pour détecter des effets très petits [@rogers_using_1993].  

(3) La différence de moyenne observée diffère significativement de 0, mais ne diffère pas significativement d'au moins une des deux bornes d'équivalence (scénario C, Figure \ref{fig:equiv1}): on conclura alors à la présence d'un effet non nul [@rogers_using_1993].  

(4) La différence de moyenne observée ne diffère significativement ni d'au moins une des deux bornes d'équivalence, ni de 0 (scénario D, Figure \ref{fig:equiv1}): c'est ce qui arrive lorsque les données sont si imprécises qu'on ne peut tirer aucune conclusion. Les données semblent compatibles tant avec un effet nul qu'avec un effet supérieur au SESOI.   

## Définir les bornes de la zone d'équivalence
 
L'aspect le plus compliqué dans la réalisation du TOST est la définition des bornes d'équivalence. Dans certains cas, il est possible de définir un critère objectif qui permettra de déterminer à partir de quand un effet est jugé pertinent [@lakens_equivalence_2018]. Dans ce cas, établir l'équivalence revient à rejeter la présence d'un effet ayant un quelconque intérêt pratique [@rogers_using_1993]. Par exemple, @burriss_changes_2015 avaient émis l'hypothèse qu'une augmentation de la rougeur de la peau chez femmes, en période d'ovulation, les rendraient plus attractives pour les hommes. Or, une telle hypothèse n'est crédible que si le changement facial est visible à l'oeil nu. Dans ce contexte, le SESOI serait la plus petite variation dans la rougeur de la peau qu'il est possible de détecter à l'oeil nu [@lakens_equivalence_2018]. Il est également parfois possible pour des experts de déterminer expérimentalement ce qui constitue un changement important, pour certaines échelles fréquemment utilisées en vue de mesurer des construits psychologiques. @button_minimal_2015, par exemple, ont interrogé un grand nombre de patients dépressifs quant à leur ressenti subjectif en termes d'amélioration de leur dépression au cours d'un certain laps de temps, et ont comparé leurs réponses à la différence de scores obtenus à l'aide du BDI \footnote{Le BDI (Beck Depression Inventory) est une échelle auto-rapportée évaluant les symptômes cognitifs courants de la dépression.  Cette échelle est constituée de 21 items évalués à l'aide des échelles de Likert allant de 0 à 3, ce qui donne un score total compris entre 0 et 63 qui sera d'autant plus élevé que la dépression sera sévère (Button et al., 2015).} dans ce même laps de temps [@lakens_equivalence_2018]. Cela leur a permis de déterminer ce qu'il considérait comme étant une différence pratiquement significative sur l'échelle du BDI. Malheureusement, il n'est pas toujours possible d'établir un critère objectif en vue de définir les bornes d'équivalence. Dans ce cas, il existe diverses stratégies, plus subjectives, en vue d'établir ces bornes. En les utilisant, il faut cependant avoir conscience du fait que la question à laquelle nous répondons varie en fonction de la stratégie utilisée.

Il est possible de déterminer des bornes en s'inspirant de balises existantes, en vue d'exclure la présence d'un effet jugé petit, moyen ou grand par ces balises [@lakens_equivalence_2018]. Notons que si cette stratégie est tentante de par sa simplicité, elle doit être utilisée avec prudence. D'abord, un effet ne devrait être qualifié de petit, moyen ou grand qu'en comparaison à d'autres effets connus, et non sur base d'impressions qualitatives [@gignac_effect_2016]. Dit autrement, il est important d'avoir un cadre de référence pour juger de la taille d'un effet. Or, les balises de Cohen (en l'occurrence, les balises les plus célèbres et les plus largement utilisées) sont dépourvues de ce cadre de référence, puisqu'elles ont été établies à une époque où très peu de chercheurs se préoccupaient de la taille des effets étudiés [@funder_evaluating_2019]. Depuis Cohen, certains chercheurs ont déployé de gros efforts en vue d'établir de nouvelles normes sur base d'analyses systématiques quantitatives de la littérature. @gignac_effect_2016, par exemple, ont établi de nouvelles balises pour interpréter le $r$ de Pearson, en définissant les quartiles d'une distribution de 708 mesures dérivées de méta-analyses issues de la psychologie sociale et de la personnalité. C'est de la sorte qu'ils ont proposé d'interpréter respectivement des mesures de 0.10, 0.20 et 0.30, dans ces domaines de la psychologie, comme représentant des effets relativement petits, typiques et relativement larges. Ces normes ont également été approuvées par @funder_evaluating_2019 \footnote{Funder $\&$ Ozer (2019) ont relevé plusieurs enquêtes ayant calculé un $r$ de Pearson moyen de .21 sur base d'effets publiés dans la litérature en psychologie sociale et en psychologie de la personnalité. Par ailleurs, ils rappellent qu'en raison du biais de publication, un chercheur obtenant un $r$ de Pearson de .21 dans une nouvelle étude peut être assuré d'avoir détecté un effet plus grand que généralement trouvé}. Ensuite, les balises ne prennent pas en compte le contexte de l'étude si bien que statuer sur la taille d'un effet ne fournit pas nécessairement d'information sur sa valeur. Imaginons un antidépresseur B qui permette de réduire très légèrement les symptômes de dépression par rapport à un antidépresseur A déjà présent sur le marché et qui, en outre coûte moins cher. Même si statistiquement parlant, on observe une très faible taille d'effet, une analyse coût/bénéfice amènerait très vraisemblablement à conclure à la pertinence de cet effet. Pour cela, les balises devraient toujours être utilisées en dernier recours, lorsqu'on ne dispose d'aucune autre information [@gignac_effect_2016]. 

```{r "lakens",echo=FALSE}
n <- 30
alpha <- .05
crit_stat <- qt(1-alpha/2,2*(n-1))
crit_dcohen<-crit_stat*sqrt(2/n)
library(pwr)
res <- pwr.t.test(n = n, sig.level = alpha,power = .33, type = "two.sample", alternative = "two.sided")
```

La taille des échantillons d'une étude est une information sur laquelle nous pouvons également nous baser en vue de fixer des bornes d'équivalence. Dans le contexte d'une réplication d'étude, cette information peut servir à déterminer si la puissance de l'outil utilisé par le chercheur d'origine était suffisante en vue de tester l'effet ciblé. Une stratégie proposée par @lakens_equivalence_2018 consiste à définir comme borne d'équivalence le plus petit effet que l'étude d'origine aurait pu détecter comme étant significative.\footnote{Dans le contexte d'une réplication, il nous semble souvent plus logique de réaliser le test d'équivalence en unilatéral, puisque l'étude d'origine précisera généralement le sens de l'effet observé. C'est pourquoi nous parlons ici de bornd d'équivalence au singulier.} L'idée sous-jacente est la suivante: bien qu'idéalement, les chercheurs devraient toujours spécifier ce qu'ils considèrent comme étant le plus petit effet d'intérêt, cette pratique n'est pas encore commune. Heureusement, même lorsqu'un auteur ne statue pas explicitement sur ce qu'il considère comme étant un effet pertinent, la taille des échantillons qu'il utilise aura un impact sur la taille des effets que l'on sera capable de mettre en évidence (en effet, plus les échantillons sont petits, plus l'effet observé doit être grand pour pouvoir être détecté comme étant significatif). Imaginons par exemple qu'un auteur compare les moyennes de deux groupes de `r n` sujets à l'aide d'un test $t$ de Student (par facilité, considérons les conditions d'application de ce test comme étant toutes respectées). Avec ces tailles d'échantillon, on conclura au rejet de l'hypothèse nulle si la statistique $t$ de Student vaut au minimum `r round(crit_stat,3)`. Compte tenu de la relation directe entre la statistique $t$ de Student et le $d$ de Cohen (voir chapitre 4), on en déduit qu'on conclura au rejet de l'hypothèse nulle si le $d$ de Cohen est supérieur ou égal à `r round(crit_dcohen,3)`. Si l'on fixe `r round(crit_dcohen,3)` comme borne d'équivalence, et que l'on démontre lors d'une réplication que l'effet observé est significativement inférieur à cette borne, on suspectera que l'étude d'origine n'aurait pu détecter proprement l'effet qu'elle prétendait détecter. Une autre stratégie, proposée par @simonsohn_p-curve_2014 consiste à déterminer la taille d'effet que l’étude d’origine aurait pu détecter avec une puissance de $33\%$, et à utiliser cette information pour définir la borne d’équivalence. Par exemple, avec une hypothèse bilatérale, un test $t$ de Student aura une puissance de $33\%$ pour détecter un effet de taille `r round(res$d,3)` avec `r n` sujets par groupe (à condition que les conditions d'application du test soient toutes respectées). Si lors de la réplication, on obtient un effet significativement inférieur à `r round(res$d,3)`, on en déduira que sur le long terme, la probabilité que l'outil d'origine puisse proprement détecter un effet de cette taille était inférieure à $33\%$, ce qui remet sa pertinence en cause \footnote{Bien entendu, la valeur $33\%$ a une dimension arbitraire, comme chaque fois que l'on fixe une valeur par défaut.}. 
Hors du contexte des réplications d'études, on peut également se baser sur la taille des échantillons que l'on est apte à collecter soi-même, en vue de déterminer si l'on dispose d'assez de ressources pour détecter un effet ciblé [@lakens_equivalence_2018]. Par exemple, si nous sommes dans l'incapacité de collecter des échantillons de plus de 2000 personnes, il y a certains effets que nous ne pourrons jamais détecter avec une puissance suffisante. Il est possible de déterminer la taille d'effet que nous sommes certains de pouvoir détecter avec suffisamment de puissance (ou autrement dit, dans un pourcentage raisonnable de cas, sur le long terme). Si l'on utilise cette information pour fixer la (ou les) borne(s) d'équivalence et que l'on conclut effectivement à l'équivalence, la conclusion sera que pour détecter proprement l'effet que nous ciblons, il est indispensable de collecter de plus grands échantillons.

## Comparaison du TOST et du SGPV

```{r "chp5p1", fig.align='center', fig.cap=NULL,echo=FALSE,out.height="94%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-1.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-1.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-1.png")
```

```{r "chp5p2", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-2.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-2.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-2.png")
```

```{r "chp5p3", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-3.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-3.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-3.png")
```

```{r "chp5p4", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-4.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-4.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-4.png")
```

```{r "chp5p5", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-5.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-5.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-5.png")
```

```{r "chp5p6", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-6.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-6.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-6.png")
```

```{r "chp5p7", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-7.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-7.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-7.png")
```

```{r "chp5p8", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-8.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-8.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-8.png")
```

```{r "chp5p9", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-9.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-9.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-9.png")
```

```{r "chp5p10", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-10.png")
#knitr::include_graphics("D:/Documents/Github_projec#ts/thesis/Chapitre 5/Chapitre 5-10.png")
#knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-10.png")
```

```{r "chp5p11", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="100%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-11.jpg")

```
