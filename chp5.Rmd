---
title: " "
output: 
  papaja::apa6_pdf:
    includes:
      extra_dependencies: ["float"]

header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
---

# Chapitre 5: les tests d'équivalence

Lorsqu'on applique un test d'hypothèse, l'hypothèse nulle la plus couramment définie est celle d'absence d'effet ou de différence entre les groupes [@nickerson_null_2000]. Il arrive également parfois que les chercheurs définissent un intervalle de valeur comme hypothèse nulle, mais le plus souvent, cet intervalle est borné par la valeur 0 [@nickerson_null_2000], on parle alors d'hypothèse unilatérale. Avec cette stratégie, le rejet de l'hypothèse nulle constitue un soutien en faveur de la présence d'un effet non nul. Par contre, le non rejet de l'hypothèse nulle ne peut être interprété comme un soutien en faveur de l'absence d'effet, or il arrive souvent que des chercheurs l'interprètent de la sorte. @finch_reporting_2001, par exemple, ont reporté que parmis 150 articles publiés entre 1940 et 1999 dans le *JAP* (*Journal of Applied Psychology*), 38% interprétaient un résultat non significatif comme une acceptation de l'hypothès nulle. Plus récemment, @lakens_equivalence_2017 a noté que l'expression "pas d'effet" a été utilisée dans 108 articles publié dans *Social Psychological and Personality Science* avant août 2016 et que dans presque tous les cas, c'était sur base du non rejet de l'hypothèse nulle que cette conclusion était tirée. 

A travers ce chapitre, notre premier objectif sera d'expliquer pourquoi interpréter le non rejet de l'hypothèse nulle comme un soutien en faveur d'une absence d'effet n'est pas une bonne stratégie. Nous introduirons ensuite les tests d'équivalence qui permettent d'obtenir un soutien en faveur d'un effet jugé non pertinent. Pour finir, nous présenterons un article dans lequel nous comparons les tests d'équivalence à la *SGPV* (Second Generation P-value), une stratégie récemment développée Blume [@blume_second-generation_2018].

## Limites de l'approche traditionnelle

Lorsqu'on teste une hypothèse nulle, il y a deux conclusions possibles: soit ont la rejette (et ont conclut alors en faveur de l'hypothèse alternative) soit on ne la rejette pas. Ne pas rejeter l'hypothèse nulle ne permet pas de conclure en faveur de cette dernière. Au mieux, cela nous montre que les données ne sont pas incompatibles avec l'hypothèse nulle, mais cela ne veut en aucun cas dire qu'elles ne sont compatibles avec aucune autres hypothèse.  Afin de l'illustrer, la Table 1 résume les résultats de simulations Monte Carlo pour un ensemble de 70 scénarios qui varient en fonction de la taille des échantillons ($n_j$) et de la différence entre les moyennes des deux populations dont sont extraits les échantillons ($\mu_1-\mu_2$). Pour chaque scénario, à 5000 reprises, nous avons généré une paire d'échantillons, réalisé un test $t$ de Student pour échantillons indépendants et extrait la $p$-valeur du test. Ensuite, nous avons calculé la proportion de $p$-valeurs supérieures à .05 [le risque alpha typiquement accepté, @meyners_equivalence_2012]. Cette proportion correspond au taux d'erreur de type II (communément appelé $\beta$). 

```{r "simu", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Illustration/Table 1.png")
```

Dans la mesure où une vraie différence entre les moyennes de population existe pour l'ensemble des scénarios envisagés, la conclusion adéquate serait le rejet de l'hypothèse nulle. Pourtant, $\beta$ est très élevé dans certains cas. Par exemple, avec 10 sujets par groupes, on ne détectera pas un effet standardisé de taille .1 dans près de 95% des cas. Avec 200 sujets par groupe, on ne le détectera toujours pas dans 80% des cas. Cela se justifie par un manque de puissance des tests réalisés, ce qui démontre bien qu'un non rejet de l'hypothèse nulle n'est pas forcément synonyme d'acceptation de l'hypothèse nulle. Il peut en fait signifier deux choses: soit qu'il n'y a pas de différence entre les moyennes des populations (ou autrement dit, que les différences observées sont dûes au hasard), soit que nous manquions de puissance pour détecter la différence. Or, ce manque de puissance est récurrent dans la littérature, comme tendent à le montrer diverses méta-analyses [voir par exemple @button_power_2013; @bakker_rules_2012; @funder_improving_2014]. Typiquement, la taille des échantillons récoltés est telle que la puissance des tests pour détecter un effet de taille moyenne varierait approximativement entre .45 et .65 (et serait nécessairement encore plus faible pour des effets plus petits) [@funder_improving_2014]. C'est d'autant plus problématique que l'argument de la puissance est rarement pris en compte pour justifier la taille des échantillons dans les analyses [@bakker_rules_2012], et ce malgré les recommandations fréquentes de mettre en oeuvre  des analyses de puissance a priori pour déterminer la taille de l’échantillon nécessaire à l’atteinte d’une puissance suffisante [voir par exemple @bakker_rules_2012; @braver_continuously_2014; @button_power_2013; @perugini_safeguard_2014; @simmons_false-positive_2011; @stroebe_are_2016]. Un autre problème lié est le fait que la probabilité de détecter l'absence d'effet, via l'approche traditionnel, va diminuer non seulement quand la taille des échantillons augmente, mais également quand l'erreur (la variabilité des scores au sein de chaque groupe) va augmenter. On se retrouve alors dans la situation paradoxale où la probabilité de démontrer l'absence d'effet sera la plus élevée lorsque des petits échantillons sont associés à une grande erreur de mesure [@meyners_equivalence_2012]. 

```{r "tauxbeta", echo=FALSE}

#simu_beta <- function(n){

#  mudiff <- seq(.1,1.9,.2)
#  NRH0 <- NULL
  
#  for(j in seq_len(length(mudiff))){

#    m2 <- mudiff[j]  
#    t <- NULL
#    pval <- NULL
    
#    for (i in seq_len(5000)){
#      G1 <- rnorm(n,0,1)
#      G2 <- rnorm(n,m2,1)
#      res <- t.test(G1,G2,var.equal=TRUE)
#      t <- c(t, res$statistic)  
#      pval <- c(pval,res $p.value)  
#    }

#    NRH0 <- c(NRH0,sum(pval>.05)/length(pval))
    
#}

# return(round(NRH0,3))
 
  
#}

#S1 <- simu_beta(10)
#S2 <- simu_beta(15)
#S3 <- simu_beta(20)
#S4 <- simu_beta(25)
#S5 <- simu_beta(50)
#S6 <- simu_beta(100)
#S7 <- simu_beta(200)

#df <- rbind(S1,S2,S3,S4,S5,S6,S7)
#colnames(df)=seq(.1,1.9,.2)
#rownames(df)=c(10,15,20,25,50,100,200)

#write.table(df,"df.txt",sep=";",dec=",")
```

## Principe du test d'équivalence

## Morceau qui servira peut-être (ou pas)

D'après @lakens_practical_2021, un test d'hypothèse (selon l'approche de Nayman-Pearson) vaut la peine à 2 conditions:  
1) que l'hypothèse nulle soit assez plausible pour que son rejet puisse surprendre au moins certains;  
2) le chercheur veut appliquer une procédure méthodol qui l'autorise à prendre des décisions quant à la manière d'agir, tout en contrôlant le taux d'erreur. Agir peut vouloir dire: adopter un traitement, une politique, une intervention, ou abandonner un domaine de rechercher, modifier une manipulation, ou de faire un certain type de déclaration ou revendication.  
*One of the most widely suggested improvements of the use of p values is to replace null-ypothesis tests (where the goal is to reject ann effect of exactly 0) with tests of range predictions (where the goal is to reject effects that fall outside of the range of effects that is predicted or considered practically important) [@lakens_practical_2021]. 

Un autre argument en défaveur de la p-valeur est la tendance des chercheurs à interpréter un effet NS comme l'acceptation de l'hypothèse nulle (Schmidt, 1996, cité par Harris, 1997). Une fois encore, l'usage des tailles d'effet peut aider à cette fin, non pas en "remplaçant" les tests d'hypothèses, mais en les complétant. --> Tests d'équivalence. 

## Remarques diverses

D'après Meyners, il existe pleins d'approches mais la différence entre elles est souvent négligeable [@meyners_equivalence_2012].

## Quand veut-on montrer une absence d'effet?

Il arrive que des chercheurs souhaitent obtenir un soutien en faveur d'une absence d'effet. A titre d'illustration, @goertzen_detecting_2010 évoquent la situation où un chercheur désire écarter de potentielles covariables de leur analyse, en démontrant préalablement que ces variables ne sont pas corrélées à la variable dépendante. ESSAYER DE TROUVER ENCORE UN EXEMPLE. Or, on constate que très souvent, 











```{r "chp5p1", fig.align='center', fig.cap=NULL,echo=FALSE,out.width="92%"}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-1.png")
```

```{r "chp5p2", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-2.png")
```

```{r "chp5p3", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-3.png")
```

```{r "chp5p4", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-4.png")
```

```{r "chp5p5", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-5.png")
```

```{r "chp5p6", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-6.png")
```

```{r "chp5p7", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-7.png")
```

```{r "chp5p8", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-8.png")
```

```{r "chp5p9", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-9.png")
```

```{r "chp5p10", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-10.png")
```

```{r "chp5p11", fig.align='center', fig.cap=NULL,echo=FALSE}
knitr::include_graphics("C:/Users/Admin/OneDrive/Documents/Github projects/thesis/Chapitre 5/Chapitre 5-10.png")
```


