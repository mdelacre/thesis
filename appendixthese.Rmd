---
title: ''
output: pdf_document

header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
  - \usepackage{caption}
  - \usepackage[labelformat=empty]{caption}
---

# Annexes

## Annexes du Chapitre 3 

### Appendix 1 : The Mathematical Development of the $\bm{F}$-test, $\bm{W}$-test, and $\bm{F^*}$-test: Numerical Example 

Descriptive statistics are presented in Table A1. The raw data are available here : https://github.com/mdelacre/W-ANOVA/tree/master/Functions (see "practical example.R"). The dependent variable is a score that can vary from 0 to 40. The independent variable is a three-level factor A (levels = $A_1$, $A_2$ and $A_3$). 

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/mdelacre/Documents/Github project/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/TableA1.PNG")
```

```{r "", echo=FALSE, out.width = '100%'}
n=c(41,21,31)
mu=c(24,23,27)
var=c(81.75,10.075,38.40)
k=length(n)
N=sum(n)
global_mu=sum(n*mu)/sum(n)
w_j=n/var
w=sum(n/var)
muprime=sum(w_j*mu)/w
```

The overall mean (i.e. the mean of the global dataset) is a weighted mean of the sample means :

$$\bar{X_{..}}=\frac{(`r n[1]`\times`r mu[1]`)+(`r n[2]`\times`r mu[2]`)+(`r n[3]`\times`r mu[3]`)}{`r n[1]`+`r n[2]`+`r n[3]`}=\frac{`r sum(n*mu)`}{`r sum(n)`} \approx `r round(sum(n*mu)/sum(n),2)`$$
The *F*-test statistic and degrees of freedom are computed by applying equation (1) and related degrees of freedom :

$$
F=\frac{\frac{1}{`r k`-1}[`r n[1]`\times(`r mu[1]`-\frac{`r sum(n*mu)`}{`r sum(n)`})^2+`r n[2]`\times(`r mu[2]`-\frac{`r sum(n*mu)`}{`r sum(n)`})^2+`r n[3]`\times(`r mu[3]`-\frac{`r sum(n*mu)`}{`r sum(n)`})^2]}
{\frac{1}{`r N`-`r k`}[(`r n[1]`-1)\times`r var[1]`+(`r n[2]`-1)\times`r var[2]`+(`r n[3]`-1)\times`r var[3]`]} \approx `r round(((1/(k-1))*(sum(n*(mu-global_mu)^2)))/((1/(N-k))*(sum((n-1)*var))),3)
`
$$

$$
df_n=`r k`-1=`r k-1`
$$

$$
df_d=`r N`-`r k`=`r N-k`
$$
The $F^*$-test statistic and his degrees of freedom are computed by applying equation (2) and related degrees of freedom : 

$$
F^*=\frac{`r n[1]`\times(`r mu[1]`-\frac{`r sum(n*mu)`}{`r sum(n)`})^2+`r n[2]`\times(`r mu[2]`-\frac{`r sum(n*mu)`}{`r sum(n)`})^2+`r n[3]`\times(`r mu[3]`-\frac{`r sum(n*mu)`}{`r sum(n)`})^2}{(1-\frac{`r n[1]`}{`r N`})\times`r var[1]`+(1-\frac{`r n[2]`}{`r N`})\times`r var[2]`+(1-\frac{`r n[3]`}{`r N`})\times`r var[3]`} \approx `r round((sum(n*(mu-global_mu)^2))/(sum((1-(n/N))*var)),3)`
$$

$$
df_n=`r k`-1=`r k-1`
$$

$$
df_d=\frac{1}{\frac{(\frac{(1-\frac{`r n[1]`}{`r N`})\times`r var[1]`}{\sum_{j=1}^k(1-\frac{n_j}{N})S_j^2})^2}{`r n[1]`-1}+\frac{(\frac{(1-\frac{`r n[2]`}{`r N`})\times`r var[2]`}{\sum_{j=1}^k(1-\frac{n_j}{N})S_j^2})^2}{`r n[2]`-1}+\frac{(\frac{(1-\frac{`r n[3]`}{`r N`})\times`r var[3]`}{\sum_{j=1}^k(1-\frac{n_j}{N})S_j^2})^2}{`r n[3]`-1}} \approx `r round(1/sum(((1-n/N)*var/sum((1-n/N)*var))^2/(n-1)),3)`
$$

where $\sum_{j=1}^k(1-\frac{n_j}{N})\times S_j^2 \approx `r round(sum((1-n/N)*var),2)`$

Finally, the *W*-test and his degrees of freedom  are computed by applying equation (3) and related degrees of freedom :

$$
W=\frac{\frac{1}{`r k`-1}[\frac{`r n[1]`}{`r var[1]`}(`r mu[1]`-\bar{X'})^2+\frac{`r n[2]`}{`r var[2]`}(`r mu[2]`-\bar{X'})^2+\frac{`r n[3]`}{`r var[3]`}(`r mu[3]`-\bar{X'})^2]}
{\frac{2(`r k`-2)}{`r k`^2-1}[(\frac{1}{`r n[1]`-1})(1-\frac{\frac{`r n[1]`}{`r var[1]`}}{w})^2+(\frac{1}{`r n[2]`-1})(1-\frac{\frac{`r n[2]`}{`r var[2]`}}{w})^2+(\frac{1}{`r n[3]`-1})(1-\frac{\frac{`r n[3]`}{`r var[3]`}}{w})^2]+1} \approx `r round(1/(k-1)*(sum(n/var*(mu-muprime)^2))/((2*(k-2))/(k^2-1)*sum(1/(n-1)*(1-(n/var)/w)^2)+1),3)`
$$

where: $w=\sum_{j=1}^k w_j \approx `r round(sum(w_j),2)`$ and $\bar{X'}=\frac{\sum_{j=1}^k (w_j\bar{X_j})}{w} \approx `r round(muprime,2)`$

$$
df_n=`r k`-1
$$

$$
df_d=\frac{`r k`^2-1}{3[\frac{(1-\frac{w_j}{w})^2}{`r n[1]`-1}+\frac{(1-\frac{w_j}{w})^2}{`r n[2]`-1}+\frac{(1-\frac{w_j}{w})^2}{`r n[3]`-1}]} \approx `r round((k^2-1)/(3*(sum((1-w_j/w)^2/(n-1)))),2)`
$$

One should notice that in this example, the biggest sample size has the biggest variance. As previously mentioned, it means that the *F*-test will be too conservative, because the *F* value decreases. The *F*\*-test will also be a little too conservative, even if the test is less affected than the *F*-test. As a consequence: $W$ > $F^*$ > $F$. 

### Appendix 2 : Justification for the choice of distributions in simulations

The set of simulations described in the article was repeated for 7 distributions. We used R commands to generate data from different distributions:

- *k* normal distributions (Figure A2.1) : in order to assess the Type I error rate and power of the different tests under the assumption of normality, data were generated by means of the function "rnorm" (from the package "stats"; "R: The Normal Distribution," 2016).

- *k* double exponential distributions (Figure A2.2) : In order to assess the impact of high kurtosis on the Type I error rate and power of all tests, data were generated by means of the function "rdoublex" (from the package "smoothmest"; "R: The double exponential (Laplace) distribution," 2012).  

- *k* mixed normal distributions (Figure A2.3) : In order to assess the impact of extremely high kurtosis on the Type I error rate and power of all tests, regardless of variance, data were generated by means of the function "rmixnorm" (from the package "bda"; Wang & Wang, 2015).  

- *k* normal right skewed distributions (Figure A2.4) : In order to assess the impact of moderate skewness on the Type I error rate and power, data were generated by means of the function "rsnorm" (from the package "fGarch"; "R: Skew Normal Distribution," 2017). The normal skewed distribution was chosen because it is the only skewed distribution where the standard deviation ratio can vary without having an impact on skewness. 

- *k*-1 normal left skewed distributions (Figure A2.5) and 1 normal right skewed distribution (Figure A2.4) : In order to assess the impact of unequal shapes, in terms of skewness, on the Type I error rate and power, when data have moderate skewness, data were generated by means of the functions "rsnorm" (from the package "fGarch"; "R: Skew Normal Distribution," 2017). 

- *k*-1 chi-squared distributions with two degrees of freedom (See Figure A2.6), and one normal rigt skewed distribution (Figure A2.4) : In order to assess the impact of high asymetry on the Type I error rate an power, k-1 distributions were generated by means of the functions "rchisq" ("R: The (non-central) Chi-squared Distribution," 2016). The last distribution was generated by means of "rsnorm" in order to follow a normal right skewed distribution with a mean of 2 (from the package "fGarch"; "R: Skew Normal Distribution," 2017). Because the chi-squared is non-negative, it is not possible to generate chi-squared where population SD= 1, 4 or 8 and population mean is the same than the chi-squared with two degrees of freedom. However, we wanted to assess the impact of different SD-ratio on Type I error rate. For these reasons, the last distribution was generated by means of "rsnorm" in order to follow a normal skewed distribution with positive skewness of +0.99 and mean = 2 (from the package "fGarch"; "R: Skew Normal Distribution," 2017). 

- *k*-1 chi-squared distributions with two degrees of freedom (See Figure A2.6), and one normal left skewed distribution (Figure A2.5) : In order to assess the impact of unequal shapes, in terms of skewness, on Type I error rate and power when distributions have extreme skewness, k-1 distributions were generated by means of the functions "rchisq" ("R: The (non-central) Chi-squared Distribution," 2016). The last distribution was generated by means of "rsnorm" in order to follow a normal right skewed distribution with a mean of 2 (from the package "fGarch"; "R: Skew Normal Distribution," 2017)

![](C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 3/Appendix figures/1_normal.png)

Figure A2.1 : centered normal probability density function, as a function of the population $SD$

![](C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 3/Appendix figures/2_doublex.png)

Figure A2.2 : centered double exponential probability density function, as a function of the population $SD$

![](C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 3/Appendix figures/3_mixednorm.png)
Figure A2.3 : centered mixed normal probability density function, as a function of the population $SD$

![](C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 3/Appendix figures/4_rightskewed.png)

Figure A2.4 : centered normal right skewed probability density function, as a function of the population $SD$

![](C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 3/Appendix figures/5_leftskewed.png)

Figure A2.5 : centered normal left skewed probability density function, as a function of the population $SD$

![](C:/Users/mdelacre/Documents/Github project/thesis/Chapitre 3/Appendix figures/6_chisquared.png)
Figure A2.6 : chi-squared with 2 degrees of freedom probability density function, when the population $SD$ equals 2


```{r "", echo=FALSE, fig.width = 15,fig.height=8,out.width = '400px',fig.cap = ""}
#n=1000000
#par(mai=c(.5,1,.5,1))
#plot(function(x) dnorm(x,0,1),-15, 15,xlab="Density",ylab="",main #="",lty=1,xlim=c(-15,15),ylim=c(0,.8),lwd=1,cex.lab=1.2)
#plot(function(x) dnorm(x,0,2),-15,15,lty=2,lwd=1,add=TRUE)
#plot(function(x) dnorm(x,0,4),-15,15,xlab="",ylab="",main = "",lty=3,lwd=1,cex.lab=1.2,add=TRUE)
#plot(function(x) dnorm(x,0,8),-15,15,xlab="",ylab="",main = "",lty=6,lwd=1,cex.lab=1.2,add=TRUE)
#legend(0,.79,legend=c("sd=1","sd=2","sd=4","sd=8"),lty=c(1,2,3,6),lwd=c(2,2,2,2),bty="n",xjust=0.5,#yjust=1,horiz=TRUE,xpd=TRUE,cex=4)
```

```{r "", echo=FALSE, fig.width = 15,fig.height=8,out.width = '400px',fig.cap = ""}
#library(smoothmest)
#par(mai=c(.5,1,.5,1))
#plot(function(x) ddoublex(x,mu=0,lambda=1/sqrt(2)),-15, 15,xlab="Density",ylab="",main #="",lty=1,xlim=c(-15,15),ylim=c(0,.8),lwd=1,cex.lab=1.2)
#plot(function(x) ddoublex(x,0,2/sqrt(2)),-15,15,lty=2,lwd=1,add=TRUE)
#plot(function(x) ddoublex(x,0,4/sqrt(2)),-15,15,xlab="",ylab="",main = #"",lty=3,lwd=1,cex.lab=1.2,add=TRUE)
#plot(function(x) ddoublex(x,0,8/sqrt(2)),-15,15,xlab="",ylab="",main = #"",lty=6,lwd=1,cex.lab=1.2,add=TRUE)
#legend(0,.79,legend=c("sd=1","sd=2","sd=4","sd=8"),lty=c(1,2,3,6),lwd=c(2,2,2,2),bty="n",xjust=0.5,#yjust=1,horiz=TRUE,xpd=TRUE,cex=4)
```

```{r "", echo=FALSE, fig.width = 15,fig.height=8,out.width = '400px',fig.cap = ""}
#library(symmetry)
#A <- rmixnorm(n=1000000,p=.1,mean1=0,mean2=0,sd1=2.53, sd2=.6325)
#B <- rmixnorm(n=1000000,p=.1,mean1=0,mean2=0,sd1=5.06, sd2=1.265)
#C <- rmixnorm(n=1000000,p=.1,mean1=0,mean2=0,sd1=10.119, sd2=2.53)
#D <- rmixnorm(n=1000000,p=.1,mean1=0,mean2=0,sd1=20.239, sd2=5.06)

#plot(density(A),xlab="Density",ylab="",main ="",lty=1,xlim=c(-15,15),ylim=c(0,.8),lwd=1,cex.lab=1.2)
#lines(density(B),lty=2,lwd=1)
#lines(density(C),lty=3,lwd=1,cex.lab=1.2)
#lines(density(D),lty=6,lwd=1,cex.lab=1.2)
#legend(0,.79,legend=c("sd=1","sd=2","sd=4","sd=8"),lty=c(1,2,3,6),lwd=c(2,2,2,2),bty="n",xjust=0.5,yjust=1,horiz=TRUE,xpd=TRUE,cex=4)
```

```{r "", echo=FALSE, fig.width = 15,fig.height=8,out.width = '400px',fig.cap = ""}
#library(fGarch)
#par(mai=c(.5,1,.5,1))
#plot(function(x) dsnorm(x, mean=0, sd=1,xi=10),-15, 15,xlab="Density",ylab="",main ="",lty=1,xlim=c(-15,15),ylim=c(0,.8),lwd=1,cex.lab=1.2)
#plot(function(x) dsnorm(x, mean=0, sd=2,xi=10) ,-15,15,lty=2,lwd=1,add=TRUE)
#plot(function(x) dsnorm(x, mean=0, sd=4,xi=10),-15,15,xlab="",ylab="",main = "",lty=3,lwd=1,cex.lab=1.2,add=TRUE)
#plot(function(x) dsnorm(x, mean=0, sd=8,xi=10),-15,15,xlab="",ylab="",main = "",lty=6,lwd=1,cex.lab=1.2,add=TRUE)
#legend(0,.79,legend=c("sd=1","sd=2","sd=4","sd=8"),lty=c(1,2,3,6),lwd=c(2,2,2,2),bty="n",xjust=0.5,yjust=1,horiz=TRUE,xpd=TRUE,cex=4)
```

```{r "", echo=FALSE, fig.width = 15,fig.height=8,out.width = '400px',fig.cap = ""}
#n=1000000
#NN1=rsnorm(n, mean=0, sd=1,xi=-10) 
#NN2=rsnorm(n, mean=0, sd=2,xi=-10) 
#NN3=rsnorm(n, mean=0, sd=4,xi=-10)  
#NN4=rsnorm(n, mean=0, sd=8,xi=-10)  
#par(mai=c(.5,1,.5,1))
#plot(density(NN1),lty=1,xlim=c(-15,15),ylim=c(0,.8),lwd=2,main="",xlab="",cex.lab=1.2)
#lines(density(NN2),lty=2,lwd=2)
#lines(density(NN3),lty=3,lwd=2)
#lines(density(NN4),lty=6,lwd=2)
#legend(0,.79,legend=c("sd=1","sd=2","sd=4","sd=8"),lty=c(1,2,3,6),lwd=c(2,2,2,2),bty="n",xjust=0.5,yjust=1,horiz=TRUE,xpd=TRUE,cex=4)
```

```{r "", echo=FALSE, fig.width = 15,fig.height=8,out.width = '400px',fig.cap = ""}
#plot(function(x) dchisq(x,df=2),-15, 15,xlab="Density",ylab="",main ="",lty=1,xlim=c(-15,15),ylim=c(0,.8),lwd=1,cex.lab=1.2)
#legend(0.79,legend="sd=2",lty=1,lwd=1,bty="n",xjust=0.5,yjust=1,horiz=TRUE,xpd=TRUE,cex=4)
```

\newpage
## Annexes du Chapitre 4 

### Appendix 1 : The bias of Cohen's $\bm{d}$ is twice as large as the bias of Shieh's $\bm{d}$ when population variances and sample sizes are equal across groups: mathematical demonstration. 

As mentioned in Table 2, the bias of Cohen's $d$ is defined as 
\begin{equation} 
Bias_{Cohen's \; d}= \delta_{Cohen} \times \left( \frac{\sqrt{\frac{df_{Student}}{2}} \times \Gamma{\left(\frac{df_{Student}-1}{2}\right)}}{\Gamma{\left( \frac{df_{Student}}{2}\right)}} -1 \right)
\label{eqn:Cohenbias}
\end{equation} 
with 
\begin{equation*} 
\delta_{Cohen}=\frac{\mu_1-\mu_2}{\sqrt{\frac{(n_1-1)\times \sigma^2_1+(n_2-1)\times\sigma^2_2}{n_1+n_2-2}}}
\label{eqn:Cohendelta}
\end{equation*} 
and 
\begin{equation*} 
df_{Student}=n_1+n_2-2
\label{eqn:Cohendf}
\end{equation*}

As mentioned in Table 3, the bias of Shieh's $d$ is defined as 
\begin{equation} 
Bias_{Shieh's \; d}=\delta_{Shieh} \times \left( \frac{\sqrt{\frac{df_{Welch}}{2}} \times \Gamma{\left(\frac{df_{Welch}-1}{2}\right)}}{\Gamma{\left( \frac{df_{Welch}}{2}\right)}} -1 \right)
\label{eqn:Shiehbias}
\end{equation} 
with 
\begin{equation*} 
\delta_{Shieh}=\frac{\mu_1-\mu_2}{\sqrt{\frac{\sigma^2_1}{n_1/N}+\frac{\sigma^2_2}{n_2/N}}} \quad (N=n_1+n_2)
\label{eqn:Shiehdelta}
\end{equation*} 
and 
\begin{equation*} 
df_{Welch}=\frac{\left(\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2} \right)^2}{\frac{(\sigma^2_1/n_1)^2}{n_1-1}+\frac{(\sigma^2_2/n_2)^2}{n_2-1}}
\label{eqn:Welchdf}
\end{equation*}

\newpage
When $n_1=n_2=n$ and $\sigma_1=\sigma_2=\sigma$, $\delta_{Cohen}$ is twice larger than $\delta_{Shieh}$, as shown below in equations \ref{eqn:Cohendeltavarbalanced} and \ref{eqn:Shiehdeltavarbalanced}:
\begin{equation} 
\delta_{Cohen}=\frac{\mu_1-\mu_2}{\sqrt{\frac{2(n-1)\sigma^2}{2(n-1)}}}=\bm{\frac{\mu_1-\mu_2}{\sigma}}
\label{eqn:Cohendeltavarbalanced}
\end{equation} 
\begin{equation} 
\delta_{Shieh}=\frac{\mu_1-\mu_2}{\sqrt{2\left( \frac{\sigma^2}{n/(2n)}\right)}}=\bm{\frac{\mu_1-\mu_2}{2\sigma}} 
\label{eqn:Shiehdeltavarbalanced}
\end{equation}   
Moreover, degrees of freedom associated with Student's *t*-test and Welch's *t*-test are identical, as shown below in equations \ref{eqn:Studentdfvarbalanced} and \ref{eqn:Welchdfvarbalanced}:
\begin{equation} 
df_{Student}=\bm{2(n-1)} 
\label{eqn:Studentdfvarbalanced}
\end{equation}
\begin{equation} 
df_{Welch}=\frac{\left[2(\sigma^2/n)\right]^2}{\frac{2(\sigma^2/n)^2}{n-1}}= \bm{2(n-1)} 
\label{eqn:Welchdfvarbalanced}
\end{equation}

Equations \ref{eqn:Cohenbias} and \ref{eqn:Shiehbias} can therefore be redefined as follows:
\begin{equation*} 
Bias_{Cohen's \; d}=\frac{\mu_1-\mu_2}{\sigma} \times \left( \frac{\sqrt{n-1} \times \Gamma{\left(\frac{2n-3}{2}\right)}}{\Gamma{\left( n-1\right)}} -1 \right)
\label{eqn:Cohenbiasvarbalanced}
\end{equation*} 
\begin{equation*} 
Bias_{Shieh's \; d}=\frac{\mu_1-\mu_2}{\bf 2\sigma} \times \left( \frac{\sqrt{n-1} \times \Gamma{\left(\frac{2n-3}{2}\right)}}{\Gamma{\left( n-1\right)}} -1 \right)
\label{eqn:Shiehbiasvarbalanced}
\end{equation*} 

We can therefore conclude that the bias of Cohen's $d$ is twice larger than the bias of Shieh's $d$.

\newpage

### Appendix 2 : The variance of Cohen's $\bm{d}$ is four times larger than the bias of Shieh's $\bm{d}$ when population variances and sample sizes are equal across groups: mathematical demonstration.

The variance of Cohen's $d$ is defined in Table 2 as 
\begin{equation}
Var_{Cohen's \; d}=\frac{N\times df_{Student}}{n_1n_2 \times (df_{Student}-2)} + \delta^2_{Cohen} \left[ \frac{df_{Student}}{df_{Student}-2} - \left( \frac{\sqrt{\frac{df_{Student}}{2}} \times \Gamma{\left(\frac{df_{Student}-1}{2}\right)}}{\Gamma{\left( \frac{df_{Student}}{2}\right)}} \right)^2\right]
\label{eqn:Cohenvar}
\end{equation} 
and the variance of Shieh's $d$ is defined in Table 2 as
\begin{equation}
Var_{Shieh's \; d}=\frac{df_{Welch}}{(df_{Welch}-2)N}  + \delta^2_{Shieh} \left[ \frac{df_{Welch}}{df_{Welch}-2} - \left( \frac{\sqrt{\frac{df_{Welch}}{2}} \times \Gamma{\left(\frac{df_{Welch}-1}{2}\right)}}{\Gamma{\left( \frac{df_{Welch}}{2}\right)}} \right)^2 \right]
\label{eqn:Shiehvar}
\end{equation} 

We have previously shown in equations \ref{eqn:Studentdfvarbalanced} and \ref{eqn:Welchdfvarbalanced} that degrees of freedom associated with Student's *t*-test and Welch's *t*-test equal $2(n-1)$, when $n_1=n_2=n$ and $\sigma_1=\sigma_2=\sigma$. As a consequence, the first term of the addition in equation \ref{eqn:Cohenvar} is 4 times larger than the first term of the addition in equation \ref{eqn:Shiehvar}: 
$$\frac{N\times df_{Student}}{n_1n_2 \times (df_{Student}-2)}=\frac{2n\times 2(n-1)}{n^2 \times (2n-4)} =\bm{\frac{4(n-1)}{n(2n-4)}} $$
$$\frac{df_{Welch}}{(df_{Welch}-2)N} = \frac{2(n-1)}{2n(2n-4)}= \bm{\frac{n-1}{n(2n-4)}}$$
We have also previously shown in equations \ref{eqn:Cohendeltavarbalanced} and \ref{eqn:Shiehdeltavarbalanced} that $\delta_{Cohen}$ is twice larger than $\delta_{Shieh}$ when $n_1=n_2=n$ and $\sigma_1=\sigma_2=\sigma$ and, therefore, $\delta^2_{Cohen}$ is four times larger than $\delta^2_{Shieh}$. As a consequence, the second term of the addition in equation \ref{eqn:Cohenvar} is also 4 times larger than the second term of the addition in equation \ref{eqn:Shiehvar}. Because both terms of the addition in equation  \ref{eqn:Cohenvar} are four times larger than those in equation \ref{eqn:Shiehvar}, we can conclude that the variance of Cohen's $d$ is four times larger than the variance of Shieh's $d$.