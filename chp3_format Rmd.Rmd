---
title: " "
output: 
  papaja::apa6_pdf:
    includes:
      extra_dependencies: ["float"]

header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
---

```{r setup2, include = FALSE}
library("papaja")
library("bookdown")
library("knitr")
library("bda")
library("smoothmest")
library("moments")
library("fGarch")
library("dplyr")
```

```{r analysis-preferences2, include = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Chapitre 3 : Utiliser l'ANOVA $\bf \emph W$ de Welch par d√©faut 
```{r "chp3p1", fig.align='center', fig.cap=NULL,echo=FALSE,out.height="54%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 3/Chapitre 3-couverture.png")
```

When comparing independent groups researchers often analyze the means by performing a Student's *t*-test or classical Analysis of Variance (ANOVA) *F*-test [@erceg-hurn_modern_2008;@keselman_statistical_1998; @Tomarken_and_Serlin_1986]. Both tests rely on the assumptions that independent and identically distributed residuals (1) are sampled from a normal distribution and (2) have equal variances between groups [or homoscedasticity; see @Lix_Keselman_Keselman_1996]. While a deviation from the normality assumption generally does not strongly affect either the Type I error rates [@glass_consequences_1972; @Harwell_et_al_1992; @tiku_power_1971] or the power of the *F*-test [@Harwell_et_al_1992; @David_and_Johnson_1951; @Srivastava_1959; @tiku_power_1971],  the *F*-test is not robust against unequal variances [@grissom_heterogeneity_2000]. Unequal variances can alter both the Type I error rate [@Harwell_et_al_1992; @David_and_Johnson_1951] and statistical power [@Nimon_2012;@Overall_et_al_1995] of the *F*-test.

Although it important to make sure test assumptions are met before a statistical test is performed, researchers rarely provide information about test assumptions when they report an *F*-test. We examined statistical tests reported in 116 articles in the *Journal of Personality and Social Psychology* published in 2016. Fourteen percent of these articles reported a one-way *F*-test, but only one article indicated that the homogeneity of variances assumption was taken into account. They reported corrected degrees of freedom for unequal variances, which could signal the use of the *W*-test instead of the classical *F*-test. A similar investigation [@hoekstra_are_2012] yielded conclusions about the lack of attention to both the homoscedasticity and the normality assumptions. Despite the fact that the *F*-test is currently used by default, better alternatives exist, such as the Welch's *W* ANOVA (*W*-test), the Alexander-Govern test, James' second order test, and the Brown-Forsythe ANOVA (*F*\*-test). Although not the focus of the current article, additional tests exist that allow researchers to compare groups either based on other estimators of central tendency than the mean [see for example @erceg-hurn_modern_2008; @wilcox_how_1998], or based on other relevant parameters of distribution than the central tendency, such as standard deviations and the shape of the distribution [@grissom_heterogeneity_2000; @Tomarken_and_Serlin_1986]. However, since most researchers currently generate hypotheses about differences between means [@erceg-hurn_modern_2008; @keselman_statistical_1998], we think that a first realistic first step towards progress would be to get researchers to correctly test the hypothesis they are used to. 

Although the debate surrounding the assumptions of the *F*-test has been widely explored [see for example the meta-analysis of @Harwell_et_al_1992], applied researchers still largely ignore the consequences of assumption violations. Non-mathematical pedagogical papers summarizing the arguments seem to be lacking from the literature, and the current paper aims to fill this gap. We will discuss the pertinence of the assumptions of the *F*-test, and focus on the question of heteroscedasticity (that, as we will see, can have major consequences on error rates). We will provide a non-mathematical explanation of how alternatives to the classical *F*-test cope with heteroscedasticity violations. We conducted simulations in which we compare the *F*-test with the most promising alternatives. We argue that when variances are equal between groups, the *W*-test has nearly the same empirical Type I error rate and power as the *F*-test, but when variances are unequal, it provides empirical Type I and Type II error rates that are closer to the expected levels compared to the *F*-test. Since the *W*-test is available in practically all statistical software packages, researchers can immediately improve their statistical inferences by replacing the *F*-test by the *W*-test.

## Normality and Homogeneity of variances under Ecological Conditions

For several reasons, assumptions of homogeneity of variances and normality are always more or less violated [@glass_consequences_1972]. In this section we will summarize the specificity of the methods used in our discipline that can account for this situation. 

## Normality Assumption

It has been argued that there are many fields in psychology where the assumption of normality does not hold [@cain_univariate_2017; @micceri_unicorn_1989; @yuan_structural_2004]. As argued by @micceri_unicorn_1989, there are several factors that could explain departures from the normality assumption, and we will focus on three of them : treatment effects, the presence of subpopulations, and the bounded measures underlying residuals.

First, although the mean can be influenced by the treatment effects, experimental treatment could also change the shape of a distribution, either by influencing the *skewness*, quantifying the asymmetry of the shape of the distribution, and *kurtosis*, a measure of the tendency to produce extreme values. A distribution with positive kurtosis will have heavier tails than the normal distribution, which means that extreme values will be more likely, while a distribution with negative kurtosis will have lighter tails than the normal distribution, meaning that extreme values will be less likely [@Wilcox_2005;@Westfall_2014]. For example, a training aiming at reducing a bias perception of threat when being exposed to ambiguous words will not uniformly impact the perception of all participants, depending on their  level of anxiety [@Grey_and_Mathiews_2000]. This could influence the kurtosis of the distribution of bias score. 

Second, prior to any experimental treatment, the presence of several subpopulations may lead to departures from the normality assumptions. Subgroups might exist that are unequal on some characteristics relevant to the measurements, that are not controlled within the studied group, which results in mixed distributions. This unavoidable lack of control is inherent of our field given its complexity. As an illustration, @Wilcox_2005 writes that pooling two normally-distributed populations that have the same mean but different variances (e.g. normally distributed scores for schizophrenic and not schizophrenic participants) could result in distributions that are very similar to the normal curve, but with thicker tails. As another example, when assessing a wellness score for the general population, data may be sampled from a left-skewed distribution, because most people are probably not depressed [see @Heun_et_al_1999]. In this case, people who suffer from depression and people who do not suffer from depression are part of the same population, which can leads to asymmetry in the distribution.   

Third, bounded measures can also explain non-normal distributions. For example, response time can be very large, but never below zero, which results in right-skewed distributions. In sum, there are many common situations in which normally distributed data is an unlikely assumption. 

## Homogeneity of Variances Assumption
Homogeneity of variances (or homoscedasticity) is a mathematical requirement that is also ecologically unlikely [@erceg-hurn_modern_2008; @grissom_heterogeneity_2000]. In a previous paper [@delacre_why_2017], we identified three different causes of heteroscedasticity : the variability inherent to the use of measured variables, the variability induced by quasi-experimental treatments on measured variables, and the variability induced by different experimental treatments on randomly assigned subjects. One additional source of variability is the presence of unidentified moderators [@Cohen_et_al_2013].

First, psychologists, as many scholars from various fields in human sciences, often use measured variables (e.g. age, gender, educational level, ethnic origin, depression level, etc.) instead of random assignment to conditions. Prior to any treatment, parameters of pre-existing groups can vary largely from one population to another, as suggested by @henrich_most_2010. For example, @Green_et_al_2005 have shown that the scores of competitiveness, self-reliance and interdependence are more variable in some ethnic groups than in others. This stands true for many pre-existing groups such as gender, cultures, or religions and for various outcomes [see for example @Adams_et_al_2014; @Beilmann_et_al_2014; @Church_et_al_2012; @Cohen_and_Hill_2007; @Haar_et_al_2014; @Montoya_Briggs_2013]. Moreover, groups are sometimes defined with the intention to have different variabilities. For example, as soon as a selective school admits its students based on the results of aptitude tests, the variability will be smaller compared to a school that accepts all students.  

Second, a quasi-experimental treatment can have different impacts on variances between pre-existing groups, that can even be of theoretical interest. For example, in the field of linguistics and social psychology, @Wasserman_and_Weseley_2009 investigated the impact of language gender structure on sexist attitudes of women and men. They tested differences between sexist attitude scores of subjects who read a text in English (i.e. a language without grammatical gender) or in Spanish (i.e. a language with grammatical gender). The results showed that (for a reason not explained by the authors), the women's score on the sexism dimension was more variable when the text was read in Spanish than in English ($S_{spanish}=.80 > S_{english}=.50$, with S = sample standard deviation). For men, the reverse was true ($S_{spanish}=.97 < S_{english}=1.33$)\footnote{Note that this is a didactic example, the differences have not been tested and might not differ statistically.}  

Third, even when the variances of groups are the same before treatment (due to a complete succesful randomization in group assignment), unequal variances can emerge later, as a consequence of an experimental treatment [@bryk_heterogeneity_1988;@cumming_understanding_2013; @erceg-hurn_modern_2008;@Keppel_and_Wickens_2004;@box_theorems_1954]. For example, @Koeser_and_Sczesny_2014 have compared arguments advocating either masculine generic or gender-fair language with control messages in order to test the impact of these conditions on the use of gender-fair wording (measured as a frequency). They report that the standard deviations increase after treatment in all experimental conditions.

# Consequences of Assumption Violations

Assumptions violations would not be a matter per se, if the *F*-test was perfectly robust against departures from them [@glass_consequences_1972]. When performing a test, two types of errors can be made : Type I errors and Type II errors. A Type I error consists of falsely rejecting the null hypothesis in favour of an alternative hypothesis, and the Type I error rate ($\alpha$) is the proportion of tests that, when sampling many times from the same population, reject the null hypothesis when there is no true effect in the population. A Type II error consists of failing to reject the null hypothesis, and the Type II error rate ($\beta$) is the proportion of tests, when sampling many times from the same population, that fail to reject the null hypothesis when there is a true effect. Finally, the statistical power (1-$\beta$) is the proportion of tests, when sampling many times from the same population, that correctly reject the null hypothesis when there is a true effect in the population.

## Violation of the Normality Assumption

Regarding the Type I error rate, the shape of the distribution has very little impact on the *F*-test [@Harwell_et_al_1992]. When departures are very small (i.e. a kurtosis between 1.2 and 3 or a skewness between -.4 and .4), the Type I error rate of the *F*-test is very close to expectations, even with sample sizes as small as 11 subjects per group [@Hsu_and_Feldt_1969].     
Regarding the Type II error rate, many authors underlined that departures from normality do not seriously affect the power [@tiku_power_1971; @David_and_Johnson_1951; @Harwell_et_al_1992; @Srivastava_1959; @Boneau_1960; @glass_consequences_1972]. However, we can conclude from @Srivastava_1959 and @Boneau_1960 that kurtosis has a slightly larger impacts on the power than skewness. The effect of non-normality on power increases when sample sizes are unequal between groups [@glass_consequences_1972]. Lastly the effect of non-normality decreases when sample sizes increase [@Srivastava_1959].    

## Violation of Homogeneity of Variances Assumption

Regarding the Type I error rate, the *F*-test is sensitive to unequal variances [@Harwell_et_al_1992]. More specifically, the more unequal the *SD* of the populations samples are extracted from, the higher the impact. When there are only two groups, the impact is smaller than when there are more than two groups [@Harwell_et_al_1992]. When there are more than two groups, the *F*-test becomes more liberal, meaning that the Type I error rate is larger than the nominal alpha level, even when sample sizes are equal across groups [@Tomarken_and_Serlin_1986]. Moreover, when sample sizes are unequal, there is a strong effect of the sample size and variance pairing. In case of a positive pairing (i.e. the group with the larger sample size also has the larger variance), the test is too conservative, meaning that the Type I error rate of the test is lower than the nominal alpha level, whereas in case of a negative pairing (i.e. the group with the larger sample size has the smaller variance), the test is too liberal [@glass_consequences_1972; @Nimon_2012; @Overall_et_al_1995; @Tomarken_and_Serlin_1986].   

Regarding the Type II error rate, there is a small impact of unequal variances when sample sizes are equal [@Harwell_et_al_1992], but there is a strong effect of the sample size and variance pairing [@Nimon_2012; @Overall_et_al_1995]. In case of a positive pairing, the Type II error rate increases (i.e. the power decreases), and in case of a negative pairing, the Type II error decreases (i.e. the power increases). 

### Cumulative Violation of Normality and Homogeneity of Variance

Regarding both Type I and Type II error rates, following @Harwell_et_al_1992, there is no interaction between normality violations and unequal variances. Indeed, the effect of heteroscedasticity is relatively constant regardless of the shape of the distribution. 

Based on mathematical explanations and Monte Carlo simulations we chose to compare the *F*-test with the *W*-test and *F*\*-test and to exclude the James' second-order and Alexander-Govern's test because the latter two yield very similar results to the *W*-test, but are less readily available in statistical software packages. @Tomarken_and_Serlin_1986 have shown that from the available alternatives, the *F*\*-test and the *W*-test perform best, and both tests are available in SPSS, which is widely used software in the psychological sciences [@hoekstra_are_2012]. For a more extended description of the James' second-order and Alexander-Govern's test, see @Schneider_and_Penfield_1997.

## The Mathematical Differences Between the *F*-test, *W*-test, and *F*\*-test

The mathematical differences between the *F*-test, *W*-test and *F*\*-test can be explained by focusing on how standard deviations are pooled across groups. As shown in equation \ref{eqn:Fstat}, the *F* statistic is calculated by dividing the inter-group variance by a pooled error term :  
\begin{equation} 
F=\frac{\frac{1}{k-1}\sum_{j=1}^k \left[n_j(\bar{X_{j}}-\bar{X_{..}})^2\right]}{\frac{1}{N-k}\sum_{j=1}^k\left(n_j-1\right)s_j^2}
\label{eqn:Fstat}
\end{equation} 
where $S_{j}^2$, $\bar{X_{j}}$ and $n_{j}$ are respectively the sample variance, sample mean and the sample size of the $j^{th}$ group ($j= 1,...,k$), $N=\sum_{j=1}^k n_j$,  and $\bar{X_{..}}$ is overall mean. The degrees of freedom in the numerator and in the denominator of the *F*-test are computed as follows :  
\begin{equation*} 
df_n=k-1
\label{eqn:FnumDF}
\end{equation*} 
\begin{equation*} 
df_d= N-k, 
\label{eqn:FdenomDF}
\end{equation*} 
As a generalization of the Student's *t*-test, the *F*-test is calculated based on a pooled error term. This implies that all samples are considered as issued from a common population variance (hence the assumption of homoscedasticity). When there is heteroscedasticity and the larger variance is associated with the larger sample size, the error term, which is the denominator in equation \ref{eqn:Fstat}, is overestimated. The *F*-value is therefore smaller, leading to fewer significant findings than expected, and the *F*-test is too conservative. When the larger variance is associated with the smaller sample size, the denominator in equation \ref{eqn:Fstat} is underestimated. The *F*-value is then inflated, which yields more significant results than expected.

The *F*\* statistic proposed by @Brown_and_Forsythe_1974 is computed as follows :
\begin{equation} 
F^*= \frac{\sum_{j=1}^k\left[n_j(\bar{X_j}-\bar{X_{..}})^2\right]}{\sum_{j=1}^k \left[\left(1-\frac{n_j}{N}\right)S_j^2\right]}
\label{eqn:BFstat}
\end{equation} 
where $X_j$ and $S_j^2$ are respectively the mean and variance of the $j^{th}$ group ($j= 1,...k$), and $\bar{X_{..}}$ is the overall mean. As it can be seen in equation \ref{eqn:BFstat}, the numerator of the *F*\* statistic is equal to the sum of squares between groups (which is equal to the numerator of the *F* statistic when one compares two groups). In the denominator, the variance of each group is weighted by 1 minus the relative frequency of each group. This adjustment implies that the variance associated with the group with the smallest sample size is given more weight compared to the *F*-test. As a result, when the larger variance is associated with the larger sample size, *F*\* is larger than *F*, because the denominator decreases, leading to more significant findings compared to the *F*-test. On the other hand, when the larger variance is associated with the smaller sample size, *F*\* is smaller than *F*, because the denominator increases, leading to fewer significant findings compared to the *F*-test. The degrees of freedom in the numerator and in the denominator of *F*\*-test are computed as follows (with the same principle as the denominator computation of the *F\** statistic) :
\begin{equation*} 
df_n= k-1
\label{eqn:BFnumDF}
\end{equation*} 
\begin{equation*} 
df_d= \frac{1}{\sum_{j=1}^k\left[\frac{\left(\frac{\left(1-\frac{n_j}{N}\right)S_j^2}{\sum_{j=1}^k\left[\left(1-\frac{n_j}{N}\right)S_j^2\right]}\right)^2}{n_j-1}\right]}
\label{eqn:BFdenomDF}
\end{equation*} 
Equation \ref{eqn:Wstat} provides the computation of the Welch's statistic ($W$). In the numerator of the *W* statistic, the squared deviation between group means and the general mean are weighted by $\frac{n_j}{s_j^2}$ instead of $n_j$ [@Brown_and_Forsythe_1974]. As a consequence, for equal sample sizes, the group with the highest variance will have smaller weight [@Liu_2015]. 
\begin{equation} 
W=\frac{\frac{1}{k-1}\sum_{j=1}^k\left[w_j(\bar{X_j}-\bar{X'})^2\right]}
{1+\frac{2(k-2)}{k^2-1}\sum_{j=1}^k\left[(\frac{1}{n_j-1})(1-\frac{w_j}{w})^2\right]}
\label{eqn:Wstat}
\end{equation} 
where $w_j=\frac{n_j}{S_j^2}$, $w=\sum_{j=1}^k(\frac{n_j}{S_j^2})$ and $\bar{X'}=\frac{\sum_{j=1}^k(w_j\bar{X_j})}{w}$. The degrees of freedom of the *W*-test are approximated as follows :
\begin{equation*} 
df_n= k-1
\label{eqn:WnumDF}
\end{equation*} 
\begin{equation*}
df_d= \frac{k^2-1}{3\sum_{j=1}^k[\frac{(1-\frac{w_j}{w})^2}{n_j-1}]}
\label{eqn:WdenomDF}
\end{equation*} 
When there are only two groups to compare, the *F*\*-test and *W*-test are identical (i.e., they have exactly the same statistical value, degrees of freedom and significance). However, when there are more than two groups to compare, the tests differ. In the Supplemental Material we illustrate the calculation of all three statistics in detail for a fictional three-group design for educational purposes. 

