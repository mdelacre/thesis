---
title: " "
output: 
  papaja::apa6_pdf:
    includes:
      extra_dependencies: ["float"]
header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
---

# Chapitre 2 : Utiliser le test $\bf \emph t$ de Welch par défaut

```{r "chp2p1", fig.align='center',echo=FALSE,out.height="35%"}
knitr::include_graphics("C:/Users/Admin/Documents/Github projects/thesis/Chapitre 2/Chapitre 2-couverture.png")
```
Independent samples $t$-tests are commonly used in the psychological literature to statistically test differences between means. There are different types of $t$-tests, such as Student’s $t$-test, Welch’s $t$-test, Yuen’s $t$-test, and a bootstrapped $t$-test. These variations differ in the underlying assumptions about whether data is normally distributed, and whether variances in both groups are equal [see e.g., @rasch_two-sample_2011;@yuen_two-sample_1974]. Student’s $t$-test is the default method to compare two groups in psychology. The alternatives that are available are considerably less often reported. This is surprising, since Welch’s $t$-test is often the preferred choice, and is available in practically all statistical software packages.   

In this article, we will review the differences between Welch’s $t$-test, Student’s $t$-test, and Yuen’s $t$-test, and suggest that Welch’s $t$-test is a better default for the social sciences than Students and Yuen’s $t$-tests. We do not include the bootstrapped $t$-test because it is known to fail in specific situations, such as when there are unequal sample sizes and standard deviations differ moderately [@hayes_further_2007].

When performing a $t$-test, several software packages (i.e., R and Minitab) present Welch’s $t$-test by default. Users can request Student’s $t$-test, but only after explicitly stating that the assumption of equal variances is met. Student’s $t$-test is a parametric test, which means it relies on assumptions about the data that are analyzed. Parametric tests are believed to be more powerful than non-parametric tests [i.e. tests that do not require assumptions about the population parameters, @sheskin_handbook_2003]. However, Student’s $t$-test is generally only more powerful if the data are normally distributed (the assumption of normality) and the variances are equal in both groups [homoscedasticity, the assumption of homogeneity of variance, @carroll_note_1985; @erceg-hurn_modern_2008]. 

When sample sizes are equal between groups, Student’s $t$-test is robust to violations of the assumption of equal variances as long as sample sizes are big enough to allow correct estimates of both means and standard deviations (i.e. $n \ge 5$)^[There is a Type I error rate inflation in a few cases where sample sizes are extremely small and SDR is big (for example, when $n_1 = n_2 = 3$ are sampled from uniform distributions and SDR = 2, the Type I error rate = 0.083; or when $n_1 = 3$ is sampled from a uniform distribution and $n_2 = 3$ is sampled from a double exponential distribution). However, with extremely small sample sizes ($n_1+n_2 \le 5$), the estimate of means and standard deviations is extremely inaccurate anyway. As we mentioned in Table A2 (see the additional file), the smaller the sample size, the further the average standard deviation is from the population standard deviation, and the larger the dispersion around this average.], except when distributions underlying the data have very high skewness and kurtosis, such as a chi-square distribution with 2 degrees of freedom. However, if variances are *not* equal across groups, and the sample sizes differ across independent groups, Student’s $t$-test can be severely biased, and lead to invalid statistical inferences [@erceg-hurn_modern_2008].\footnote{This is called the Behren-Fisher problem  (Hayes $\&$ Cai, 2007).}\footnote{In a simulation that explored Type 1 error rates, we varied the size of the first sample from 10 to 40 in steps of 10, and the sample sizes ratio and the standard deviation ratio from 0.5 to 2 in steps of 0.5, resulting in 64 simulations designs. Each design was tested 1,000,000 times. Considering these parameter values, we found that the alpha level can be inflated up to .11 or deflated down to .02 (See the Additional file).} Here, we argue that there are no strong reasons to assume equal variances in the psychological literature by default, nor substantial costs in abandoning this assumption.

In this article, we will first discuss why we need a default test, and why a two-step procedure where researchers decide whether or not to use Welch’s $t$-test based on a check of the assumption of normality and equal variances is undesirable. Then, we will discuss whether the assumption of equal variances is plausible in psychology, and point out research areas where this assumption is implausible. We will then review differences between Student’s $t$-test, Welch’s $t$-test, and Yuen’s $t$-test, and show through simulations that the bias in Type I error rates when Yuen’s $t$-test is used is often severely inflated [above .075, which is “critical inflation”, following @bradley_robustness_1978], and that the bias in Type I error rates when Student’s $t$-test is used has a larger impact on statistical inferences than the rather modest impact on the Type II error rate of always using Welch’s $t$-test by default. Given our analysis, and the availability of Welch’s $t$-test in all statistical software, we recommend a procedure where Welch’s $t$-test is used by default when sample sizes are unequal. 

## Limitations of Two-Step Procedures
Readers may have learned that the assumptions of normality and of equal variances (or the homoscedasticity assumption) must be examined using assumption checks prior to performing any $t$-test. When data are not normally distributed, with small sample sizes, alternatives should be used. Classic nonparametric statistics are well known, such as the Mann-Whitney $U$-test and Kruskal-Wallis. However, unlike $t$-test, tests based on rank assume that the distributions are the same between groups. Any departure to this assumption, such as unequal variances, will therefore lead to the rejection of the assumption of equal distributions [@zimmerman_statistical_2000]. Alternatives exist, known as the “modern robust statistics” [@wilcox_modern_2013]. For example, data sets with low kurtosis (i.e., a distribution flatter than the normal distribution) should be analyzed with the two-sample trimmed $t$-test for unequal population variances, also called Yuen’s $t$-test [@luh_approximate_2007; @yuen_two-sample_1974]. However, analyses in a later section will show that the normality assumption is not very important for Welch’s $t$-test, and that there are good reasons to, in general, prefer Welch’s $t$-test over Yuen’s $t$-test.

With respect to the assumption of homogeneity of variance, if the test of the equality of variance is non-significant, and the assumption of equal variances cannot be rejected, homoscedastic methods such as the Student’s $t$-test should be used [@wilcox_modern_2013]. If the test of the equality of variances is significant, Welch’s $t$-test should be used instead of Student’s $t$-test, because the assumption of equal variances is violated. However, testing the equality of variances before deciding which $t$-test is performed is problematic for several reasons, which will be explained after having described some of the most widely used tests of equality of variances.

## Different ways to test for equal variances
Researchers have proposed several tests for the assumption of equal variances. Levene’s test and/or the $F$-ratio test are the most likely to be used by researchers because they are available in popular statistical software [@hayes_further_2007]. Levene’s test is the default option in SPSS. Levene’s test is the One-Way ANOVA computed on the terms $|X_{ij}-\hat{\theta_j}|$, where $X_{ij}$ is the $i^{th}$ observation in the $j^{th}$ group, and $\hat{\theta_j}$ is the “center” of the distribution for the $j^{th}$ group [@carroll_note_1985]. In R, the “center” is by default the median, which is also called “Brown Forsythe test for equal variances”. In SPSS, the “center” is by default the mean (which is the most powerful choice when the underlying data are symmetrical).^[ Other variants have been proposed such as the % trimmed mean [@lim_comparison_1996].] The $F$-ratio statistic is obtained by computing  $\frac{max(S_1,S_2)}{min(S_1,S_2)}$  where $S_j$ is the sample standard deviation of the $j^{th}$ group ($j=1,2$). A generalization of the $F$-ratio test, to be used when there are more than two groups to compare, is known as the Bartlett’s test. 

The $F$-ratio test and the Bartlett test are powerful, but are only valid under the assumption of normality, and they collapse as soon as one deviates even slightly from the normal distribution. They are therefore not recommended [@rakotomalala_comparaison_2008].

Levene’s test is more robust than Bartlett’s test and the $F$-ratio test, but there are three arguments against the use of Levene’s test. First, there are several ways to compute Levene’s test (i.e., using the median or mean as center), and the best version of the test for equal variances depends on how symmetrically the data is distributed, which is itself difficult to statistically quantify. 

Second, performing two tests (Levene’s test followed by a $t$-test) on the same data makes the alpha level and power of the $t$-test dependent upon the outcome of Levene’s test. When we perform Student’s or Welch’s $t$-test conditionally on a significant Levene’s test, the long-run Type I and Type II error rates will depend on the power of Levene’s test. When the power of Levene’s test is low, the error rates of the conditional choice will be very close to Student’s error rates (because the probability of choosing Student’s $t$-test is very high). On the other hand, when the power of Levene’s test is very high, the error rates of the conditional choice will be very close to Welch’s error rate [because the probability of choosing Welch’s $t$-test is very high; see @rasch_two-sample_2011]. When the power of Levene's test is medium, the error rates of the conditional choice will be somewhere between Student's and Welch's error rates [see, for example, @zimmerman_note_2004]. This is problematic when the test most often performed actually has incorrect error rates.

Third, and relatedly, Levene’s test can have very low power leading to Type II errors when sample sizes are small and unequal [@nordstokke_cautionary_2007]. As an illustration, in order to estimate the power of Levene’s test, we simulated 1,000,000 simulations with balanced designs of different sample sizes (ranging from 10 to 80 in each condition, with a step of 5) under three population standard deviation ratio (SDR = $\frac{\sigma_2}{\sigma_1}$ where $\sigma_j$ is the population standard deviation of the $j^{th}$ group ($j=1,2$) : respectively 1.1, 1.5 and 2, yielding 45,000,000 simulations in total. When SDR = 1, the equal variances assumption is true, when SDR > 1 the standard deviation of the second population is bigger than the standard deviation of the first population, and when SDR < 1 the standard deviation of the second population is smaller than the standard deviation of the first population. We ran Levene’s test centered around the mean and Levene’s test centered around the median and estimated the power (in $\%$) to detect unequal variances, with equal sample sizes (giving the best achievable power for a given total $N$; see Figure 1). \footnote{Because sample sizes are equal for each pair of samples, which sample has the bigger standard deviation is not applicable. In this way, $SDR = X$ will return the same answer in terms of $\%$ power of Levene’s test as $SDR = \frac{1}{X}$. For example, SDR = 2 will return the same answer as SDR = 0.5.}

As we can see in the graph, the further SDR is from 1, the smaller the sample size is needed to detect a statistically significant difference in the SDR. Furthermore, for each SDR, power curves of the Levene’s test based on the mean are slightly above power curves of the Levene’s test based on the median, meaning that it leads to slightly higher power than Levene’s test based on the median. This can be due to the fact that data is extracted from normal distributions. With asymmetric data, the median would perform better. When SDR = 2, approximately 50 subjects are needed to have 80 percent power to detect differences while approximately 70 subjects are needed to have 95 percent power to detect differences (for both versions of Levene’s test). To detect an SDR of 1.5 with Levene’s test, approximately 120 subjects are needed to reach a power of 80$\%$ and about 160 to reach a power of 95$\%$. Since such an SDR is already very problematic in terms of the type I error rate for the Student’s $t$-test [@bradley_robustness_1978], needing such a large sample size to detect it is a serious hurdle. This issue becomes even worse for lower SDR, since an SDR as small as 1.1 already calls for the use of Welch’s $t$-test (See Table A3.1 to A3.9 in the additional file). Detecting such a small SDR calls for a huge sample size (a sample size of 160 provides a power rate of 16$\%$).

Since Welch’s $t$-test has practically the same power as Student’s $t$-test, even when SDR = 1, as explained using simulations later, we should seriously consider using Welch’s $t$-test by default.

The problems in using a two-step procedure (first testing for equality of variances, then deciding upon which test to use) have already been discussed in the field of statistics [see e.g., @rasch_two-sample_2011; @ruxton_unequal_2006; @wilcox_modern_2013; @zimmerman_note_2004], but these insights have not changed the current practices in psychology, as of yet. More importantly, researchers do not even seem to take the assumptions of Student’s $t$-test into consideration before performing the test, or at least rarely discuss assumption checks. We surveyed statistical tests reported in the journal SPPS (Social Psychological and Personality Science) between April 2015 and April 2016. From the total of 282 studies, 97 used a $t$-test (34.4$\%$), and the homogeneity of variance was explicitly discussed in only 2 of them. Moreover, based on the reported degrees of freedom in the results section it seems that Student’s $t$-test is used most often, and that alternatives are considerably less popular. For 7 studies, there were decimals in the values of the degrees of freedom, which suggests Welch’s $t$-test might have been used, although the use of Welch’s $t$-test might be higher but not identifiable, because some statisticians recommend to round the degrees of freedom to round numbers. 

To explain this lack of attention to assumption checks, some authors have argued that researchers might have a lack of knowledge (or a misunderstanding) of the parametric assumptions and consequences of their violations, or that they might not know how to check assumptions and/or what to do when assumptions are violated [@hoekstra_are_2012].^[ For example, many statistical users believe that the Mann-Whitney non-parametric test can cope with both normality and homosedasticity issues [@ruxton_unequal_2006]. This assumption is false, since the Mann-Whitney test remains sensitive to heterosedasticity [@grissom_heterogeneity_2000; @nachar_mann-whitney_2008; @neuhauser_distribution-free_2009] .] Finally, many researchers don’t even know there are options other than the Student’s $t$-test for comparing two groups [@erceg-hurn_modern_2008]. How problematic this is depends on how plausible the assumption of equal variances is in psychological research. We will discuss circumstances under which the equality of variances assumption is especially improbable, and provide real-life examples where the assumption of equal variances is violated.