---
title: " "
output: pdf_document
header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
---

# Introduction

## Truc qui n'ont rien à voir mais dont je pourrais avoir besoin à la défense

"A review by van de Schoot, Winter, Ryan, Zondervan-Zwijnenburg, and Depaoli (2017) revealed that 31% of articles in the psychological literature that used Bayesian analyses did not even specifiy the prior that was used, at least in part because the defaults by the software package were used. Mindless statistic are not limited to pvalues" (dans l'article de Daniel... j'adore cet argument!)

## Début de la vraie intro

Les statistiques font partie intégrante du cursus de formation des psychologues et jouent un rôle très important dans le parcours des chercheurs en psychologie, et des psychologues de manière générale. On attend d'eux qu'ils soient capables de produire des connaissances, fondées sur des preuves scientifiques (et non sur des croyances et opinions), et également de comprendre et évaluer les recherches menées par d'autres [@haslam_research_2014]. Or, dans un domaine dominé par les analyses quantitatives\footnote{parmi 68 articles analysés en 2013 par Counsell et ses collaborateurs (2017) dans 4 revues canadiennes, $92.7 \%$ incluaient au moins une analyse quantitative (contre $7.3 \%$ incluant une analyse qualitative)} [@counsell_reporting_2017], les connaissances statistiques s'avèrent fondamentales pour comprendre, planifier et analyser une recherche [@howitt_understanding_2017; @everitt_statistics_2001]. 

Au cours des 50 dernières années, on a assisté à un développement inouï de méthodes statistiques de plus en plus complexes: méta-analyses, équations structurelles, modèles linéaires hiérarchiques...[@byrne_status_1996; @sharpe_why_2013]. Pourtant, les programmes de statistiques au sein des facultés de psychologies n'ont que peu évolués [@henson_methodology_2010] et les tests de comparaison de moyennes ainsi que les régressions avec prédicteurs continus perdurent au choeur de la grande majorité des programmes [@aiken_doctoral_2008; @golinski_expanding_2009 REFERENCE PR L EUROPE A TROUVER] et des livres d'introduction aux statistiques pour psychologues. Ceci explique vraisemblablement leur succès persistant dans la recherche en psychologie [@counsell_reporting_2017]. Ces tests sont, depuis plus de 60 ans [@nunnally_place_1960; @byrne_status_1996], les tests les plus fréquemment cités dans la littérature scientifique [@golinski_expanding_2009; @delacre_why_2017; @delacre_taking_2019]. Dans une revue de 486 articles publiés en 2000 dans des journaux populaires en psychologie \footnote{Les revues analysées étaient les suivantes: "Child Development", "Journal of Abnormal Psychology", "Journal of Consulting and Clinical Psychology", "Journal of Experimental Psychology: General", "Journal of Personality" et "Social Psychology"}, @golinski_expanding_2009 avaient relevé 140 articles ($\approx 29 \%$) au sein desquels les auteurs avaient mené au moins une ANOVA à un ou plusieurs facteurs. Plus récemment, @counsell_reporting_2017 mentionnaient que parmis un ensemble de 151 études soumises dans 4 revues canadiennes en 2013, environ 40% incluaient une comparaison de moyennes. Plus récemment encore, nous avons trouvé qu'environ $34\%$ des études reportées dans *Social Psychological $\&$ Personality Science* entre avril 2015 et avril 2016 utilisaient un test-$t$ [@delacre_why_2017], et qu'environ $14\%$ des articles publiés en 2016 dans le *Journal of Personality and Social psychology* utilisaient une analyse de variance à un facteur [@delacre_taking_2019].     

On imagine, souvent à tort, que la plupart des chercheurs en psychologie ont une parfaite maîtrise des tests de comparaisons de moyennes \footnote{Parmi les professeurs interrogés en 2008 par Aiken et ses collaborateurs au sein de 201 départements des USA et du Canada, presque tous (entre $80\%$ et $100\%$) affirmaient que la plupart (si pas tous) leurs étudiants étaient capable de réaliser une ANOVA à un ou plusieurs facteurs (que ce soit l'approche a priori ou post hoc).}[@aiken_doctoral_2008; @hoekstra_are_2012].  ESSAYER DE TROUVER DES LIVRES QUI EN PARLENT THEORIQUEMENT ET D AUTRES QUI LES METTENT EN LIEN AVEC DES LOGICIELS CONNUS. Pourtant, la littérature tend à révéler que leur usage pourrait être amélioré. Parmi les critiques avancées à l'égard de ces tests, certaines relèvent des conditions dans lesquelles ils peuvent être utilisés et d'autres sont liées à la manière dont les chercheurs interprètent leur résultat (2 choses: la nil-null hypothesis, et la croyance que la pvaleur donne une info sur la pertinence de l'effet).  

## La manière dont l'ANOVA et les tests $t$ sont utilisés en psychologie.

Lorsqu'on applique un test-$t$ (ou une $ANOVA$), la distribution de deux (ou plus de deux) groupes indépendants est "résumée" par un indicateur de tendance centrale (par exemple la moyenne), et l'on compare ensuite les indicateurs de tendance centrale de chaque groupe, en vue de déterminer s'il existe des différences entre eux. On peut faire plusieurs constats quant à la manière dont ces tests sont reportés dans la littérature scientifique. 

### Limite 1: conditions d'application

D'abord, il n'y a pas un unique test-$t$ ou une unique $ANOVA$, mais une famille de tests-$t$ et une famille d'$ANOVA$. Pourtant, malgré la diversité des tests disponibles, le test-$t$ de Student (qui sert à comparer deux moyennes indépendantes) et l'$ANOVA$ de Fisher (qui permet de comparer deux ou plus de deux moyennes indépendantes) sont presque systématiquement utilisés par défaut. Ces deux tests, mathématiquement liés entre eux, ne seront pourtant valides qu'à condition que les résidus indépendants et identiquement distribués se distribuent normalement, et soient extraits de populations ayant des variances identiques. Un non respect des conditions d'application pourra sérieusement affecter le taux d'erreur de types I et II des tests. Pourtant, ces conditions ne sont que peu ou pas vérifiées [@hoekstra_are_2012]:  
- @keselman_statistical_1998: sur 61 articles contenant des designs dans lesquels des échantillons indépendants étaient définis par un seul facteur (between-subjects univariate designs, 13 n'ont pas donné d'information relatives aux écart-types. Parmi les autres, il y avait de l'hétéroscédasticité. En tout, moins d'un article sur 5 se préoccupait des violations possibles des conditions d'application, et lorsqu'ils le faisaient, ils s'inquiétaient surtout de la condition de normalité (moins de l'hétéroscédasticité), et ce malgré le fait que pourtant, des violations de cette conditions sont bien plus dommageables!     
- voir dans mes 2 articles sur le Welch, je donne des exemples aussi de ça  
- @golinski_expanding_2009: parmi 140 articles publiés dans plusieurs revues célèbres qui utilisaient l'ANOVA (voir plus haut), très peu vérifiaient les conditions d'application. *"It is interesting that, of the 11 articles that mentioned the normality assumption, 10 found distributions thate were nonnormal. Althought it is possible that the remaining articles taht did not mention th enormality assumptiona ll found noevidence of nonnormality, it seems highly unlikely given that Micceri (1989) who examined 440 variables from pubmished articles in educaiton and psychology, found that 84% showed moderate to extreme skew"*. En ce qui concerne l'homogénéité des variances, seulement 3 mentionnaient l'hypothèse d'homogénéité des variances, alors que parmis les 65 articles donnant l'info sur les variances de groupe, 27 avaient un SDR de plus de 2 (dans une étude, le SDR était même de 104), et que la plupart d'entre elles avaient des designs non balancés.       
- @counsell_reporting_2017: sur 151 études analysées, 44 reportaient des informations relatives aux conditions d'application des tests utilisés. Parmi celles ci, seulement 2 étaient exhaustifs: toutes les autres soit reportaient seulement une partie des conditions, soit le faisaient de manière inappropriée (ex.: vérifier la normalité de la VD au lieu de celle des résidus, dans le cas d'une régression). 

*Remarque: il est commun que les chercheurs annoncent l'usage d'un test-$t$ ou d'une $ANOVA$ dans leur étude sans préciser de quel "test-$t$" ou de quelle "$ANOVA$" il s'agit [@counsell_reporting_2017]. N'EST-CE PAS PARCE QUILS UTILISENT L OPTION DISPO PAR DEFAUT DANS LES LOGICIELS CLIC BOUTON?*

A cause du manque de transparence des chercheurs quant au respect ou non respect des conditions d'application, il est très compliqué de vérifier si le test qu'ils ont choisi est approprié ou non [counsell_reporting_2017].CA JE PEUX COMPLETER AVEC MON PROJET DE RECHERCHE: *le non report peut vouloir dire que la condition était ok mais ça semble très peu crédible qu'elle le soit dans autant d'études*. 

--> POURQUOI ELLES NE SONT PAS VERIFIEES? -> 
1) Leur non report peut être dû à un désir de limiter le nombre de pages (on se contente de reporter ce que les reviewers/éditeurs nous demandent de reporter; @counsell_reporting_2017)
2) Par manque de connaissances, les chercheurs se contentent souvent des informations fournies dans les logiciels clic/bouton. *for example, if software does not report a CI on Cohen's $d$, it is unlikely that a researcher will calculate one his or herself* (@counsell_reporting_2017). 

Ce constat est loin d'être récent. Par exemple, @keselman_statistical_1998 mettaient en évidence le fait que les chercheurs tendaient à utiliser des tests non robustes aux violations des conditions d'application, sans vérifier au préalable si ces conditions étaient respectées. 

Même lorsqu'un chercheur souhaite vérifier les conditions d'application, il reste confronté à plusieurs problèmes. 

1. Les conditions reposent sur les paramètres de *population* et non sur les paramètres d'échantillon. Or, ces paramètres de population ne sont pas connus (s'ils l'étaient, on n'aurait pas besoin des statistiques) [@hoekstra_are_2012].
2. Les conditions sont souvent très irréalistes. 

Il existe des tests dit "tests robustes" qui ne sont théoriquement pas affectés par une violation des conditions d'application [@erceg-hurn_modern_2008], mais ces derniers ne sont que peu ou pas utilisés [@sharpe_why_2013]. Malgré de nombreuses tentatives [@keselman_statistical_1998], leur succès reste très mitigé. 



Comment améliorer les pratiques de recherche facilement, d'une manière qui assure que les chercheurs appliqueront les conseils? En proposant des switchs faciles. L'usage des tests de Welch est un bel exemple de switch facile. Ceci dit, ce n'est pas parcpe que le switch est facile qu'il est forcément fait: @keselman_statistical_1998 écrit ceci: "Despite these repeated cautionary notes, behavioral science researchers have clearly not taken this mesasge to heart. It is strongly recommended that test procedures that have been desiged specifically for use in the presence of variance heterogeneity and/or nonnormality be adopted on a routine basis" (p.358).
Rem.: ils parlent d'un article de Lix et al. (1996) qui mentionne des packages qui permettent de le faire mais l'article est introuvable sur google scholar. L'open access est une des clés pour moi. w

ARTICLE1
ARTICLE2.

## Limite 2: hypothèse nulle

Rem.: "une violation des conditions d'application peut amener à une sous- ou sur-estimation des mesure de taille d'effet (Osborne & Waters, 2002, cités par Hoekstra!)

Le NHST fait l'objet d'énormément de critiques, si bien que certains recommandent de le remplacer par une mesure de taille d'effet accompagnée d'un intervalle de confiance autour de la taille d'effet. Le raisonnement est que si l'IC contient la valeur 0, on ne peut conclure à une différence significative [@counsell_reporting_2017]. 

Une des principales critiques des tests d'hypothèse est le fait que l'on compare la différence observée à l'absence totale de différence ( = un effet de 0). C'est une question qui est peu intéressante, car peu surprenante. Mais pourquoi comparer à 0 et pas à une autre valeur? 

D'après @lakens_practical_2021, un test d'hypothèse (selon l'approche de Nayman-Pearson) vaut la peine à 2 conditions:  
1) que l'hypothèse nulle soit assez plausible pour que son rejet puisse surprendre au moins certains;  
2) le chercheur veut appliquer une procédure méthodol qui l'autorise à prendre des décisions quant à la manière d'agir, tout en contrôlant le taux d'erreur. Agir peut vouloir dire: adopter un traitement, une politique, une intervention, ou abandonner un domaine de rechercher, modifier une manipulation, ou de faire un certain type de déclaration ou revendication.  

@counsell_reporting_2017:*the constant calls for reporting effect sizes appears to have had an effect on the Canadian psychology articles as just over 90% of the analyses that used a significance test also included a standardized or unstandardized effect size. Few articles presented an effect size without hypothesis testing, and few of the analyses' results included a CI*.

Ca se fait apparemment de plus en plus de reporter la taille d'effet (dans leur analyse de 151 études, 90% des analyses incluaient une mesure de taille d'effet, standardisée ou non... mais très peu incluaient les IC et de plus, ils les donnaient mais sans vraiment en discuter... @@counsell_reporting_2017 dans la discussion). 

Comme déjà mentionné, l'hypothèse nulle est l'absence d'effet. On en reste sur la nil-hypothesis. Du coup, un effet significatif n'a pas vraiment de valeur. En réponse à ce problème, on a écrit deux articles:

- On peut commencer par ajouter une information sur les tailles d'effets (mais du coup ça n'oblige pas à réfléchir à l'avance à l'effet qui nous intéresse)

Dans la revue de @keselman_statistical_1998, ils mentionnent que les tailles d'effet ne sont pratiquement jamais reportées malgré les recommandations du panuel de l'APA (1994) (et qu'elles ne sont fournies qu'en cas d'effet significatif). 

- On peut aussi faire des tests plus informatifs (tests d'équivalence et/ou tests d'effets minimaux). *One of the most widely suggested improvements of the use of p values is to replace null-ypothesis tests (where the goal is to reject ann effect of exactly 0) with tests of range predictions (where the goal is to reject effects that fall outside of the range of effects that is predicted or considered practically important) [@lakens_practical_2021]. 

## Pourquoi jusque là la sauce n'a pas pris?

Je suis loin d'être la première à signaler tt ça. Ce qui manque encore dans mon plan d'introduction, c'est que je dois encore trouver le moyen de montrer en quoi mes articles sont une plus-value, ce qu'ils apportent.
2) Parler des packages, des applications Shiny, etc.

D'aucun on fait le constat d'un fossé entre les méthodes inférentielles recommandées dans la littérature scientifique et les techniques réellement utilisées par les chercheurs appliqués [keselman_statistical_1998]. 

PARLER DES DIFFERENTES REVUES DE LITTERATURE QUI LE DISENT. 

Qu'est-ce qui pourrait expliquer cela? 
1) @sharpe_why_2013: lack of awareness (p.573) Manque de conscience des développements dans le domaine?  
2) @sharpe_why_2013: journal edotors (p.573) Les éditeurs ne poussent pas assez? --> Pas convaincue que ça m'intéresse  
3) @sharpe_why_2013: Publish or perish? (p.574) je ne comprends mm pas en quoi c'est un argument  
4) @sharpe_why_2013: Software (p.574) --> aaahh! Certaines pratiques comme les équations structurelles et les analyses de puissance ont été facilitées par des software comme gpower. Cela explique leur popularité. En ce qui concerne les statistiques plus robustes, par contre, elles ont moins de succès car non dispo dans les softwares dispo. Les gens veulent juste qu'on leur dise où cliquer pour avoir le test qu'ils veulent! C'est triste mais faut faire avec (à mon avis).   
5) @sharpe_why_2013: inadequate education (p.574)  
6) @sharpe_why_2013: mindset: facteurs psychologiques t.q. la peur de dévier des pratiques courantes (comme si on n'allait pas être publié si on ne faisait pas comme tlm).  


Anecdote: les chercheurs font souvent l'erreur de croire qu'il faut vérifier la normalité de la VD en faisant une régression. Dans SPSS, il est assez complexe de le faire car il faut d'abord calculer les résidus, ce qui implique de comprendre que les tests t et ANOVA sont des cas particuliers de régression, puis ensuite a posteriori représenter graphiquement les résidus. C'est chronophage et complexe. Dans Jamovi, par contre, la vérification de la normalité des résidus est automatiquement réalisée lorsqu'on fait un test t. Le rôle des méthodologistes, à mon sens, est de prémacher le travail, pour permettre à d'autres de créer des outils conçus pour améliorer les pratiques de recherche. à partir du moment où c'est automatiquement fait correctement, il devient moins problématique que les psychologues maîtrisent le détail. Débarassés de ces questions, ils pourront peut-être alors plus se focaliser sur l'important pour mieux comprendre et interpréter les résultats de leur tests: càd comprendre la distribution d'échantillonnage, dont pratiquement tt découle.


