---
title: " "
output: 
  papaja::apa6_pdf:
    includes:
      extra_dependencies: ["float"]

header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
---

\bibliographystyle{apalike-fr}

# Introduction

## Truc qui n'ont rien à voir mais dont je pourrais avoir besoin à la défense

"A review by van de Schoot, Winter, Ryan, Zondervan-Zwijnenburg, and Depaoli (2017) revealed that 31% of articles in the psychological literature that used Bayesian analyses did not even specifiy the prior that was used, at least in part because the defaults by the software package were used. Mindless statistic are not limited to pvalues" (dans l'article de Daniel... j'adore cet argument!)

## Début de la vraie intro

On attend des chercheurs en psychologie, et des psychologues en général, qu'ils soient capables de produire des connaissances fondées sur des preuves scientifiques (et non sur des croyances et opinions), et également de comprendre et évaluer les recherches menées par d'autres [@haslam_research_2014]. Or, dans un domaine dominé par les analyses quantitatives\footnote{parmi 68 articles analysés en 2013 par Counsell et ses collaborateurs (2017) dans 4 revues canadiennes, $92.7 \%$ incluaient au moins une analyse quantitative (contre $7.3 \%$ incluant une analyse qualitative)} [@counsell_reporting_2017], les connaissances statistiques s'avèrent fondamentales pour comprendre, planifier et analyser une recherche [@howitt_understanding_2017; @everitt_statistics_2001]. Les statistiques font dès lors partie intégrante du cursus de formation des psychologues et jouent un rôle très important dans leur parcours [@hoekstra_are_2012]. 

Traditionnellement, depuis plus de 50 ans, les tests-$t$ et les $ANOVA$ se trouvent au choeur de la grande majorité des programmes dans les domaines des Sciences Psychologiques et de l'Education [@aiken_doctoral_2008; @golinski_expanding_2009;@curtis_training_1998] et des livres d'introduction aux statistiques pour psychologues [@field_discovering_2013; autres exemples?]. Cela pourrait vraisemblablement expliquer pourquoi ils sont si persistants dans la recherche en psychologie [@counsell_reporting_2017]. Ces tests sont, depuis plus de 60 ans [@nunnally_place_1960; @byrne_status_1996], les tests les plus fréquemment cités dans la littérature scientifique [@golinski_expanding_2009]. Dans une revue de 486 articles publiés en 2000 dans des journaux populaires en psychologie \footnote{Les revues analysées étaient les suivantes: "Child Development", "Journal of Abnormal Psychology", "Journal of Consulting and Clinical Psychology", "Journal of Experimental Psychology: General", "Journal of Personality" et "Social Psychology"}, @golinski_expanding_2009 avaient relevé 140 articles ($\approx 29 \%$) au sein desquels les auteurs avaient mené au moins une ANOVA à un ou plusieurs facteurs. Plus récemment, @counsell_reporting_2017 mentionnaient que parmis un ensemble de 151 études soumises dans 4 revues canadiennes en 2013, environ 40% incluaient une comparaison de moyennes. Peut-être est-ce en raison de leur grande fréquence d'usage, ajoutée à leur apparente simplicité, qu'on tend à croire que la plupart des chercheurs, si pas tous, ont une bonne maîtrise des tests de comparaisons de moyennes [@aiken_doctoral_2008; @hoekstra_are_2012]. Pourtant, certains indices semblent contredire cette conviction. A travers cette thèse, nous allons adresser deux aspects de l'usage des tests de comparaison de moyennes qui peuvent être améliorés. *D'abord, ces sont souvent utilisés dans des contextes inadéquats, ce qui a pour conséquences d'augmenter le nombre de conclusions invalides et non réplicables  [@hoekstra_are_2012]. Ensuite, ils sont presque invariablement utilisés en définissant comme hypothèse nulle une absence de différence entre les moyennes des groupes [@meehl_appraising_1990; @steyn_practical_2000].* A AMELIORER. 

### Limite 1: conditions d'application

Bien qu'il existe plusieurs types de tests $t$, les chercheurs en psychologie tendent à privilégier par défaut le test $t$ de Student et l'$ANOVA$ de Fisher. La statistique $t$ de Student se calcule comme suit [@student_probable_1908]:
\begin{equation} 
t_{Student}=\frac{\bar{X_1}-\bar{X_2}}{\sqrt{\left(\frac{1}{N-2}\sum_{j=1}^2(n_j-1)S_j^2\right) \times \left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}
\label{Student}
\end{equation} 
où $N$ = le nombre total de sujets, et $n_j$ et $\bar{X_j}$ sont respectivement la taille et la moyenne du $j^{ème}$ échantillon ($j = 1, 2$). Sous l'hypothèse de normalité, la statistique $t$ de Student suit une distribution $t$ avec $n_1+n_2-2$ degrés de liberté. La statistique $F$ de Fisher se calcule comme suit:  
\begin{equation} 
F=\frac{\frac{1}{k-1}\sum_{j=1}^k \left[n_j\left(\bar{x_{j}}-\bar{x_{..}}\right)^2 \right]}{\frac{1}{N-k}\sum_{j=1}^k \left[\left(n_j-1\right)S_j^2 \right]}
\label{Fisher}
\end{equation} 
où $k$ est le nombre d'échantillons indépendants, et $S^2_j$ est la variance du $j^{ème}$ échantillon ($1 \le j \le k$). Sous l'hypothèse de normalité, la statistique $F$ suit loi de Fisher caractérisée par 2 paramètres:
\begin{equation*} 
df_1 = k-1
\end{equation*} 
\begin{equation*} 
df_2 = \sum_{j=1}^k{n_j}-k
\end{equation*}
Comme le révêlent les équations \eqref{Student} et \eqref{Fisher}, la variance poolée intervient au dénominateur des statistiques $t$ de Student et $F$ de Fisher. Or, utiliser ce terme n'a de sens que si la condition d'homogénéité des variances est respectée. Dans certains cas, la violation des conditions de normalité et d'homogénéité des variances peut entraîner une augmentation du risque alpha mais également amener à une perte de puissance [@hoekstra_are_2012;@osborne_four_2002]. Pour cette raison, les chercheurs devraient toujours s’assurer de systématiquement faire mention des conditions des tests qu'ils utilisent, afin de rassurer le lecteur sur la confiance qu’il peut accorder à la fiabilité des résultats [@osborne_four_2002]. Pourtant, malgré la demande de plus de transparence dans la transmission des analyses de données [@counsell_reporting_2017], on constate que dans les articles publiés, il n’est que rarement fait mention de ces conditions d’application. @osborne_educational_2001, par exemple, avaient trouvé que seulement 8% des auteurs reportaient des informations sur les conditions d’application des tests, soit à peine 1% de plus qu’en 1969. Plus récemment, @hoekstra_are_2012 ont montré que sur 50 articles publiés en 2011 dans *Psychological Science* utilisant au moins une ANOVA, test-$t$ ou régression, seulement trois discutaient des questions de normalité et d’homogénéité des variances. Par ailleurs, les informations reportées sont souvent non exhaustives [@counsell_reporting_2017], et la condition d'homogénéité des variances est encore moins fréquemment citée que celle de normalité. Parmi les 61 articles analysés par @keselman_statistical_1998, seulement $5\%$ des articles mentionnaient simultanément les conditions de normalité et d'homogénéité des variances (et en tout, la condition de normalité était mentionnée dans $11\%$ des cas, contre seulement 8% pour la condition d'homogénéité des variances). @golinski_expanding_2009 ont fait un constat similaire: parmi les 140 articles qu'ils ont analysé, seulement 11 mentionnaient explicitement la condition de normalité, contre 3 qui mentionnaient celle d'homogénéité des variances. Bien sûr, ne pas mentionner les conditions d'application ne veut pas forcément dire qu'elles n'ont pas été prises en compte dans les analyses, et plusieurs pistes pourraient être envisagées. 

On pourrait envisager que les auteurs vérifient les conditions d'application des tests mais ne le mentionnent la plupart du temps que lorsqu'elles sont violées [@counsell_reporting_2017; @golinski_expanding_2009]. @golinski_expanding_2009, par exemple, ont relevé que parmi les 11 articles de leur revue de littérature qui mentionnaient la condition de normalité, 10 montraient une violation de cette dernière. Le manuel des normes APA *sûr pour la 6ème édition, à confirmer pour la 7ème?* ne demande pas explicitement le report des informations relatives aux conditions d'application des tests [@hoekstra_are_2012], et l'on pourrait imaginer que motivés par le désir de rentabiliser l'espace disponible dans les manuscripts [@counsell_reporting_2017], les auteurs soient tentés de se limiter aux informations explicitement demandées par les éditeurs et les reviewers des journaux [@counsell_reporting_2017]. Il est cependant peu probable que cela suffise à expliquer pourquoi les conditions d'application soient mentionnées dans un pourcentage si faible d'articles, puisqu'il a été argumenté à de nombreuses reprises que le respect des conditions de normalité et d'homogénéité des variances est plus l'exception que la norme dans de nombreux domaines de la psychologie [@cain_univariate_2017; @micceri_unicorn_1989; @yuan_structural_2004; @erceg-hurn_modern_2008; @grissom_heterogeneity_2000]. Une hypothèse plus probable est que les chercheurs ne vérifient pas les conditions d'application des tests paramétriques. C'était déjà l'hypothèse soutenue il y a plus de 20 ans par @keselman_statistical_1998. @hoekstra_are_2012, ayant fait le même constat, l'expliquent par un manque de familiarité des chercheurs avec les conditions d'application des tests, plutôt que par un choix délibéré. Ils ajoutent que lorsque les conditions sont vérifiées, elles le sont d'une manière inappropriée [généralement, à travers l'usage de tests statistiques, alors qu'il a été démontré que l'application d'un test conditionnellement aux résultats d'un test statistique préliminaire a pour effet d'augmenter l'erreur de type I; @schucany_preliminary_2006].   


On pourrait également envisager que les chercheurs utilisent des alternatives plus robustes au test $t$ de Student et à l'$ANOVA$ de Fisher, sans le mentionner dans leur article. Il arrive en effet fréquemment que les chercheurs annoncent avoir réalisé un test-$t$ ou une $ANOVA$ sans spécifier de quel type de test-$t$ ou d'$ANOVA$ il s'agit [@counsell_reporting_2017]. Or, dans ces familles de tests, il existe des alternatives qui ne sont théoriquement pas affectées par une violation des conditions d'application [@erceg-hurn_modern_2008]. Cette hypothèse semble cependant contredite par plusieurs méthodologiques. D'après @sharpe_why_2013, ces alternatives robustes ne sont que peu ou pas utilisés: malgré de nombreuses tentatives [@keselman_statistical_1998], leur succès reste très mitigé. Afin d'étudier les pratiques des chercheurs lorsqu'ils étaient confrontés à un scénario qui impliquait la réalisation d'un test $t$, d'une $ANOVA$ ou d'une régression linéaire, @hoekstra_are_2012 ont observé 30 doctorants qui travaillaient depuis au moins deux ans dans des départements de psychologie aux Pays Bas et qui avaient dû pratiquer tous ces tests au moins une fois. *Aucun* d'entre eux n'a décidé d'opter pour des techniques qui ne dépendent pas des conditions d'application.  


*Lié au manque de connaissance liées aux conditions: [Le théorème central limite, par exemple, fait l’objet d’incompréhensions, si bien que même des chercheurs expérimentés ont l’intuition erronée que la loi des grands nombres s’applique également à de petits nombres (Braver et al., 2014). ]*

Il faut noter que même lorsqu'un chercheur souhaite vérifier les conditions d'application, il reste confronté à deux problèmes majeurs. D'abord, les conditions d'homogénéité des variances et de normalité reposent sur les paramètres de *population* et non sur les paramètres d'échantillon. Or, ces paramètres de population ne sont pas connus. Ensuite, comme déjà mentionné, ces conditions ne seront souvent pas respectées au sens strict du terme, puisqu'elles sont définies de manière très stricte (par exemple, l'hypothèse d'homogénéité des variances est que la variance des deux populations sont sont extraits les échantillons soient exactement égales) [@hoekstra_are_2012]. 
- *Du coup, ce qu'on doit vérifier, ce n'est pas si les conditions sont violées, mais si elles ne le sont "pas trop". 
- il faut aussi voir à quel point les tests sont robustes aux violations de ces conditions. 
--> Il y a eu pleins de simulations qui ont testés tout ça et qui ont révélé que les tests de comparaisons de moyennes étaient plus sensibles aux violations de la condition d'homogénéité des variances qu'à celle de normalité. C'est pourquoi, on propose comme switch facile d'utiliser par défaut un test qui dépend de la condition de normalité, mais pas de celle d'homogénéité des variances --> Welch.







--> POURQUOI ELLES NE SONT PAS VERIFIEES? -> 
2) Par manque de connaissances, les chercheurs se contentent souvent des informations fournies dans les logiciels clic/bouton. *for example, if software does not report a CI on Cohen's $d$, it is unlikely that a researcher will calculate one his or herself* (@counsell_reporting_2017). 













Comment améliorer les pratiques de recherche facilement, d'une manière qui assure que les chercheurs appliqueront les conseils? En proposant des switchs faciles. L'usage des tests de Welch est un bel exemple de switch facile. Ceci dit, ce n'est pas parcpe que le switch est facile qu'il est forcément fait: @keselman_statistical_1998 écrit ceci: "Despite these repeated cautionary notes, behavioral science researchers have clearly not taken this mesasge to heart. It is strongly recommended that test procedures that have been desiged specifically for use in the presence of variance heterogeneity and/or nonnormality be adopted on a routine basis" (p.358).
Rem.: ils parlent d'un article de Lix et al. (1996) qui mentionne des packages qui permettent de le faire mais l'article est introuvable sur google scholar. L'open access est une des clés pour moi. w

ARTICLE1
ARTICLE2.

## Limite 2: hypothèse nulle

Effect sizes are an important outcome of empirical research. Moving beyond decisions about statistical significance, there is a strong call for researchers to report and interpret effect sizes and associated confidence intervals. This practice is highly endorsed by the American Psychological Association (APA) and the American Educational Research Association (American Educational Research Association, 2006; American Psychological Association, 2010).

En parlant des tailles d'effets, on commence de plus en plus à les utiliser (j'ai une réf qui le dit) mais:
- on les calcule sans vraiment les comprendres/interpréter
- comme pour le test t de Student et l'ANOVA, on utilise un test qui dépend des mm conditions d'application.
Utiliser des tests plus adéquats permettrait d'améliorer les pratiques et à termes, de déterminer des mesures de taille d'effets qui pourront être utilisées a priori dans des tests plus informatifs que ceux visant à détecter l'absence d'effet (cf. tests d'équivalence). 

Un paragraphe relatif à la taille d'effet. EN EXPLORATOIRE, ce qui à termes pourrait servir à définir des hypothèses plus informatives pour d'autres chercheurs, qui pourraient être utilisées, soit dans des tests d'effets minimaux, soit pour des tests d'équivalence. Et that's it.

Rem.: "une violation des conditions d'application peut amener à une sous- ou sur-estimation des mesure de taille d'effet (Osborne & Waters, 2002, cités par Hoekstra!)

Le NHST fait l'objet d'énormément de critiques, si bien que certains recommandent de le remplacer par une mesure de taille d'effet accompagnée d'un intervalle de confiance autour de la taille d'effet. Le raisonnement est que si l'IC contient la valeur 0, on ne peut conclure à une différence significative [@counsell_reporting_2017]. 

Une des principales critiques des tests d'hypothèse est le fait que l'on compare la différence observée à l'absence totale de différence ( = un effet de 0). C'est une question qui est peu intéressante, car peu surprenante. Mais pourquoi comparer à 0 et pas à une autre valeur? 

D'après @lakens_practical_2021, un test d'hypothèse (selon l'approche de Nayman-Pearson) vaut la peine à 2 conditions:  
1) que l'hypothèse nulle soit assez plausible pour que son rejet puisse surprendre au moins certains;  
2) le chercheur veut appliquer une procédure méthodol qui l'autorise à prendre des décisions quant à la manière d'agir, tout en contrôlant le taux d'erreur. Agir peut vouloir dire: adopter un traitement, une politique, une intervention, ou abandonner un domaine de rechercher, modifier une manipulation, ou de faire un certain type de déclaration ou revendication.  

@counsell_reporting_2017:*the constant calls for reporting effect sizes appears to have had an effect on the Canadian psychology articles as just over 90% of the analyses that used a significance test also included a standardized or unstandardized effect size. Few articles presented an effect size without hypothesis testing, and few of the analyses' results included a CI*.

Ca se fait apparemment de plus en plus de reporter la taille d'effet (dans leur analyse de 151 études, 90% des analyses incluaient une mesure de taille d'effet, standardisée ou non... mais très peu incluaient les IC et de plus, ils les donnaient mais sans vraiment en discuter... @@counsell_reporting_2017 dans la discussion). 

Comme déjà mentionné, l'hypothèse nulle est l'absence d'effet. On en reste sur la nil-hypothesis. Du coup, un effet significatif n'a pas vraiment de valeur. En réponse à ce problème, on a écrit deux articles:

- On peut commencer par ajouter une information sur les tailles d'effets (mais du coup ça n'oblige pas à réfléchir à l'avance à l'effet qui nous intéresse)

Dans la revue de @keselman_statistical_1998, ils mentionnent que les tailles d'effet ne sont pratiquement jamais reportées malgré les recommandations du panuel de l'APA (1994) (et qu'elles ne sont fournies qu'en cas d'effet significatif). 

- On peut aussi faire des tests plus informatifs (tests d'équivalence et/ou tests d'effets minimaux). *One of the most widely suggested improvements of the use of p values is to replace null-ypothesis tests (where the goal is to reject ann effect of exactly 0) with tests of range predictions (where the goal is to reject effects that fall outside of the range of effects that is predicted or considered practically important) [@lakens_practical_2021]. 

## Pourquoi jusque là la sauce n'a pas pris?

Je suis loin d'être la première à signaler tt ça. Ce qui manque encore dans mon plan d'introduction, c'est que je dois encore trouver le moyen de montrer en quoi mes articles sont une plus-value, ce qu'ils apportent.
2) Parler des packages, des applications Shiny, etc.

D'aucun on fait le constat d'un fossé entre les méthodes inférentielles recommandées dans la littérature scientifique et les techniques réellement utilisées par les chercheurs appliqués [keselman_statistical_1998]. 

PARLER DES DIFFERENTES REVUES DE LITTERATURE QUI LE DISENT. 

Qu'est-ce qui pourrait expliquer cela? 
1) @sharpe_why_2013: lack of awareness (p.573) Manque de conscience des développements dans le domaine?  
2) @sharpe_why_2013: journal edotors (p.573) Les éditeurs ne poussent pas assez? --> Pas convaincue que ça m'intéresse  
3) @sharpe_why_2013: Publish or perish? (p.574) je ne comprends mm pas en quoi c'est un argument  
4) @sharpe_why_2013: Software (p.574) --> aaahh! Certaines pratiques comme les équations structurelles et les analyses de puissance ont été facilitées par des software comme gpower. Cela explique leur popularité. En ce qui concerne les statistiques plus robustes, par contre, elles ont moins de succès car non dispo dans les softwares dispo. Les gens veulent juste qu'on leur dise où cliquer pour avoir le test qu'ils veulent! C'est triste mais faut faire avec (à mon avis).   
5) @sharpe_why_2013: inadequate education (p.574)  
6) @sharpe_why_2013: mindset: facteurs psychologiques t.q. la peur de dévier des pratiques courantes (comme si on n'allait pas être publié si on ne faisait pas comme tlm).  


Anecdote: les chercheurs font souvent l'erreur de croire qu'il faut vérifier la normalité de la VD en faisant une régression. Dans SPSS, il est assez complexe de le faire car il faut d'abord calculer les résidus, ce qui implique de comprendre que les tests t et ANOVA sont des cas particuliers de régression, puis ensuite a posteriori représenter graphiquement les résidus. C'est chronophage et complexe. Dans Jamovi, par contre, la vérification de la normalité des résidus est automatiquement réalisée lorsqu'on fait un test t. Le rôle des méthodologistes, à mon sens, est de prémacher le travail, pour permettre à d'autres de créer des outils conçus pour améliorer les pratiques de recherche. à partir du moment où c'est automatiquement fait correctement, il devient moins problématique que les psychologues maîtrisent le détail. Débarassés de ces questions, ils pourront peut-être alors plus se focaliser sur l'important pour mieux comprendre et interpréter les résultats de leur tests: càd comprendre la distribution d'échantillonnage, dont pratiquement tt découle.


